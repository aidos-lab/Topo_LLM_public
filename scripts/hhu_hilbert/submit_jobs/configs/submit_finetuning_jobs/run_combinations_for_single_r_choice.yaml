defaults:
  - base_submit_finetuning_jobs_config
  - training_schedule: 
    - short_linear_lr_scheduler
    - long_constant_lr_scheduler
  - _self_

base_model:
  - roberta-base

finetuning_dataset:
  - train_and_eval_on_multiwoz21_10000_samples
  - train_and_eval_on_one-year-of-tsla-on-reddit

peft:
  - standard
  - lora

gradient_modifier:
  - do_nothing
  - freeze_first_layers_bert-style-models
  - freeze_last_layers_bert-style-models

lora_parameters:
  first:
    lora_r: 16
    lora_alpha: 32
    use_rslora: false

wandb_project: "Topo_LLM_run_combinations_for_single_r_choice"
