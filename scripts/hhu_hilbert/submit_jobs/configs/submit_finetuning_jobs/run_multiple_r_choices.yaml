defaults:
  - base_submit_finetuning_jobs_config
  - training_schedule:
    - long_constant_lr_scheduler
  - _self_

base_model:
  - roberta-base

finetuning_dataset:
  - train_and_eval_on_multiwoz21_10000_samples
  - train_and_eval_on_one-year-of-tsla-on-reddit

peft:
  - lora

gradient_modifier:
  - do_nothing

lora_parameters:
  "0":
    lora_r: 4
    lora_alpha: 8
    use_rslora: true
  "1":
    lora_r: 8
    lora_alpha: 16
    use_rslora: true
  "2":
    lora_r: 16
    lora_alpha: 32
    use_rslora: true
  "3":
    lora_r: 32
    lora_alpha: 64
    use_rslora: true
  "4":
    lora_r: 64
    lora_alpha: 128
    use_rslora: true
  "5":
    lora_r: 128
    lora_alpha: 256
    use_rslora: true
  "6":
    lora_r: 256
    lora_alpha: 512
    use_rslora: true
  "7":
    lora_r: 512
    lora_alpha: 1024
    use_rslora: true

wandb_project: "Topo_LLM_run_multiple_r_choices"
