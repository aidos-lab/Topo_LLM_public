mode: freeze_layers
# Note: The dots at the end of the strings are important.
# If left out, a module like 'roberta.encoder.layer.10.attention.self.query.weight'
# would be frozen if 'encoder.layer.1' is in the list, becaus we check for string containment.
target_modules_to_freeze:
- "encoder.layer.0."