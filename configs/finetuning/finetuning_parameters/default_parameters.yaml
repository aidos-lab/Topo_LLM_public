batch_sizes:
  train: 16
  eval: 32
eval_steps: 400
fp16: false
gradient_accumulation_steps: 2
gradient_checkpointing: true
learning_rate: 5e-5
lr_scheduler_type: linear
log_level: info
logging_steps: 100
max_length: 512
max_steps: -1 # '-1' for no 'max_steps' limit
mlm_probability: 0.15
num_train_epochs: 5
save_steps: 400
# seed is contained in the main config file
use_cpu: false
warmup_steps: 500
weight_decay: 0.01