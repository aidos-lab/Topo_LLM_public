checkpoint_no: 0
lm_mode: mlm
task_type: masked_lm
pretrained_model_name_or_path: ${paths.data_dir}/models/finetuned_models/data=one-year-of-tsla-on-reddit_rm-empty=True_spl-mode=proportions_spl-shuf=True_spl-seed=0_tr=0.8_va=0.1_te=0.1_ctxt=dataset_entry_feat-col=ner_tags/split=train_samples=80_sampling=take_first/model=roberta-base_task=masked_lm_dr=defaults/ftm=standard/lora-None/gradmod=freeze_layers_target-freeze=lm_head/lr=5e-05_lr-scheduler-type=linear_wd=0.01/bs-train=16/ep=2/seed=1236/model_files/checkpoint-${language_model.checkpoint_no}
manual_tokenizer_override_pretrained_model_name_or_path: null
seed: 1236
short_model_name: roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-80-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-2_seed-${language_model.seed}_ckpt-${language_model.checkpoint_no}
dropout:
  mode: defaults
  probabilities:
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    classifier_dropout: null
tokenizer_modifier:
  mode: do_nothing
  padding_token: <pad>
