checkpoint_no: 2800
lm_mode: clm
task_type: causal_lm
pretrained_model_name_or_path: ${paths.data_dir}/models/finetuned_models/data=multiwoz21_rm-empty=True_spl-mode=do_nothing_ctxt=dataset_entry_feat-col=ner_tags/split=train_samples=10000_sampling=random_sampling-seed=778/add-prefix-space=False_max-len=512/model=Llama-3.2-1B_task=causal_lm_dr=defaults/ftm=lora/r=16_alpha=32_lora-target=o_proj_q_proj_k_proj_v_proj_lora-dr=0.01_rslora=True/gradmod=do_nothing_target-freeze=None/lr=5e-05_lr-scheduler-type=linear_wd=0.01/bs-train=8/ep=5/seed=1234/model_files/checkpoint-${language_model.checkpoint_no}
model_log_file_path: null
manual_tokenizer_override_pretrained_model_name_or_path: null
seed: 1234
short_model_name: Llama-3.2-1B-causal_lm-defaults_multiwoz21-r-T-dn-ner_tags_tr-10000-r-778_aps-F-mx-512_lora-16-32-o_proj_q_proj_k_proj_v_proj-0.01-T_5e-05-linear-0.01-f-None-5_seed-${language_model.seed}_ckpt-${language_model.checkpoint_no}
dropout:
  mode: defaults
  probabilities:
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    classifier_dropout: null
tokenizer_modifier:
  mode: replace_pad_token_with_other_special_token
  padding_token: null
  replace_pad_token_with_other_special_token_identifier: eos_token
