{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": ""
        },
        {
            "name": "Python Debugger: Current File with Arguments",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": "${command:pickArgs}"
        },
        {
            "name": "Python Debugger: Current File with Multirun",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=sgd_test",
                "language_model=roberta-base,roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset",
                "hydra.job.env_set.CUDA_VISIBLE_DEVICES=0",
                "tokenizer.add_prefix_space=True",
            ]
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Huggingface dataloaders; RoBERTa, BERT, SentiX, ContextBERT-ERToD_emowoz, GPT-2; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                //
                // >>>> ERC datasets
                "data=ertod_emowoz",
                // "data.dataset_seed=42",
                "data.use_context=False",
                "data.dataset_seed=50",
                //
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=512",
                "data.data_subsampling.split=validation",
                //
                //    >> END: Data options.
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=gpt2-medium",
                //
                "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                "language_model.seed=42",
                // "language_model.checkpoint_no=0",
                "language_model.checkpoint_no=1",
                // "language_model.checkpoint_no=1,2",
                "language_model.use_context=True", // <-- This is the debugging model we might have saved locally
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                // "local_estimates=lpca",
                // "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                //    >> NOTE: Only skip the embedding computation if the embeddings are already available.
                "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=false", // <-- Only set this to true if the embeddings are available
                "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=false",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Huggingface dataloaders; SetSUMBT or Trippy checkpoints; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3549",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Stored dataloaders, SetSUMBT-dataloaders, Trippy-dataloaders; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                //
                "data=trippy_dataloaders_processed",
                // "data.data_subsampling.split=train",
                "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439", // <-- locally, we have these checkpoints available for "language_model.seed=0"
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3549",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - lPCA; Stored dataloaders, SetSUMBT-dataloaders; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                "data=setsumbt_dataloaders_processed",
                "data.dataset_seed=0",
                // "data.data_subsampling.split=train",
                "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                "language_model.seed=0",
                // "language_model.seed=1",
                "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439", // <-- locally, we have these checkpoints available for "language_model.seed=0"
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HHU Hilbert HPC submission - TwoNN; RoBERTa, BERT, ContextBERT-ERToD, SetSUMBT, Trippy checkpoints; last layer only - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.template=GTX1080",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=01:30:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                //    >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // >>>> ERC datasets
                // "data=ertod_emowoz",
                // "data.dataset_seed=42",
                // "data.dataset_seed=43,44",
                // "data.dataset_seed=50,51,52",
                //
                // >>>> SetSUMBT datasets
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                //
                // >>>> Trippy datasets
                "data=trippy_dataloaders_processed",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                "data.data_subsampling.split=train,dev,test",
                // "data.data_subsampling.split=train,validation,test",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=roberta-base,bert-base-uncased,bert-base-uncased-SentiX",
                //
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.seed=42",
                // "language_model.seed=43,44",
                // "language_model.seed=43,44,45",
                // "language_model.seed=50,51,52",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=0,1,2,3,4",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- checkpoints for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,30943,42195,47821,53447,61886,109707", // <-- checkpoints for "language_model.seed=1"
                // "language_model.seed=2",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,22504,28130,30943,33756,42195,50634,67512,106894,126585", // <-- checkpoints for "language_model.seed=2"
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=3549,7098",
                "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HHU Hilbert HPC submission - lPCA; SetSUMBT or Trippy checkpoints; last layer only - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.walltime=03:00:00", // <-- Until we have a better estimate of how long the lPCA estimates take, we will set the walltime to a larger number of hours.
                // ===== GPU-machine =====
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // ===== CPU-machine =====
                "hydra.launcher.queue=DEFAULT",
                "hydra.launcher.template=CPU",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=6",
                "hydra.launcher.ngpus=0",
                // >>>> END: Hydra options
                //
                //    >> START: Data options.
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                //
                // "data.data_subsampling.split=train",
                "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,dev,test",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- checkpoints for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,30943,42195,47821,53447,61886,109707", // <-- checkpoints for "language_model.seed=1"
                // "language_model.seed=2",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,22504,28130,30943,33756,42195,50634,67512,106894,126585", // <-- checkpoints for "language_model.seed=2"
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439",
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490", // <-- checkpoints for "language_model.seed=42"
                // "language_model.checkpoint_no=3549",
                // "language_model.checkpoint_no=3549,7098",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                //    >> NOTE: Only skip the embedding computation if the embeddings are already available.
                "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=true", // <-- Only set this to true if the embeddings are available
                "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=false",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=false", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HHU Hilbert HPC submission - TwoNN; SetSUMBT or Trippy selected checkpoints; multiple layers - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.template=GTX1080",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,16878",
                // "++language_model.checkpoint_no=5626,8439,11252,14065",
                "++language_model.checkpoint_no=14065,19691,25317,33756,36569,39382,42195,50634,56260",
                // "++language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585",
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=3549,7098",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1],[-2],[-3],[-4],[-5],[-6],[-7],[-8],[-9],[-10],[-11],[-12]", // <-- Compute estimates for all layers
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=false", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run local estimates - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Run local estimates: Duplicate vectors test case - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            // This prepared data leads to the `ValueError: Input X contains NaN.` error in `LinearRegression`.
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=3000",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "local_estimates.filtering.num_samples=2500",
                "local_estimates.filtering.num_samples=5000",
                // "local_estimates.filtering.deduplication_mode=identity",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                // "local_estimates.pointwise.absolute_n_neighbors=64",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=false"
            ]
        },
        {
            "name": "Load huggingface dataset - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/data_processing/load_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                // "data=bookcorpus",
                // "data=multiwoz21",
                "data=wikitext-103-v1",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_DEBUG",
            ]
        },
        {
            "name": "Compute eval loss via huggingface transformers Trainer - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/compute_eval_loss_via_huggingface_trainer.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21",
                // "data=one-year-of-tsla-on-reddit",
                // "data.data_subsampling.number_of_samples=512",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                //
                "data.data_subsampling.number_of_samples=128", // <-- This can be used for quick testing of the script
                "data.data_subsampling.split=validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=False" to be consistent with the fine-tuning setup
                //
                "language_model=roberta-base",
                // "language_model=gpt2-medium",
                //
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_DEBUG",
            ],
            "env": {}
        },
        {
            "name": "Compute eval loss via huggingface transformers Trainer - HHU Hilbert HPC submission -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/compute_eval_loss_via_huggingface_trainer.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:30:00", // <-- The evaluation should not take more than a few minutes
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21",
                // "data=one-year-of-tsla-on-reddit",
                // "data=iclr_2024_submissions_train,multiwoz21_train,one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train", // <-- all training splits
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                //
                // "data.data_subsampling.number_of_samples=128", // <-- This can be used for quick testing of the script
                "data.data_subsampling.number_of_samples=10000",
                // No data split selected here, since we will do this above in the data configuration
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.sampling_seed=778",
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=False" to be consistent with the fine-tuning setup
                //
                //    >> START Model options.
                //    > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // 
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head_embeddings.word_embeddings-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-5",
                "++language_model.checkpoint_no=100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                //    >> END Model options.
                //
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_DEBUG",
            ],
            "env": {}
        },
        {
            "name": "Fine-tuning - skip finetuning; only config file creation; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.finetuning.skip_finetuning=true",
                "feature_flags.finetuning.use_wandb=false",
            ]
        },
        {
            "name": "Fine-tuning - on NER task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_token_classification",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_finetuning_for_token_classification_DEBUG",
            ]
        },
        {
            "name": "Fine-tuning - Local debugging - on MLM task; for 2 epochs",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "finetuning=finetuning_for_masked_lm",
                // <-- Saving every 2 steps should result in two saved checkpoints for a dataset of size 80 and batch size of: "Instantaneous batch size per device = 16"
                "finetuning.save_steps=2",
                "finetuning.eval_steps=2",
                "finetuning/finetuning_datasets=train_and_eval_on_one-year-of-tsla-on-reddit_train-samples-small",
                // "finetuning/finetuning_datasets=train_and_eval_on_wikitext_train-samples-small",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=80",
                // "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_mode=take_first",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_mode=random",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_seed=112",
                // "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_seed=113",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.num_train_epochs=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "finetuning.seed=1235",
                // "finetuning.seed=1235,1236",
                // >> START Optional: Change gradient modifier
                "finetuning/gradient_modifier=do_nothing",
                // "finetuning/gradient_modifier=freeze_lm_head_bert-style-models",
                // "finetuning/gradient_modifier=freeze_lm_head_and_word_embeddings_bert-style-models",
                // >> END Optional: Change gradient modifier
                "feature_flags.wandb.use_wandb=true",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "Fine-tuning - on CLM task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_causal_lm",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                // "data=one-year-of-tsla-on-reddit_validation",
                // "data=one-year-of-tsla-on-reddit_test",
                "data=multiwoz21_validation",
                // "data=sgd_test",
                // "data.data_splitting.data_splitting_mode=proportions",
                // "data.data_splitting.split_shuffle=true",
                // "data.data_splitting.split_seed=123",
                // "data.data_split.split_seed=null", // 'null' in the Hydra config is mapped to `None` in the Python code, and then no seed is set for the split
                "data.data_subsampling.number_of_samples=128",
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler, with Gaussian noise - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.noise.artificial_noise_mode=gaussian",
                // "local_estimates.noise.artificial_noise_mode=do_nothing",
                "local_estimates.noise.distortion_parameter=0.001",
                "local_estimates.noise.seed=2",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Masked Token Embedding; on reddit, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=one-year-of-tsla-on-reddit", // Note: Since the sequences in the reddit dataset are quite long, this takes a long time to run in the masked token mode
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/perplexity/saved_perplexity_processing/run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "console": "integratedTerminal",
            "args": [
                // "data=multiwoz21",
                "data=iclr_2024_submissions_test",
                // "language_model=roberta-base",
                "language_model=model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "embeddings_data_prep.sampling.sampling_mode=take_first",
            ]
        },
        {
            "name": "run_general_comparisons.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": []
        },
        {
            "name": "run_general_comparisons.py; without iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; without iteration; only noise analysis",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
                "feature_flags.analysis.compare_sampling_methods.do_noise_analysis=True",
                "feature_flags.analysis.compare_sampling_methods.do_checkpoint_analysis=False",
                "feature_flags.analysis.compare_sampling_methods.do_data_subsampling_number_of_samples_analysis=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; with iteration; but without expensive plots during iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True",
                "feature_flags.analysis.compare_sampling_methods.do_analysis_influence_of_local_estimates_n_neighbors=False",
                "feature_flags.analysis.compare_sampling_methods.do_create_boxplot_of_mean_over_different_sampling_seeds=False"
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example multiwoz21 test data, compare clean with noisy data",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=masked_token",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data.local_estimates.noise.artificial_noise_mode=gaussian",
                "+comparison_data.local_estimates.noise.distortion_parameter=0.01",
                "+comparison_data.local_estimates.noise.seed=4",
                //
                //    >> END: Comparison data.
                //
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=600",
                // "analysis.investigate_distances.array_truncation_size=5000",
                "analysis.investigate_distances.array_truncation_size=20000",
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example wikitext test data, compare masked token embeddings with regular token embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=wikitext-103-v1_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // Option: Take a different token mode for comparison
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - Local debugging - small array_truncation_size; for selected datasets and models; with comparison of local estimates with losses, but without comparison with other data (i.e., loss computation only on base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //    >> START Data options.
                "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=True",
                // >>>> Set this feature flag to avoid the comparison with other data
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=False",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (no loss computation) - Local debugging - small array_truncation_size; with comparison of local estimates between settings (but without loss computation)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //
                //    >> START Data options.
                //
                "tokenizer.add_prefix_space=False",
                //
                // "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //
                "data=setsumbt_dataloaders_processed",
                "data.dataset_seed=0",
                "data.data_subsampling.split=dev",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //    >> END Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-setsumbt_multiwoz21",
                //
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Comparison data.
                // TODO: Update this
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data/local_estimates=twonn",
                //
                //    >> END: Comparison data.
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=False", // <-- Set this feature flag to avoid the predictions and loss computation
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=True", // <-- Set this feature flag to compare with different local estimates
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HHU Hilbert HPC submission - for validation or test splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:20:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21_validation", // <-- only multiwoz21_validation
                // "data=iclr_2024_submissions_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits except multiwoz21_validation
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                //    >> START Model options.
                //    > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // 
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head_embeddings.word_embeddings-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-5",
                "++language_model.checkpoint_no=100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                //    >> END Model options.
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HHU Hilbert HPC submission - for different subsampling seeds of validation splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=02:00:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                // "data=multiwoz21_validation",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779", // <-- Different choices of data_subsampling.sampling_seed here
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                //
                // > Note: Remember to set checkpoint_no for the fine-tuned models!
                //
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HHU Hilbert HPC submission - for multiple train datasets (take first sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                //
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                // "hydra.launcher.queue=DSML",
                // "hydra.launcher.template=DSML",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:30:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX6000-24GB (CUDA queue) or an GTX2080-8GB (DSML queue)
                //
                // >>>> END: Hydra options
                // "data=iclr_2024_submissions_train,multiwoz21_train,one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                // "data=one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                "data=iclr_2024_submissions_train",
                // "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.number_of_samples=10000",
                // Note: No sampling seed for take_first sampling: "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                // Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=2800",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "compare_mean_local_estimates_with_mean_losses.py - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/comparisons/compare_mean_estimates_over_different_datasets_and_models/compare_mean_local_estimates_with_mean_losses.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                "data=multiwoz21_validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Feature flags
            ]
        },
        {
            "name": "run_iterate_over_twonn_results_and_compare_hausdorff_distances.py; on example directory",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_local_estimates/run_iterate_over_twonn_results_and_compare_hausdorff_distances.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42,43",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=5000",
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True", // This is an example for setting the feature flags
            ]
        },
        {
            "name": "gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/google_cloud/gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "console": "integratedTerminal",
            "args": [
                "--source",
                "bucket",
                "--target",
                "local",
                "--subdirectory",
                "data/analysis/twonn",
                "hydra_output_dir",
                "--dry_run",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; multiple data lists",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                "--run-option",
                "dry_run",
                "--run-only-selected-configs-option",
                "run_all",
                // "run_single_random",
                "--submission-mode",
                "local",
                // >> Concrete experiment configurations
                // "--data-list-options",
                // "reddit_only",
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "wikitext_only"
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, multiple layers, for data validation splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML", # Note: The GPT-2-medium embedding computation does not run on the 8 GB GPUs
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_multiple_layers_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "iclr_validation_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                "--data-list-options",
                "reddit_validation_only",
                "--data-list-options",
                "sgd_validation_only",
                "--data-list-options",
                "wikitext_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                //"--model-group-options",
                // "gpt2_medium_without_modifications",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_checkpoints_1200_1600",
                "--model-group-options",
                "gpt2_medium_finetuned_for_few_epochs_wikitext_data_single_seed_checkpoints_1200_1600",
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, last layer, single sample",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_without_modifications",
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for random subsample of selected data splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                //
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                // "--experiment-selector-options",
                // "masked_token_embeddings_last_layer_two_data_subsampling_sampling_seeds",
                //
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //
                //    >> BEGIN Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "validation_split_only",
                //
                // "--data-list-options",
                // "iclr_validation_only",
                // "--data-list-options",
                // "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_validation_only",
                // "--data-list-options",
                // "sgd_validation_only",
                // "--data-list-options",
                // "wikitext_validation_only",
                //
                // "--data-list-options",
                // "iclr_test_only",
                // "--data-list-options",
                // "multiwoz21_test_only",
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "sgd_test_only",
                // "--data-list-options",
                // "wikitext_test_only",
                //
                //    >> END Data list options.
                //
                //    >> BEGIN Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_all_checkpoints_step_100",
                "--model-group-options",
                "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_frozen_lm_head_all_checkpoints_step_100",
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for take_first sample of data train splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000", // <-- Change this back to "RTX6000" when the RTX6000 is available again
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "iclr_train_only",
                // "--data-list-options",
                // "multiwoz21_train_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "sgd_train_only",
                // "--data-list-options",
                // "wikitext_train_only",
                //
                //    >> END Data list options.
                "--data-subsampling-sampling-mode",
                "take_first",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                "--model-group-options",
                "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity analysis multiwoz21/reddit different data subsampling number of samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_multiwoz21_different_data_subsampling_number_of_samples",
                "--experiment-selector-options",
                "sensitivity_analysis_reddit_different_data_subsampling_number_of_samples",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                // "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> NOTE: These experiments set the data list option themselves.
                //    >>       Make sure that the default data list option is a single entry, otherwise this script might get called multiple times.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
                "--experiment-stage",
                // "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "reddit_only",
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "reddit_validation_only",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors for single data split",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "local",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors",
                "--experiment-stage",
                "skip_compute_embeddings_but_do_multiple_pipeline_runs",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                "--data-list-options",
                // "multiwoz21_validation_and_reddit_validation",
                "reddit_validation_only",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications",
                //    >> END Model group options.
            ]
        },
        {
            "name": "Load model and log model info - Local debugging - RoBERTa, ContextBERT-ERToD, SetSUMBT, Trippy;",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/inspection/load_model_and_log_model_info.py",
            "console": "integratedTerminal",
            "args": [
                //   >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                //   >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                // "language_model=bert-base-uncased",
                "language_model=bert-base-uncased-SentiX",
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.seed=0",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                // "++language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585",
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=7098",
                // "++language_model.checkpoint_no=10647",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - create_line_plots_of_local_estimates_over_checkpoints.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/create_line_plots_of_local_estimates_over_checkpoints.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Create distribution plots of local estimates over checkpoints and over layers",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/run_create_distribution_plots_of_local_estimates_over_checkpoints_and_over_layers.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                //    >> END: Model options.
                //
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_checkpoints=True", // Note: Change this to "True" to create the distribution plots
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_layers=True",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Parse SetSUMBT run JSONL file and plot performance metrics",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/setsumbt/run_parse_setsumbt_run_jsonl_and_plot_performance_metrics.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                "language_model.seed=0,1,2",
                "language_model.checkpoint_no=2813", // Note: The checkpoint_no should not matter for this script.
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Load dataloaders - Local debugging - try_out_load_dataloader_processed.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/setsumbt/try_out_load_dataloader_processed.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Analyse local estimate distribution over tokens - Local debugging - ",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/distribution_over_tokens/analyse_local_estimate_distribution_over_tokens.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // ===== Local =====
                // "hydra/launcher=basic",
                // ===== HHU Hilbert CPU-machine =====
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=DEFAULT",
                "hydra.launcher.template=CPU",
                // Note: There is a problem with the CPU template here when setting memory=16 and ncpus=2
                // (in that case, CPUs and memory are taken from the template, not from the extra arguments).
                // As a workaround, we do not choose these problematic values here.
                "hydra.launcher.memory=17",
                "hydra.launcher.ncpus=3",
                "hydra.launcher.ngpus=0",
                "hydra.launcher.walltime=00:15:00", // <-- The plotting script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // >>>> ERC datasets:
                // "data=ertod_emowoz",
                // "data.dataset_seed=42",
                // "data.dataset_seed=43,44",
                // "data.dataset_seed=42,43,44",
                // "data.dataset_seed=50",
                // "data.dataset_seed=50,51,52",
                //
                // >>>> SetSUMBT datasets:
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                // "data.data_subsampling.split=dev",
                //
                // >>>> Trippy datasets:
                "data=trippy_dataloaders_processed",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                "data.data_subsampling.split=train,dev,test",
                // "data.data_subsampling.split=train,validation,test",
                //
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=roberta-base,bert-base-uncased,bert-base-uncased-SentiX",
                //
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.seed=42",
                // "language_model.seed=43,44",
                // "language_model.seed=43,44,45",
                // "language_model.seed=50",
                // "language_model.seed=50,51,52",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=1",
                // "language_model.checkpoint_no=0,1,2,3,4",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- locally, we have metadata and local estimates for these checkpoints available for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                // "language_model.checkpoint_no=3549",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
    ]
}