{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": ""
        },
        {
            "name": "Python Debugger: Current File with Arguments",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": "${command:pickArgs}"
        },
        {
            "name": "Python Debugger: Current File with Multirun",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=sgd_test",
                "language_model=roberta-base,roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset",
                "hydra.job.env_set.CUDA_VISIBLE_DEVICES=0",
                "tokenizer.add_prefix_space=True",
            ]
        },
        {
            "name": "skip finetuning; only config file creation; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.finetuning.skip_finetuning=true",
                "feature_flags.finetuning.use_wandb=false",
            ]
        },
        {
            "name": "on NER task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_token_classification",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_finetuning_for_token_classification_DEBUG",
            ]
        },
        {
            "name": "on MLM task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "finetuning=finetuning_for_masked_lm",
                "finetuning.eval_steps=2",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=80",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.num_train_epochs=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "finetuning.seed=1235,1236",
                "feature_flags.wandb.use_wandb=true",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "on CLM task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_causal_lm",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=false",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "run_local_estimates.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Duplicate vectors test case - run_local_estimates.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            // This prepared data leads to the `ValueError: Input X contains NaN.` error in `LinearRegression`.
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.number_of_samples=3000",
                "data.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "+embeddings_data_prep.sampling.sampling_mode=random",
                // "local_estimates.filtering.num_samples=2500",
                "local_estimates.filtering.num_samples=5000",
                // "local_estimates.filtering.deduplication_mode=identity",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                // "local_estimates.pointwise.absolute_n_neighbors=64",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=false"
            ]
        },
        {
            "name": "run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.number_of_samples=512",
                "data.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "+embeddings_data_prep.sampling.sampling_mode=random",
                // "+embeddings_data_prep.sampling.sampling_mode=take_first",
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
            ]
        },
        {
            "name": "Regular Token Embedding; with POS on multiwoz21 - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21",
                "data.number_of_samples=128",
                "data.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "+embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                // "data=one-year-of-tsla-on-reddit_validation",
                // "data=one-year-of-tsla-on-reddit_test",
                "data=multiwoz21_validation",
                // "data=sgd_test",
                // "data.data_splitting.data_splitting_mode=proportions",
                // "data.data_splitting.split_shuffle=true",
                // "data.data_splitting.split_seed=123",
                // "data.data_split.split_seed=null", // 'null' in the Hydra config is mapped to `None` in the Python code, and then no seed is set for the split
                "data.data_subsampling.number_of_samples=128",
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler, with Gaussian noise - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.noise.artificial_noise_mode=gaussian",
                // "local_estimates.noise.artificial_noise_mode=do_nothing",
                "local_estimates.noise.distortion_parameter=0.001",
                "local_estimates.noise.seed=2",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Masked Token Embedding; on multiwoz21, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21_validation",
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Masked Token Embedding; on reddit, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=one-year-of-tsla-on-reddit", // Note: Since the sequences in the reddit dataset are quite long, this takes a long time to run in the masked token mode
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Masked Token Embedding; on wikipedia, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=wikitext-103-v1",
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "Regular Token Embedding; on wikipedia, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=wikitext-103-v1",
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=true",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=true",
            ]
        },
        {
            "name": "run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/perplexity/saved_perplexity_processing/run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "console": "integratedTerminal",
            "args": [
                // "data=multiwoz21",
                "data=iclr_2024_submissions_test",
                // "language_model=roberta-base",
                "language_model=model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "+embeddings_data_prep.sampling.sampling_mode=take_first",
            ]
        },
        {
            "name": "run_general_comparisons.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": []
        },
        {
            "name": "run_general_comparisons.py; without iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; without iteration; only noise analysis",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
                "feature_flags.analysis.compare_sampling_methods.do_noise_analysis=True",
                "feature_flags.analysis.compare_sampling_methods.do_checkpoint_analysis=False",
                "feature_flags.analysis.compare_sampling_methods.do_data_subsampling_number_of_samples_analysis=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; with iteration; but without expensive plots during iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True",
                "feature_flags.analysis.compare_sampling_methods.do_analysis_influence_of_local_estimates_n_neighbors=False",
                "feature_flags.analysis.compare_sampling_methods.do_create_boxplot_of_mean_over_different_sampling_seeds=False"
            ]
        },
        {
            "name": "run_compare_losses_and_local_estimates.py; on example multiwoz21 test data, compare clean with noisy data",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_losses_and_local_estimates/run_compare_losses_and_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=masked_token",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data.local_estimates.noise.artificial_noise_mode=gaussian",
                "+comparison_data.local_estimates.noise.distortion_parameter=0.01",
                "+comparison_data.local_estimates.noise.seed=4",
                // END: Comparison data.
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=600",
                // "analysis.investigate_distances.array_truncation_size=5000",
                "analysis.investigate_distances.array_truncation_size=20000",
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True", // This is an example for setting the feature flags
            ]
        },
        {
            "name": "run_compare_losses_and_local_estimates.py; on example wikitext test data, compare masked token embeddings with regular token embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_losses_and_local_estimates/run_compare_losses_and_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=wikitext-103-v1_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // Option: Take a different token mode for comparison
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True", // This is an example for setting the feature flags
            ]
        },
        {
            "name": "run_compare_losses_and_local_estimates.py; local; small array_truncation_size; for selected datasets and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_losses_and_local_estimates/run_compare_losses_and_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=4",
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=06:00:00",
                // >>>> END: Hydra options
                // "data=multiwoz21_validation",
                "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                // "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "run_compare_losses_and_local_estimates.py; on HHU Hilbert; for multiple datasets and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_losses_and_local_estimates/run_compare_losses_and_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=06:00:00",
                // >>>> END: Hydra options
                // "data=multiwoz21_validation",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // "language_model=roberta-base",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // Note: We have not computed the masked token embeddings for the ICLR-finetuned models yet, so we will skip the loss computation for now.
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // "++language_model.checkpoint_no=400,800,1200,1600,2000",
                "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "run_iterate_over_twonn_results_and_compare_hausdorff_distances.py; on example directory",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_local_estimates/run_iterate_over_twonn_results_and_compare_hausdorff_distances.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42,43",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=5000",
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True", // This is an example for setting the feature flags
            ]
        },
        {
            "name": "gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/google_cloud/gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "console": "integratedTerminal",
            "args": [
                "--source",
                "bucket",
                "--target",
                "local",
                "--subdirectory",
                "data/analysis/twonn",
                "hydra_output_dir",
                "--dry_run",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; multiple data lists",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                "--run-option",
                "dry_run",
                "--run-only-selected-configs-option",
                "run_all",
                // "run_single_random",
                "--submission-mode",
                "local",
                // >> Concrete experiment configurations
                // "--data-list-options",
                // "reddit_only",
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "wikitext_only"
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, multiple layers, for data validation splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_multiple_layers_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                // "--data-list-options",
                // "iclr_validation_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_validation_only",
                // "--data-list-options",
                // "sgd_validation_only",
                // "--data-list-options",
                // "wikitext_validation_only",
                //    >> END Data list options.
                //    >> Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                "--model-group-options",
                "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for data validation splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                // "--data-list-options",
                // "iclr_validation_only",
                // "--data-list-options",
                // "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_validation_only",
                // "--data-list-options",
                // "sgd_validation_only",
                "--data-list-options",
                "wikitext_validation_only",
                //    >> Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                "--model-group-options",
                "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
                "--experiment-stage",
                // "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage
                "skip_compute_embeddings_but_do_multiple_pipeline_runs",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "multiwoz_only",
                // "--data-list-options",
                // "reddit_only",
                "--data-list-options",
                "reddit_test_only",
                "--data-list-options",
                "reddit_train_only",
                "--data-list-options",
                "reddit_validation_only",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors for single data split",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "local",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors",
                "--experiment-stage",
                "skip_compute_embeddings_but_do_multiple_pipeline_runs",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                "--data-list-options",
                // "multiwoz21_validation_and_reddit_validation",
                "reddit_validation_only",
                //    >> Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications",
            ]
        },
    ]
}