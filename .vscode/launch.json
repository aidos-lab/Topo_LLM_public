{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": ""
        },
        {
            "name": "Python Debugger: Current File with Arguments",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": "${command:pickArgs}"
        },
        {
            "name": "Python Debugger: Current File with Multirun",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=sgd_test",
                "language_model=roberta-base,roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset",
                "hydra.job.env_set.CUDA_VISIBLE_DEVICES=0",
                "tokenizer.add_prefix_space=True",
            ]
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Huggingface dataloaders; RoBERTa, BERT, SentiX, ContextBERT-ERToD_emowoz, GPT-2; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=wikitext-103-v1",
                // "data.normalization.apply_string_strip=True",
                // "data.normalization.apply_string_strip=False",
                //
                // >>>> ERC datasets
                // "data=ertod_emowoz",
                // "data.dataset_seed=42",
                // "data.use_context=False",
                // "data.dataset_seed=50",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=128",
                // "data.data_subsampling.number_of_samples=512",
                "data.data_subsampling.split=validation",
                //
                // "language_model=roberta-base",
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                "language_model=gpt2", // <-- Smallest GPT-2 model (137M params)
                // "language_model=gpt2-medium",
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.seed=42",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=1",
                // "language_model.checkpoint_no=1,2",
                // "language_model.use_context=True", // <-- This is the debugging model we might have saved locally
                //
                // "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                // "local_estimates=lpca",
                // "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=False", // <-- Only set this to true if the embeddings are available
                "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=False",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Huggingface dataloaders; SetSUMBT, Trippy, Trippy-R checkpoints; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                "data=multiwoz21",
                // "data=sgd",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "language_model.seed=42",
                // "language_model.checkpoint_no=3549",
                //
                "language_model=roberta-base-trippy_r_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3550",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Debugging (Local and Cluster) - TwoNN; LUSTER, Phi-3.5, Phi-4, Gemma-3; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                //
                // >> Local run:
                "hydra/launcher=basic",
                "preferred_torch_backend=cpu", // Locally on a MacBook with 16GB of memory, loading Phi-3.5 on MPS is not possible
                //
                // >> HPC run:
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000", // Note: 12 GB of GPU memory appears to not be enough for the GPT-2 pipeline, i.e., do not select the "hydra.launcher.template=GTX1080"
                // // "hydra.launcher.memory=64", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                // "hydra.launcher.memory=50", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                // "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=04:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                //
                // >>>> Parameters to make sure that memory usage is within limits
                "embeddings.batch_size=2",
                "storage.chunk_size=32",
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                "data=luster",
                "data.column_name=source",
                //
                // "data=multiwoz21",
                // "data=sgd",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=Phi-3.5-mini-instruct",
                // "language_model=Phi-4-mini-instruct",
                //
                // "language_model=gemma-3-1b-pt",
                // "language_model=gemma-3-1b-it",
                //
                // "language_model=luster-base",
                // "language_model=luster-base-emotion",
                // "language_model=luster-chitchat",
                // "language_model=luster-full",
                // "language_model=luster-rl-sent",
                // "language_model=luster-rl-succ",
                //
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Debugging (Local and Cluster) - TwoNN; Llama family; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >> Local run
                "hydra/launcher=basic",
                // >> Locally on a MacBook with 16GB of memory, loading "Llama-3.1-8B" on MPS is not possible.
                // "preferred_torch_backend=cpu",
                "preferred_torch_backend=auto",
                // >> HPC run
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000", // Note: TODO GB of GPU memory appears to not be enough for the TODO pipeline, i.e., do not select the "hydra.launcher.template=GTX1080"
                // "hydra.launcher.memory=64", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                // "hydra.launcher.memory=50", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                // "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=04:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                //
                // >>>> Parameters to make sure that memory usage is within limits
                "embeddings.batch_size=2",
                "storage.chunk_size=32",
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                "data=multiwoz21",
                // "data=sgd",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=Llama-3.2-1B",
                // "language_model=Llama-3.2-3B",
                // "language_model=Llama-3.1-8B",
                //
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "embeddings_data_prep.sampling.sampling_mode=take_first",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Stored dataloaders: SetSUMBT-dataloaders, Trippy-dataloaders, Trippy-R-dataloaders; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                //
                // "data=trippy_dataloaders_processed",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                //
                "data=trippy_r_dataloaders_processed",
                "data.data_subsampling.split=train",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439", // <-- locally, we have these checkpoints available for "language_model.seed=0"
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "language_model.seed=42",
                // "language_model.checkpoint_no=3549",
                //
                // "language_model=roberta-base-trippy_r_multiwoz21_short_runs",
                // "language_model.seed=42",
                // "language_model.checkpoint_no=3550",
                //
                "language_model=roberta-base-trippy_r_multiwoz21_long_runs",
                "language_model.seed=1111",
                "language_model.checkpoint_no=1775",
                "language_model.lr_scheduler_type=constant_schedule_with_warmup",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - TwoNN; Huggingface dataloaders Pre-tokenized with BIO-tags - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // Note: This should be compatible with the pre-tokenization
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21_with_bio_tags",
                "data=sgd_with_bio_tags",
                //
                "data.data_subsampling.split=validation",
                //"data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                //
                "local_estimates=twonn",
                //
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - Local debugging - lPCA; Stored dataloaders, SetSUMBT-dataloaders; - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                "data=setsumbt_dataloaders_processed",
                "data.dataset_seed=0",
                // "data.data_subsampling.split=train",
                "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                "language_model.seed=0",
                // "language_model.seed=1",
                "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439", // <-- locally, we have these checkpoints available for "language_model.seed=0"
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        // Notes: 
        // - Even though the documentation of the HPC cluster claims that the maximum duration of a GPU-Job is 47:59:59, 
        //   we can sucessfully submit jobs with a duration of 59:00:00.
        //   On the DSML queue, we can even start jobs with a duration of 72:00:00.
        // - The GTX1080TI and RTX6000 nodes have 20 CPU-Cores and 10 GPUs each, 
        //   thus per GPU you should not use more than 2 CPUs.
        // - For the gpt2-medium embeddings, the 12GB GPUs are not enough
        {
            "name": "Run pipeline - HPC cluster submission - TwoNN - RoBERTa, BERT, ContextBERT-ERToD, SetSUMBT, Trippy, Trippy-R checkpoints; last layer only - Fixed subsampling seed - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                // "hydra.launcher.template=GTX1080",
                "hydra.launcher.memory=32", // <-- 32GB of RAM is enough for the Trippy-R pipeline runs to succeed, but we should check whether we can decrease this
                // "hydra.launcher.memory=16", // 16 GB of RAM also appears to work for the Trippy-R pipeline runs, but is not enough for the ERC pipeline runs
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:30:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                "hydra.launcher.walltime=02:30:00", // <-- Note: Since the startup of the computation sometimes takes a long time, we set a longer walltime.
                // "hydra.launcher.walltime=08:00:00", // <-- For the masked version, take a longer walltime
                //    >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // >> Single dataset:
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=wikitext-103-v1",
                // "data=multiwoz21_with_bio_tags",
                // "data=sgd_with_bio_tags",
                //
                // >> Multiple datasets:
                // "data=multiwoz21,wikitext-103-v1,one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit,wikitext-103-v1,iclr_2024_submissions",
                "data=multiwoz21_with_bio_tags,sgd_with_bio_tags",
                //
                // >>>> EmoLoop ERC datasets
                // >> Note: The splits for the ERC datasets are called: train,validation,test
                // "data=ertod_emowoz",
                // "data.data_subsampling.split=train,validation,test",
                // "data.dataset_seed=50",
                // "data.dataset_seed=42",
                // "data.dataset_seed=43,44",
                // "data.dataset_seed=50,51,52",
                //
                // >>>> SetSUMBT datasets
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                //
                // >>>> Trippy datasets
                // >> Note: The splits for Trippy-R are called: train,dev,test
                // "data=trippy_dataloaders_processed",
                // "data.data_subsampling.split=train,dev,test",
                //
                // >>>> Trippy-R datasets
                // >> Note: The splits for Trippy-R are called: train,dev,test
                // "data=trippy_r_dataloaders_processed",
                // "data.data_subsampling.split=train,dev,test",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,test",
                // "data.data_subsampling.split=train,dev,test",
                "data.data_subsampling.split=train,validation,test",
                //
                "data.data_subsampling.sampling_mode=random",
                // "data.data_subsampling.sampling_mode=take_first,random",
                // "data.data_subsampling.number_of_samples=7000",
                "data.data_subsampling.number_of_samples=10000",
                // "data.data_subsampling.number_of_samples=7000,10000",
                // 
                "data.data_subsampling.sampling_seed=778",
                // "data.data_subsampling.sampling_seed=778,779,780",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // >>>> Base models 
                // "language_model=roberta-base",
                // "language_model=bert-base-uncased",
                // "language_model=gpt2-medium",
                //
                // "++language_model.checkpoint_no=-1", // <-- checkpoint_no not necessary for the base models, setting it to '-1' to skip unnecessary iterations over different choices
                //
                // >>>> Fine-tuned models (Masked)
                // >> Individual:
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // >> Multiple:
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                //
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                "++language_model.checkpoint_no=2800",
                //
                // >>>> ERC models
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=roberta-base,bert-base-uncased,bert-base-uncased-SentiX",
                //
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.seed=50", // Note: Seed 50 already submitted.
                // "language_model.seed=51",
                // "language_model.seed=52,53,54",
                // "language_model.seed=50,51,52,53,54",
                //
                // >> Note: For the ERC models the checkpoints are given by the epoch numbers
                // "language_model.num_train_epochs=5",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=0,1,2,3,4",
                //
                // "language_model.num_train_epochs=50",
                // "language_model.checkpoint_no=0",
                // Note: You can generate the comma-separated list of checkpoints for the long ERC runs with the following python command:
                // >>> ','.join(list(map(str, range(0,50,1))))
                // "language_model.checkpoint_no=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49",
                //
                // >>>> SetSUMBT models
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- checkpoints for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,30943,42195,47821,53447,61886,109707", // <-- checkpoints for "language_model.seed=1"
                // "language_model.seed=2",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,22504,28130,30943,33756,42195,50634,67512,106894,126585", // <-- checkpoints for "language_model.seed=2"
                //
                // >>>> Trippy models
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=3549,7098",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                // >>>> Trippy-R models
                //
                // >> Short Trippy-R runs
                // "language_model=roberta-base-trippy_r_multiwoz21_short_runs",
                // >> Note: The following line contains all the checkpoints available for a full short Trippy-R training run:
                // "language_model.checkpoint_no=1775,3550,5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                // "language_model.checkpoint_no=1775,3550",
                // "language_model.checkpoint_no=5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                //
                // >> Long Trippy-R runs
                // "language_model=roberta-base-trippy_r_multiwoz21_long_runs",
                // "language_model.lr_scheduler_type=constant_schedule_with_warmup,linear_schedule_with_warmup",
                // "language_model.lr_scheduler_type=linear_schedule_with_warmup",
                // Note: You can generate the comma-separated list of checkpoints for the long Trippy-R runs with the following python command:
                // >>> ','.join(list(map(str, range(1775,88751,1775))))
                // "language_model.checkpoint_no=1775,3550,5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500,37275,39050,40825,42600,44375,46150,47925,49700,51475,53250,55025,56800,58575,60350,62125,63900,65675,67450,69225,71000,72775,74550,76325,78100,79875,81650,83425,85200,86975,88750",
                // "language_model.checkpoint_no=1775,3550",
                // "language_model.checkpoint_no=5325",
                // "language_model.checkpoint_no=7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500,37275,39050,40825,42600,44375,46150,47925,49700,51475,53250,55025,56800,58575,60350,62125,63900,65675,67450,69225,71000,72775,74550,76325,78100,79875,81650,83425,85200,86975,88750",
                //
                // "language_model.seed=1111",
                // "language_model.seed=40",
                // "language_model.seed=41",
                // "language_model.seed=42", // Note: seed=42 is already submitted
                // "language_model.seed=43",
                // "language_model.seed=43,44",
                // "language_model.seed=42,43,44",
                // "language_model.seed=1111,40,41",
                //
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                // "embeddings.embedding_data_handler.mode=masked_token",
                // "embeddings.embedding_data_handler.mode=regular,masked_token",
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HPC cluster submission - TwoNN - Autoregressive: GPT-2; last layer only - Fixed subsampling seed - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000", // Note: 12 GB of GPU memory appears to not be enough for the GPT-2 pipeline, i.e., do not select the "hydra.launcher.template=GTX1080"
                // "hydra.launcher.memory=64", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                "hydra.launcher.memory=50", // <-- The embeddings data prep step failed with 32GB of memory for the GPT2 medium model. 
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:30:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                "hydra.launcher.walltime=02:30:00", // <-- Note: Since the startup of the computation sometimes takes a long time, we set a longer walltime.
                //    >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // >> Single dataset:
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=wikitext-103-v1",
                // >> Multiple datasets:
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit,wikitext-103-v1,iclr_2024_submissions",
                "data=multiwoz21,sgd,one-year-of-tsla-on-reddit,wikitext-103-v1,iclr_2024_submissions",
                //
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,test",
                // "data.data_subsampling.split=train,dev,test",
                "data.data_subsampling.split=train,validation,test",
                //
                // "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_mode=take_first,random",
                // "data.data_subsampling.number_of_samples=7000",
                "data.data_subsampling.number_of_samples=10000",
                // "data.data_subsampling.number_of_samples=7000,10000",
                // 
                "data.data_subsampling.sampling_seed=778",
                // "data.data_subsampling.sampling_seed=778,779,780",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // >>>> Base models (Autoregressive)
                // "language_model=gpt2-medium",
                //
                // "++language_model.checkpoint_no=-1", // <-- checkpoint_no not necessary for the base models, setting it to '-1' to skip unnecessary iterations over different choices
                //
                // >>>> Fine-tuned models (Autoregressive)
                // >> Individual:
                // "language_model=gpt2-medium-causal_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_sgd-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // >> Multiple:
                "language_model=gpt2-medium-causal_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_sgd-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                //
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                "++language_model.checkpoint_no=1200",
                // "++language_model.checkpoint_no=2800",
                //
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular", // 'masked_token' does not make sense for autoregressive models
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HPC cluster submission - TwoNN - Autoregressive: Phi-3.5-mini-instruct, Phi-4-mini-instruct; last layer only - Fixed subsampling seed - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                // ===== CPU-machine =====
                // Note: The argument "hydra.launcher.queue=DEFAULT" is problematic, so we skip it for now.
                // "hydra.launcher.template=CPU",
                // "hydra.launcher.ngpus=0",
                // "hydra.launcher.memory=129", //  
                // "hydra.launcher.ncpus=3", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // ===== GPU-machine =====
                // Note: 16 GB of GPU memory appears to not be enough for the Phi-3.5-mini-instruct or Phi-4-mini-instruct pipeline, 
                // i.e., do NOT select: "hydra.launcher.template=GTX1080", "hydra.launcher.template=TESLAT4"
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.ngpus=1",
                // Note:
                // - For high dimensional embeddings, 
                //   there is a problem with not enough memory in the embeddings data prep step
                // "hydra.launcher.memory=50", // <-- The embeddings data prep step FAILED with 50GB of memory for the Phi-4 model. 
                "hydra.launcher.memory=64", //  
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // Global hydra launcher options:
                "hydra.launcher.walltime=10:00:00", // <-- Note: Longer time for larger hidden dimension.
                //    >> END: Hydra options
                //
                // >>>> Parameters to make sure that memory usage is within limits
                "embeddings.batch_size=4",
                "storage.chunk_size=32",
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                // >> Single dataset:
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=wikitext-103-v1",
                // >> Multiple datasets:
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit,wikitext-103-v1,iclr_2024_submissions",
                "data=multiwoz21,sgd,one-year-of-tsla-on-reddit,wikitext-103-v1,iclr_2024_submissions",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,test",
                // "data.data_subsampling.split=train,dev,test",
                // "data.data_subsampling.split=train,validation,test",
                //
                "data.data_subsampling.sampling_mode=random",
                // "data.data_subsampling.sampling_mode=take_first,random",
                // "data.data_subsampling.number_of_samples=7000",
                "data.data_subsampling.number_of_samples=10000",
                // "data.data_subsampling.number_of_samples=7000,10000",
                // 
                "data.data_subsampling.sampling_seed=778",
                // "data.data_subsampling.sampling_seed=778,779,780",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // >>>> Base models (Autoregressive)
                //
                // "language_model=gpt2-medium",
                "language_model=Phi-3.5-mini-instruct",
                // "language_model=Phi-4-mini-instruct",
                //
                "++language_model.checkpoint_no=-1", // <-- checkpoint_no not necessary for the base models, setting it to '-1' to skip unnecessary iterations over different choices
                //
                // >>>> Fine-tuned models (Autoregressive)
                //
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=1200",
                // "++language_model.checkpoint_no=2800",
                //
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular", // 'masked_token' does not make sense for autoregressive models
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                // NOTE: Only skip the embedding computation if the embeddings are already available.
                "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=False", // <-- Only set this to true if the embeddings are available
                // Note: We skip the embeddings computation while debugging the memory problem
                // "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=True", // <-- Only set this to true if the embeddings are available
                // "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=False",
                "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=True",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False",
                // "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HPC cluster submission - TwoNN - Different subsampling seed for each model seed - RoBERTa, BERT, GPT-2, ContextBERT-ERToD, SetSUMBT, Trippy, Trippy-R checkpoints; last layer only - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.template=GTX1080",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=01:30:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                //    >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                // >>>> Language modeling datasets
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // >>>> EmoLoop ERC datasets
                // "data=ertod_emowoz",
                // "data.dataset_seed=42",
                // "data.dataset_seed=43,44",
                // "data.dataset_seed=50,51,52",
                //
                // >>>> SetSUMBT datasets
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                //
                // >>>> TripPy datasets
                // >> Note: The splits for TripPy are called: train,dev,test
                // "data=trippy_dataloaders_processed",
                //
                // >>>> TripPy-R datasets.
                // >> Note: The splits for TripPy-R are called: train,dev,test
                "data=trippy_r_dataloaders_processed",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,test",
                "data.data_subsampling.split=train,dev,test",
                // "data.data_subsampling.split=train,validation,test",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=7000",
                // "data.data_subsampling.number_of_samples=10000",
                // "data.data_subsampling.number_of_samples=7000,10000",
                // 
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=roberta-base,bert-base-uncased,bert-base-uncased-SentiX",
                //
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=0,1,2,3,4",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- checkpoints for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,30943,42195,47821,53447,61886,109707", // <-- checkpoints for "language_model.seed=1"
                // "language_model.seed=2",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,22504,28130,30943,33756,42195,50634,67512,106894,126585", // <-- checkpoints for "language_model.seed=2"
                //
                // >>>> Trippy models
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=3549,7098",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                // >>>> Trippy-R models
                "language_model=roberta-base-trippy_r_multiwoz21",
                //
                // "language_model.checkpoint_no=1775",
                // "language_model.checkpoint_no=1775,3550",
                // "language_model.checkpoint_no=5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                // >> Note: The following line contains all the checkpoints available for a full Trippy-R training run:
                "language_model.checkpoint_no=1775,3550,5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                //
                //    >> END: Model options.
                //
                //    >> START: Seed selection.
                // 
                // >!!!> START NOTE: 
                // >!!!> - When running locally, do not escape the $ sign
                // >!!!> - There used to be a problem when running via 'hpc run' 
                // >!!!>   (we did not escape the $ sign, so that it is was evaluated in the bash script, which lead to empty arguments)
                //
                // >> GOAL: 
                // >> The omegaConf value extrapolation should fill in the gobal seed for the other seeds.
                // >> For example, for the global_seed=44, the following should be filled. 
                //
                // >> "global_seed=44",
                // >> "language_model.seed=44",
                // >> "data.data_subsampling.sampling_seed=44",
                // >> "embeddings_data_prep.sampling.seed=44",
                //
                "global_seed=1111,40,41",
                // "global_seed=40,41",
                // "global_seed=42,43,44",
                //
                "language_model.seed=${global_seed}",
                "data.data_subsampling.sampling_seed=${global_seed}",
                "embeddings_data_prep.sampling.seed=${global_seed}",
                // >!!!> END NOTE
                //
                //    >> END: Seed selection.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HPC cluster submission - lPCA; SetSUMBT or Trippy checkpoints; last layer only - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.walltime=03:00:00", // <-- Until we have a better estimate of how long the lPCA estimates take, we will set the walltime to a larger number of hours.
                // ===== GPU-machine =====
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // ===== CPU-machine =====
                "hydra.launcher.queue=DEFAULT",
                "hydra.launcher.template=CPU",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=6",
                "hydra.launcher.ngpus=0",
                // >>>> END: Hydra options
                //
                //    >> START: Data options.
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                //
                // "data.data_subsampling.split=train",
                "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=dev",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,dev,test",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- checkpoints for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,30943,42195,47821,53447,61886,109707", // <-- checkpoints for "language_model.seed=1"
                // "language_model.seed=2",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,22504,28130,30943,33756,42195,50634,67512,106894,126585", // <-- checkpoints for "language_model.seed=2"
                // "language_model.checkpoint_no=2813",
                // "language_model.checkpoint_no=2813,5626,8439",
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                "language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490", // <-- checkpoints for "language_model.seed=42"
                // "language_model.checkpoint_no=3549",
                // "language_model.checkpoint_no=3549,7098",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                // "local_estimates.estimator.lpca_ver=Fan",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                //    >> NOTE: Only skip the embedding computation if the embeddings are already available.
                "feature_flags.compute_and_store_embeddings.skip_compute_and_store_embeddings_in_pipeline=True", // <-- Only set this to true if the embeddings are available
                "feature_flags.embeddings_data_prep.skip_embeddings_data_prep_in_pipeline=False",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run pipeline - HPC cluster submission - TwoNN; SetSUMBT or Trippy selected checkpoints; multiple layers - Run pipeline.",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.template=GTX1080",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Data options.
                //
                "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                "data.data_subsampling.split=validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,16878",
                // "++language_model.checkpoint_no=5626,8439,11252,14065",
                "++language_model.checkpoint_no=14065,19691,25317,33756,36569,39382,42195,50634,56260",
                // "++language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585",
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=3549,7098",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1],[-2],[-3],[-4],[-5],[-6],[-7],[-8],[-9],[-10],[-11],[-12]", // <-- Compute estimates for all layers
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False", // <-- Decide whether to create plots in the local estimates worker.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Run local estimates - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
            ]
        },
        {
            "name": "Run local estimates: Duplicate vectors test case - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/local_estimates_computation/run_local_estimates.py",
            "console": "integratedTerminal",
            // This prepared data leads to the `ValueError: Input X contains NaN.` error in `LinearRegression`.
            "args": [
                "tokenizer.add_prefix_space=True", // `add_prefix_space=True` is necessary to use the tokenizer with input split into words
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=3000",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                // "local_estimates.filtering.num_samples=2500",
                "local_estimates.filtering.num_samples=5000",
                // "local_estimates.filtering.deduplication_mode=identity",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.relative_n_neighbors=0.3",
                // "local_estimates.pointwise.absolute_n_neighbors=64",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=False"
            ]
        },
        {
            "name": "Load huggingface dataset - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/data_processing/load_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                // "data=bookcorpus",
                // "data=multiwoz21",
                "data=wikitext-103-v1",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_DEBUG",
            ]
        },
        {
            "name": "Compute eval loss via huggingface transformers Trainer - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/compute_eval_loss_via_huggingface_trainer.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21",
                // "data=one-year-of-tsla-on-reddit",
                // "data.data_subsampling.number_of_samples=512",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                //
                "data.data_subsampling.number_of_samples=128", // <-- This can be used for quick testing of the script
                "data.data_subsampling.split=validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=False" to be consistent with the fine-tuning setup
                //
                "language_model=roberta-base",
                // "language_model=gpt2-medium",
                //
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_DEBUG",
            ],
            "env": {}
        },
        {
            "name": "Compute eval loss via huggingface transformers Trainer - HPC cluster submission -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/compute_eval_loss_via_huggingface_trainer.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:30:00", // <-- The evaluation should not take more than a few minutes
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21",
                // "data=one-year-of-tsla-on-reddit",
                // "data=iclr_2024_submissions_train,multiwoz21_train,one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train", // <-- all training splits
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                //
                // "data.data_subsampling.number_of_samples=128", // <-- This can be used for quick testing of the script
                "data.data_subsampling.number_of_samples=10000",
                // No data split selected here, since we will do this above in the data configuration
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.sampling_seed=778",
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=False" to be consistent with the fine-tuning setup
                //
                //    >> START Model options.
                //    > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // 
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head_embeddings.word_embeddings-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-5",
                "++language_model.checkpoint_no=100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                //    >> END Model options.
                //
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_DEBUG",
            ],
            "env": {}
        },
        {
            "name": "Run inference pipeline - Local debugging",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/run_inference_pipeline.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                //
                "preferred_torch_backend=auto",
                //
                //    >> START Model options.
                //
                // "language_model=gpt2",
                //
                // "language_model=gemma-3-1b-pt",
                // "language_model=gemma-3-1b-it",
                // "language_model=gemma-3-1b-pt,gemma-3-1b-it",
                //
                "language_model=Llama-3.2-1B",
                //
                //    >> END Model options.
                //
                // "inference.prompts=null", // Use default prompts
                "inference.prompts=['The hotel should be in the', 'I would like to invest in', 'The Eiffel tower is located in', 'Eigenspaces corresponding to distinct eigenvalues are']",
                //
                // Notes: 
                // - For causal language modeling, the sampling in inference might depend on the global_seed.
                // "global_seed=1111",
                "global_seed=1111,1112",
                //
                "inference.include_timestamp_in_filename=True",
                //
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_DEBUG",
            ],
            "env": {
                // The operator 'aten::linalg_cholesky_ex.L' is not currently implemented for the MPS device. 
                // As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. 
                // WARNING: this will be slower than running natively on MPS.
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Fine-tuning - skip finetuning; only config file creation; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.finetuning.skip_finetuning=True",
                "feature_flags.finetuning.use_wandb=False",
            ]
        },
        {
            "name": "Fine-tuning - on NER task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_token_classification",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_finetuning_for_token_classification_DEBUG",
            ]
        },
        {
            "name": "Fine-tuning - Local debugging - on MLM task; for 2 epochs",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "finetuning=finetuning_for_masked_lm",
                // <-- Saving every 2 steps should result in two saved checkpoints for a dataset of size 80 and batch size of: "Instantaneous batch size per device = 16"
                "finetuning.save_steps=2",
                "finetuning.eval_steps=2",
                "finetuning/finetuning_datasets=train_and_eval_on_one-year-of-tsla-on-reddit_train-samples-small",
                // "finetuning/finetuning_datasets=train_and_eval_on_wikitext_train-samples-small",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=80",
                // "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_mode=take_first",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_mode=random",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_seed=112",
                // "finetuning.finetuning_datasets.train_dataset.data_subsampling.sampling_seed=113",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.num_train_epochs=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "finetuning.seed=1235",
                // "finetuning.seed=1235,1236",
                // >> START Optional: Change gradient modifier
                "finetuning/gradient_modifier=do_nothing",
                // "finetuning/gradient_modifier=freeze_lm_head_bert-style-models",
                // "finetuning/gradient_modifier=freeze_lm_head_and_word_embeddings_bert-style-models",
                // >> END Optional: Change gradient modifier
                "feature_flags.wandb.use_wandb=True",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "Fine-tuning - on CLM task; run_finetune_language_model_on_huggingface_dataset.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py",
            "console": "integratedTerminal",
            "args": [
                "finetuning=finetuning_for_causal_lm",
                "finetuning.finetuning_datasets.train_dataset.data_subsampling.number_of_samples=100",
                "finetuning.finetuning_datasets.eval_dataset.data_subsampling.number_of_samples=20",
                "finetuning.eval_steps=2",
                "finetuning.trainer_modifier.mode=add_wandb_prediction_progress_callback",
                "finetuning.trainer_modifier.frequency=4",
                "feature_flags.wandb.use_wandb=False",
                "wandb.project=Topo_LLM_finetuning_for_language_modeling_DEBUG",
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                // "data=one-year-of-tsla-on-reddit_validation",
                // "data=one-year-of-tsla-on-reddit_test",
                "data=multiwoz21_validation",
                // "data=sgd_test",
                // "data.data_splitting.data_splitting_mode=proportions",
                // "data.data_splitting.split_shuffle=True",
                // "data.data_splitting.split_seed=123",
                // "data.data_split.split_seed=null", // 'null' in the Hydra config is mapped to `None` in the Python code, and then no seed is set for the split
                "data.data_subsampling.number_of_samples=128",
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler, with Gaussian noise - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.noise.artificial_noise_mode=gaussian",
                // "local_estimates.noise.artificial_noise_mode=do_nothing",
                "local_estimates.noise.distortion_parameter=0.001",
                "local_estimates.noise.seed=2",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Masked Token Embedding; on reddit, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=one-year-of-tsla-on-reddit", // Note: Since the sequences in the reddit dataset are quite long, this takes a long time to run in the masked token mode
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
            ]
        },
        {
            "name": "run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/model_inference/perplexity/saved_perplexity_processing/run_single_setup_load_saved_perplexity_and_local_estimates_and_analyse.py",
            "console": "integratedTerminal",
            "args": [
                // "data=multiwoz21",
                "data=iclr_2024_submissions_test",
                // "language_model=roberta-base",
                "language_model=model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=30000",
                "embeddings_data_prep.sampling.sampling_mode=take_first",
            ]
        },
        {
            "name": "run_general_comparisons.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": []
        },
        {
            "name": "run_general_comparisons.py; without iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; without iteration; only noise analysis",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=False",
                "feature_flags.analysis.compare_sampling_methods.do_noise_analysis=True",
                "feature_flags.analysis.compare_sampling_methods.do_checkpoint_analysis=False",
                "feature_flags.analysis.compare_sampling_methods.do_data_subsampling_number_of_samples_analysis=False",
            ]
        },
        {
            "name": "run_general_comparisons.py; with iteration; but without expensive plots during iteration",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_sampling_methods/run_general_comparisons.py",
            "console": "integratedTerminal",
            "args": [
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True",
                "feature_flags.analysis.compare_sampling_methods.do_analysis_influence_of_local_estimates_n_neighbors=False",
                "feature_flags.analysis.compare_sampling_methods.do_create_boxplot_of_mean_over_different_sampling_seeds=False"
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example multiwoz21 test data, compare clean with noisy data",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=masked_token",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data.local_estimates.noise.artificial_noise_mode=gaussian",
                "+comparison_data.local_estimates.noise.distortion_parameter=0.01",
                "+comparison_data.local_estimates.noise.seed=4",
                //
                //    >> END: Comparison data.
                //
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=600",
                // "analysis.investigate_distances.array_truncation_size=5000",
                "analysis.investigate_distances.array_truncation_size=20000",
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example wikitext test data, compare masked token embeddings with regular token embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=wikitext-103-v1_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // Option: Take a different token mode for comparison
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - Local debugging - small array_truncation_size; for selected datasets and models; with comparison of local estimates with losses, but without comparison with other data (i.e., loss computation only on base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //    >> START Data options.
                "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=True",
                // >>>> Set this feature flag to avoid the comparison with other data
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=False",
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (no loss computation) - Local debugging - small array_truncation_size; with comparison of local estimates between settings (but without loss computation)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //
                //    >> START Data options.
                //
                "tokenizer.add_prefix_space=False",
                //
                // "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //
                "data=setsumbt_dataloaders_processed",
                "data.dataset_seed=0",
                "data.data_subsampling.split=dev",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //    >> END Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-setsumbt_multiwoz21",
                //
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data/local_estimates=twonn",
                //
                //    >> END: Comparison data.
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=False", // <-- Set this feature flag to avoid the predictions and loss computation
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=True", // <-- Set this feature flag to compare with different local estimates
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for validation or test splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:20:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21_validation", // <-- only multiwoz21_validation
                // "data=iclr_2024_submissions_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits except multiwoz21_validation
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                //    >> START Model options.
                //    > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // 
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head_embeddings.word_embeddings-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-5",
                "++language_model.checkpoint_no=100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                //    >> END Model options.
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for different subsampling seeds of validation splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=02:00:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                // "data=multiwoz21_validation",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779", // <-- Different choices of data_subsampling.sampling_seed here
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                //
                // > Note: Remember to set checkpoint_no for the fine-tuned models!
                //
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for multiple train datasets (take first sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                //
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                // "hydra.launcher.queue=DSML",
                // "hydra.launcher.template=DSML",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:30:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX6000-24GB (CUDA queue) or an GTX2080-8GB (DSML queue)
                //
                // >>>> END: Hydra options
                // "data=iclr_2024_submissions_train,multiwoz21_train,one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                // "data=one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                "data=iclr_2024_submissions_train",
                // "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.number_of_samples=10000",
                // Note: No sampling seed for take_first sampling: "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                // Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=2800",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False",
            ]
        },
        {
            "name": "compare_mean_local_estimates_with_mean_losses.py - Local debugging -",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/comparisons/compare_mean_estimates_over_different_datasets_and_models/compare_mean_local_estimates_with_mean_losses.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // >>>> END: Hydra options
                "data=multiwoz21_validation",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Feature flags
            ]
        },
        {
            "name": "run_iterate_over_twonn_results_and_compare_hausdorff_distances.py; on example directory",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_local_estimates/run_iterate_over_twonn_results_and_compare_hausdorff_distances.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42,43",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=5000",
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True", // This is an example for setting the feature flags
            ]
        },
        {
            "name": "gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/google_cloud/gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "console": "integratedTerminal",
            "args": [
                "--source",
                "bucket",
                "--target",
                "local",
                "--subdirectory",
                "data/analysis/twonn",
                "hydra_output_dir",
                "--dry_run",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; multiple data lists",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                "--run-option",
                "dry_run",
                "--run-only-selected-configs-option",
                "run_all",
                // "run_single_random",
                "--submission-mode",
                "local",
                // >> Concrete experiment configurations
                // "--data-list-options",
                // "reddit_only",
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "wikitext_only"
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, multiple layers, for data validation splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML", # Note: The GPT-2-medium embedding computation does not run on the 8 GB GPUs
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_multiple_layers_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "iclr_validation_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                "--data-list-options",
                "reddit_validation_only",
                "--data-list-options",
                "sgd_validation_only",
                "--data-list-options",
                "wikitext_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                //"--model-group-options",
                // "gpt2_medium_without_modifications",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_checkpoints_1200_1600",
                "--model-group-options",
                "gpt2_medium_finetuned_for_few_epochs_wikitext_data_single_seed_checkpoints_1200_1600",
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, last layer, single sample",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_without_modifications",
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for random subsample of selected data splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                //
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                // "--experiment-selector-options",
                // "masked_token_embeddings_last_layer_two_data_subsampling_sampling_seeds",
                //
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //
                //    >> BEGIN Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "validation_split_only",
                //
                // "--data-list-options",
                // "iclr_validation_only",
                // "--data-list-options",
                // "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_validation_only",
                // "--data-list-options",
                // "sgd_validation_only",
                // "--data-list-options",
                // "wikitext_validation_only",
                //
                // "--data-list-options",
                // "iclr_test_only",
                // "--data-list-options",
                // "multiwoz21_test_only",
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "sgd_test_only",
                // "--data-list-options",
                // "wikitext_test_only",
                //
                //    >> END Data list options.
                //
                //    >> BEGIN Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_all_checkpoints_step_100",
                "--model-group-options",
                "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_frozen_lm_head_all_checkpoints_step_100",
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for take_first sample of data train splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000", // <-- Change this back to "RTX6000" when the RTX6000 is available again
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "iclr_train_only",
                // "--data-list-options",
                // "multiwoz21_train_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "sgd_train_only",
                // "--data-list-options",
                // "wikitext_train_only",
                //
                //    >> END Data list options.
                "--data-subsampling-sampling-mode",
                "take_first",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                "--model-group-options",
                "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity analysis multiwoz21/reddit different data subsampling number of samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_multiwoz21_different_data_subsampling_number_of_samples",
                "--experiment-selector-options",
                "sensitivity_analysis_reddit_different_data_subsampling_number_of_samples",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                // "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> NOTE: These experiments set the data list option themselves.
                //    >>       Make sure that the default data list option is a single entry, otherwise this script might get called multiple times.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
                "--experiment-stage",
                // "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "reddit_only",
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "reddit_validation_only",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors for single data split",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "local",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors",
                "--experiment-stage",
                "skip_compute_embeddings_but_do_multiple_pipeline_runs",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                "--data-list-options",
                // "multiwoz21_validation_and_reddit_validation",
                "reddit_validation_only",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications",
                //    >> END Model group options.
            ]
        },
        {
            "name": "Load model and log model info - Local debugging - RoBERTa, ContextBERT-ERToD, SetSUMBT, Trippy;",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/inspection/load_model_and_log_model_info.py",
            "console": "integratedTerminal",
            "args": [
                //   >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                //   >> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                // "language_model=bert-base-uncased",
                "language_model=bert-base-uncased-SentiX",
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz",
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "++language_model.seed=0",
                // "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                // "++language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585",
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "++language_model.seed=42",
                // "++language_model.checkpoint_no=3549",
                // "++language_model.checkpoint_no=7098",
                // "++language_model.checkpoint_no=10647",
                // "++language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - create_line_plots_of_local_estimates_over_checkpoints.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/create_line_plots_of_local_estimates_over_checkpoints.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Create plots: distribution of local estimates over checkpoints and over layers",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/run_create_distribution_plots_of_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                //    >> END: Model options.
                //
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_checkpoints=True", // Note: Change this to "True" to create the distribution plots
                "feature_flags.task_performance_analysis.plotting_create_mean_plots_over_model_checkpoints_with_different_seeds=True",
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_layers=True",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Create plots: mean for different seeds over checkpoints - Trippy-R",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/run_create_distribution_plots_of_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Model options.
                //
                // "language_model=roberta-base",
                //
                //    >> END: Model options.
                //
                //    >> START: Plotting options.
                //
                "analysis/task_performance_analysis/plotting=trippy_r_same_data_subsampling",
                // "analysis/task_performance_analysis/plotting=trippy_r_different_sample_for_different_models",
                //
                // "analysis.task_performance_analysis.plotting.publication_ready=False",
                "analysis.task_performance_analysis.plotting.publication_ready=True",
                //
                "analysis.task_performance_analysis.plotting.add_legend=True,False",
                // "analysis.task_performance_analysis.plotting.add_legend=True",
                // "analysis.task_performance_analysis.plotting.add_legend=False",
                //
                "analysis.task_performance_analysis.plotting.maximum_x_value=null",
                //
                //    >> END: Plotting options.
                //
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_checkpoints=False",
                "feature_flags.task_performance_analysis.plotting_create_mean_plots_over_model_checkpoints_with_different_seeds=True",
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_layers=False",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Create plots: mean for different seeds over checkpoints - ERC",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/plotting/run_create_distribution_plots_of_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Plotting options.
                //
                "analysis/task_performance_analysis/plotting=ertod",
                //
                // "analysis.task_performance_analysis.plotting.publication_ready=False",
                "analysis.task_performance_analysis.plotting.publication_ready=True",
                //
                "analysis.task_performance_analysis.plotting.add_legend=True,False",
                // "analysis.task_performance_analysis.plotting.add_legend=True",
                // "analysis.task_performance_analysis.plotting.add_legend=False",
                //
                // "analysis.task_performance_analysis.plotting.maximum_x_value=5",
                "analysis.task_performance_analysis.plotting.maximum_x_value=7",
                //
                //    >> END: Plotting options.
                //
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_checkpoints=False",
                "feature_flags.task_performance_analysis.plotting_create_mean_plots_over_model_checkpoints_with_different_seeds=True",
                "feature_flags.task_performance_analysis.plotting_create_distribution_plots_over_model_layers=False",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Parse SetSUMBT run JSONL file and plot performance metrics",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/setsumbt/run_parse_setsumbt_run_jsonl_and_plot_performance_metrics.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                "language_model.seed=0,1,2",
                "language_model.checkpoint_no=2813", // Note: The checkpoint_no should not matter for this script.
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Task performance analysis - Local debugging - Create plots: Trippy performance metrics",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/trippy/run_plot_trippy_performance_metrics.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False", // "tokenizer.add_prefix_space=True" is necessary to use the tokenizer with input split into words
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-trippy_multiwoz21",
                "language_model.seed=42",
                // "language_model.checkpoint_no=3549", // Note: The checkpoint_no should not matter for this script. 
                //
                //    >> END: Model options.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Load dataloaders - Local debugging - try_out_load_dataloader_processed.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/task_performance_analysis/setsumbt/try_out_load_dataloader_processed.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=01:00:00", // <-- The pipeline run for regular embeddings should only take a few minutes.
                // >>>> END: Hydra options
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=512",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base-setsumbt_multiwoz21",
                "++language_model.checkpoint_no=2813",
                // "++language_model.checkpoint_no=2813,5626,8439",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=500",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Analyse local estimate distribution over tokens - Single HPC cluster submission & Local debugging - ",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/distribution_over_tokens/analyse_local_estimate_distribution_over_tokens.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // ===== Local =====
                "hydra/launcher=basic",
                // ===== HPC cluster CPU-machine =====
                // Notes: 
                // - Do NOT set the DEFAULT queue in the CPU template, as this will lead to problems. "hydra.launcher.queue=DEFAULT"
                // - There used to be a problem with the CPU template here when setting memory=16 and ncpus=2
                //   (in that case, CPUs and memory are taken from the template, not from the extra arguments).
                //   As a workaround, we do not choose these problematic values here.
                //
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.template=CPU",
                // "hydra.launcher.memory=17",
                // "hydra.launcher.ncpus=3",
                // "hydra.launcher.ngpus=0",
                // "hydra.launcher.walltime=00:15:00", // <-- The plotting script should run very quickly.
                //
                // >>>> END: Hydra options
                //
                "tokenizer.add_prefix_space=False",
                //
                //    >> START: Data options.
                //
                // "data=multiwoz21",
                // "data=sgd",
                // "data=one-year-of-tsla-on-reddit",
                // "data=sgd,one-year-of-tsla-on-reddit",
                // "data=multiwoz21,sgd,one-year-of-tsla-on-reddit",
                //
                // >> Data with aligned BIO-tags
                // "data=multiwoz21_with_bio_tags",
                // "data=sgd_with_bio_tags",
                "data=multiwoz21_with_bio_tags,sgd_with_bio_tags",
                //
                // >>>> ERC datasets:
                // "data=ertod_emowoz",
                // "data.dataset_seed=42",
                // "data.dataset_seed=43,44",
                // "data.dataset_seed=42,43,44",
                // "data.dataset_seed=50",
                // "data.dataset_seed=50,51,52",
                //
                // >>>> SetSUMBT datasets:
                // "data=setsumbt_dataloaders_processed",
                // "data.dataset_seed=0",
                // "data.data_subsampling.split=dev",
                //
                // >>>> Trippy datasets:
                // "data=trippy_dataloaders_processed",
                //
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                // "data.data_subsampling.split=test",
                // "data.data_subsampling.split=train,dev,test",
                "data.data_subsampling.split=train,validation,test",
                //
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //
                //    >> END: Data options.
                //
                //    >> START: Model options.
                //
                // >>>> Base models (Masked)
                // "language_model=roberta-base",
                // "++language_model.checkpoint_no=-1", // <-- checkpoint_no not necessary for the base models, setting it to '-1' to skip unnecessary iterations over different choices
                //
                // >>>> Fine-tuned models (Masked)
                // >> Individual:
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // >> Multiple:
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                //
                "++language_model.checkpoint_no=2800",
                //
                // >>>> Base models (Autoregressive)
                // "language_model=gpt2-medium",
                // "++language_model.checkpoint_no=-1", // <-- checkpoint_no not necessary for the base models, setting it to '-1' to skip unnecessary iterations over different choices
                //
                // >>>> Fine-tuned models (Autoregressive)
                // >> Individual:
                // "language_model=gpt2-medium-causal_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_sgd-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=gpt2-medium-causal_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // >> Multiple:
                // "language_model=gpt2-medium-causal_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_sgd-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,gpt2-medium-causal_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                //
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=1200",
                // "++language_model.checkpoint_no=2800",
                //
                // >>>> ERC models
                //
                // "language_model=bert-base-uncased",
                // "language_model=bert-base-uncased-SentiX",
                // "language_model=roberta-base,bert-base-uncased,bert-base-uncased-SentiX",
                //
                // "language_model=bert-base-uncased-ContextBERT-ERToD_emowoz_basic_setup",
                // "language_model.seed=42",
                // "language_model.seed=43,44",
                // "language_model.seed=43,44,45",
                // "language_model.seed=50",
                // "language_model.seed=50,51,52",
                // "language_model.checkpoint_no=0",
                // "language_model.checkpoint_no=1",
                // "language_model.checkpoint_no=0,1,2,3,4",
                //
                // >>>> SetSUMBT models
                //
                // "language_model=roberta-base-setsumbt_multiwoz21",
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813,5626,8439,11252,14065,16878,19691,25317,33756,36569,39382,42195,50634,56260,70325,90016,109707,115333,126585", // <-- locally, we have metadata and local estimates for these checkpoints available for "language_model.seed=0"
                // "language_model.seed=1",
                // "language_model.checkpoint_no=109707", // <-- locally, we have these checkpoints available for "language_model.seed=1"
                //
                // >>>> TripPy models
                //
                // "language_model=roberta-base-trippy_multiwoz21",
                // "language_model.seed=42",
                // "language_model.checkpoint_no=3549,7098,10647,14196,17745,21294,24843,28392,31941,35490",
                // "language_model.checkpoint_no=3549",
                //
                // >>>> TripPy-R models
                //
                // >> Short Trippy-R runs
                // "language_model=roberta-base-trippy_r_multiwoz21_short_runs",
                // >> Note: The following line contains all the checkpoints available for a full short Trippy-R training run:
                // "language_model.checkpoint_no=1775,3550,5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                // "language_model.checkpoint_no=1775,3550",
                // "language_model.checkpoint_no=5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500",
                //
                // >> Long Trippy-R runs
                // "language_model=roberta-base-trippy_r_multiwoz21_long_runs",
                // "language_model.lr_scheduler_type=constant_schedule_with_warmup,linear_schedule_with_warmup",
                // "language_model.lr_scheduler_type=linear_schedule_with_warmup",
                // Note: You can generate the comma-separated list of checkpoints for the long Trippy-R runs with the following python command:
                // >>> ','.join(list(map(str, range(1775,88751,1775))))
                // "language_model.checkpoint_no=1775,3550,5325,7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500,37275,39050,40825,42600,44375,46150,47925,49700,51475,53250,55025,56800,58575,60350,62125,63900,65675,67450,69225,71000,72775,74550,76325,78100,79875,81650,83425,85200,86975,88750",
                // "language_model.checkpoint_no=1775,3550",
                // "language_model.checkpoint_no=5325",
                // "language_model.checkpoint_no=7100,8875,10650,12425,14200,15975,17750,19525,21300,23075,24850,26625,28400,30175,31950,33725,35500,37275,39050,40825,42600,44375,46150,47925,49700,51475,53250,55025,56800,58575,60350,62125,63900,65675,67450,69225,71000,72775,74550,76325,78100,79875,81650,83425,85200,86975,88750",
                //
                // "language_model.seed=1111",
                // "language_model.seed=40",
                // "language_model.seed=41",
                // "language_model.seed=42", // Note: seed=42 is already submitted
                // "language_model.seed=43",
                // "language_model.seed=43,44",
                // "language_model.seed=42,43,44",
                // "language_model.seed=1111,40,41",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                //
                "local_estimates=twonn",
                //
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Feature flags.
                //    >> END: Feature flags.
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Submit: Analyse local estimate distribution over tokens - Local debugging - Dry run",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/distribution_over_tokens/submit_analyse_local_estimate_distribution.py",
            "console": "integratedTerminal",
            "args": [
                "--launcher",
                "basic",
                "--run-mode",
                "dry_run",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "wandb plots recreation - Local debugging - ",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/plotting/wandb_export/use_wandb_api_to_recreate_plots.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=basic",
                // "hydra/launcher=hpc_submission",
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                // "hydra.launcher.memory=32",
                // "hydra.launcher.ncpus=2",  // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                // "hydra.launcher.ngpus=1",
                // "hydra.launcher.walltime=00:10:00", // <-- The logging script should run very quickly.
                // >>>> END: Hydra options
                //
                "analysis.wandb_export.samples=50000", // <-- 50k. Note: 50k is already enough to get all points for the runs we have
                // "analysis.wandb_export.samples=100000", // <-- 100k. Note: 50k is already enough to get all points for the runs we have
                //
                // "analysis.wandb_export.use_saved_concatenated_df=False",
                "analysis.wandb_export.use_saved_concatenated_df=True",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        // TripPy-R analysis
        {
            "name": "Load cached features and save into format for Topo_LLM - Local debugging - TripPy-R data",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/experiments/local_dimensions_detect_exhaustion_of_training_capabilities/data_post_processing/load_cached_features_and_save_into_format_for_topo_llm.py",
            "console": "integratedTerminal",
            "args": [
                "--data-mode",
                "trippy_r",
                "--metadata-handling-mode",
                "create_and_save_bio_tags",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
    ]
}