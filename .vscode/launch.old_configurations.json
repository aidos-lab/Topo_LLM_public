{
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                // "data=one-year-of-tsla-on-reddit_validation",
                // "data=one-year-of-tsla-on-reddit_test",
                "data=multiwoz21_validation",
                // "data=sgd_test",
                // "data.data_splitting.data_splitting_mode=proportions",
                // "data.data_splitting.split_shuffle=True",
                // "data.data_splitting.split_seed=123",
                // "data.data_split.split_seed=null", // 'null' in the Hydra config is mapped to `None` in the Python code, and then no seed is set for the split
                "data.data_subsampling.number_of_samples=128",
                // "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                // "data.data_subsampling.split=train",
                // "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True"
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Regular Token Embedding; with POS, with data_subsampler, with Gaussian noise - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=multiwoz21",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "data.data_subsampling.split=validation",
                "language_model=roberta-base",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=300",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "local_estimates.noise.artificial_noise_mode=gaussian",
                // "local_estimates.noise.artificial_noise_mode=do_nothing",
                "local_estimates.noise.distortion_parameter=0.001",
                "local_estimates.noise.seed=2",
                //    >> START: Feature flags.
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True"
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Masked Token Embedding; on reddit, with POS, with data_subsampler - run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/pipeline_scripts/run_pipeline_compute_embeddings_and_data_prep_and_local_estimate.py",
            "console": "integratedTerminal",
            "args": [
                "tokenizer.add_prefix_space=True", // This is necessary to use the tokenizer with input split into words
                "+data.dataset_type=huggingface_dataset_named_entity",
                "data=one-year-of-tsla-on-reddit", // Note: Since the sequences in the reddit dataset are quite long, this takes a long time to run in the masked token mode
                "data.data_subsampling.split=validation",
                "data.data_subsampling.number_of_samples=128",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_seed=777",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=3000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "local_estimates.filtering.num_samples=1000",
                "local_estimates.compute_global_estimates=True",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.pointwise.absolute_n_neighbors=256",
                "feature_flags.analysis.create_plots_in_local_estimates_worker=True"
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example multiwoz21 test data, compare clean with noisy data",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                //    >> START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=masked_token",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data.local_estimates.noise.artificial_noise_mode=gaussian",
                "+comparison_data.local_estimates.noise.distortion_parameter=0.01",
                "+comparison_data.local_estimates.noise.seed=4",
                //
                //    >> END: Comparison data.
                //
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=600",
                // "analysis.investigate_distances.array_truncation_size=5000",
                "analysis.investigate_distances.array_truncation_size=20000"
            ]
        },
        {
            "name": "Compare local estimates (and potentially losses) - Local debugging - on example wikitext test data, compare masked token embeddings with regular token embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                "data=wikitext-103-v1_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // Option: Take a different token mode for comparison
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=500" // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - Local debugging - small array_truncation_size; for selected datasets and models; with comparison of local estimates with losses, but without comparison with other data (i.e., loss computation only on base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //    >> START Data options.
                "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "++language_model.checkpoint_no=2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=True",
                // >>>> Set this feature flag to avoid the comparison with other data
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=False"
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (no loss computation) - Local debugging - small array_truncation_size; with comparison of local estimates between settings (but without loss computation)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // >>>> END: Hydra options
                //
                //    >> START Data options.
                //
                "tokenizer.add_prefix_space=False",
                //
                // "data=multiwoz21_validation",
                // "data=one-year-of-tsla-on-reddit_validation,multiwoz21_validation",
                //
                "data=setsumbt_dataloaders_processed",
                "data.dataset_seed=0",
                "data.data_subsampling.split=dev",
                //
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778",
                //    >> END Data options.
                //
                //    >> START: Model options.
                //
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-setsumbt_multiwoz21",
                //
                // "language_model.seed=0",
                // "language_model.checkpoint_no=2813",
                //
                //    >> END: Model options.
                //
                "embeddings.embedding_data_handler.mode=regular",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates=lpca",
                "local_estimates.estimator.lpca_ver=FO",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                //
                "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                // "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                //
                //    >> START: Comparison data.
                //
                // Note: We need to also set all the parameters we overwrote in the main data configuration,
                // since otherwise the comparison data configs will be initialized with the default values.
                //
                // << This is the same as in the main data configuration
                "+comparison_data.embeddings.embedding_data_handler.mode=regular",
                "+comparison_data.local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "+comparison_data.local_estimates.filtering.deduplication_mode=array_deduplicator",
                "+comparison_data.local_estimates.filtering.num_samples=60000",
                "+comparison_data.local_estimates.pointwise.absolute_n_neighbors=128",
                // << This is different from the main data configuration
                "+comparison_data/local_estimates=twonn",
                //
                //    >> END: Comparison data.
                //
                //    >> START: Feature flags.
                "feature_flags.comparison.do_comparison_of_local_estimates_with_losses=False", // <-- Set this feature flag to avoid the predictions and loss computation
                "feature_flags.comparison.do_comparison_of_local_estimates_between_base_data_and_comparison_data=True" // <-- Set this feature flag to compare with different local estimates
                //    >> END: Feature flags.
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for validation or test splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:20:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                //
                // "data=multiwoz21_validation", // <-- only multiwoz21_validation
                // "data=iclr_2024_submissions_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits except multiwoz21_validation
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation", // <-- all validation splits
                // "data=iclr_2024_submissions_test,multiwoz21_test,one-year-of-tsla-on-reddit_test,sgd_test,wikitext-103-v1_test", // <-- all test splits
                //
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                //    >> START Model options.
                //    > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // 
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // > Note: Set checkpoint_no for the fine-tuned models
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head_embeddings.word_embeddings-5,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-freeze-lm_head-5",
                "++language_model.checkpoint_no=100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                //    >> END Model options.
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False"
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for different subsampling seeds of validation splits of datasets (random sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=02:00:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX 6000
                // >>>> END: Hydra options
                //    >> START Data options.
                // "data=multiwoz21_validation",
                "data=iclr_2024_submissions_validation,multiwoz21_validation,one-year-of-tsla-on-reddit_validation,sgd_validation,wikitext-103-v1_validation",
                //    >> END Data options.
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779", // <-- Different choices of data_subsampling.sampling_seed here
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                // > Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                // "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                //
                // > Note: Remember to set checkpoint_no for the fine-tuned models!
                //
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                //
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                // "++language_model.checkpoint_no=2800",
                //
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False"
            ]
        },
        {
            "name": "Compare local estimates (loss computation only on base data) - HPC cluster submission - for multiple train datasets (take first sampling) and models; without comparison (i.e., only computing the losses on the base data)",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/compare_local_estimates_and_distances_and_losses/run_compare_local_estimates.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Hydra options
                //
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission",
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=RTX6000",
                // "hydra.launcher.queue=DSML",
                // "hydra.launcher.template=DSML",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=00:30:00", // For 60000 samples and one dataset and model, a forward pass takes only a few minutes on an RTX6000-24GB (CUDA queue) or an GTX2080-8GB (DSML queue)
                //
                // >>>> END: Hydra options
                // "data=iclr_2024_submissions_train,multiwoz21_train,one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                // "data=one-year-of-tsla-on-reddit_train,sgd_train,wikitext-103-v1_train",
                "data=iclr_2024_submissions_train",
                // "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.sampling_mode=take_first",
                "data.data_subsampling.number_of_samples=10000",
                // Note: No sampling seed for take_first sampling: "data.data_subsampling.sampling_seed=777",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=False", // <-- Note: We set `add_prefix_space` to `False` to be consistent with the fine-tuning
                // Note: You need to have the masked embeddings precomputed for the given checkpoints before you can call this loss computation.
                "language_model=roberta-base", // Note: Do not set checkpoint_no for the base model
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base,roberta-base-masked_lm-defaults_multiwoz21-rm-empty-True-do_nothing-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_one-year-of-tsla-on-reddit-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5,roberta-base-masked_lm-defaults_wikitext-103-v1-rm-empty-True-proportions-True-0-0.8-0.1-0.1-ner_tags_train-10000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "language_model=roberta-base-masked_lm-defaults_iclr_2024_submissions-rm-empty-True-do_nothing-ner_tags_train-5000-take_first-111_standard-None_5e-05-linear-0.01-5",
                // "++language_model.checkpoint_no=2800",
                // "++language_model.checkpoint_no=400,800,1200,1600,2000,2400,2800",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                // >>>> Since we are only interested in the base data, we do not need to set the comparison data
                // "analysis.investigate_distances.array_truncation_size=500", // The value '500' can be used for quick testing
                // "analysis.investigate_distances.array_truncation_size=5000",
                // "analysis.investigate_distances.array_truncation_size=20000",
                // Note: "analysis.investigate_distances.array_truncation_size=30000" runs out of memory on the 16GB MacBook Pro M1
                "analysis.investigate_distances.array_truncation_size=60000", // The value '60000' potentially takes a long time to run, and this might run out of memory on the 16GB MacBook Pro M1
                // >>>> Set this feature flag to avoid the comparison
                "feature_flags.comparison.do_comparison_of_local_estimates=False"
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; multiple data lists",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                "--run-option",
                "dry_run",
                "--run-only-selected-configs-option",
                "run_all",
                // "run_single_random",
                "--submission-mode",
                "local"
                // >> Concrete experiment configurations
                // "--data-list-options",
                // "reddit_only",
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "wikitext_only"
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, multiple layers, for data validation splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML", # Note: The GPT-2-medium embedding computation does not run on the 8 GB GPUs
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_multiple_layers_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "iclr_validation_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                "--data-list-options",
                "reddit_validation_only",
                "--data-list-options",
                "sgd_validation_only",
                "--data-list-options",
                "wikitext_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                //"--model-group-options",
                // "gpt2_medium_without_modifications",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_checkpoints_1200_1600",
                "--model-group-options",
                "gpt2_medium_finetuned_for_few_epochs_wikitext_data_single_seed_checkpoints_1200_1600"
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; regular embeddings, last layer, single sample",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "regular_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "validation_split_only",
                //    >> Individual dataset and splits, so that you can submit them in separate tmux sessions
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only",
                //    >> END Data list options.
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications"
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "gpt2_medium_without_modifications",
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for random subsample of selected data splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000",
                // >>>> Concrete experiment configurations
                //
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                // "--experiment-selector-options",
                // "masked_token_embeddings_last_layer_two_data_subsampling_sampling_seeds",
                //
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //
                //    >> BEGIN Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "validation_split_only",
                //
                // "--data-list-options",
                // "iclr_validation_only",
                // "--data-list-options",
                // "multiwoz21_validation_only",
                // "--data-list-options",
                // "reddit_validation_only",
                // "--data-list-options",
                // "sgd_validation_only",
                // "--data-list-options",
                // "wikitext_validation_only",
                //
                // "--data-list-options",
                // "iclr_test_only",
                // "--data-list-options",
                // "multiwoz21_test_only",
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "sgd_test_only",
                // "--data-list-options",
                // "wikitext_test_only",
                //
                //    >> END Data list options.
                //
                //    >> BEGIN Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                // "--model-group-options",
                // "roberta_base_without_modifications",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_all_checkpoints_step_100",
                "--model-group-options",
                "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_frozen_lm_head_all_checkpoints_step_100"
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; NOTE: expensive computation; masked embeddings, last layer, for take_first sample of data train splits",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                // "local",
                "hpc_submission",
                "--template-to-use-for-compute-embeddings",
                // "DSML",
                "RTX6000", // <-- Change this back to "RTX6000" when the RTX6000 is available again
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "masked_token_embeddings_last_layer_single_sample",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run",
                //    >> START Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                //
                "--data-list-options",
                "iclr_train_only",
                // "--data-list-options",
                // "multiwoz21_train_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "sgd_train_only",
                // "--data-list-options",
                // "wikitext_train_only",
                //
                //    >> END Data list options.
                "--data-subsampling-sampling-mode",
                "take_first",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                //
                "--model-group-options",
                "roberta_base_without_modifications"
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_old_and_new_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_data_single_seed_last_checkpoint",
                // "--model-group-options",
                // "roberta_base_finetuned_for_few_epochs_multiwoz_and_reddit_and_wikitext_data_single_seed_all_checkpoints_step_400",
                //
                //    >> END Model group options.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity analysis multiwoz21/reddit different data subsampling number of samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_multiwoz21_different_data_subsampling_number_of_samples",
                "--experiment-selector-options",
                "sensitivity_analysis_reddit_different_data_subsampling_number_of_samples",
                "--experiment-stage",
                "compute_embeddings_plus_single_pipeline_run" // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                // "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> NOTE: These experiments set the data list option themselves.
                //    >>       Make sure that the default data list option is a single entry, otherwise this script might get called multiple times.
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "hpc_submission",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_filtering_number_of_samples",
                "--experiment-stage",
                // "compute_embeddings_plus_single_pipeline_run", // Note: You need to make sure the embedding arrays exist before you can call the skip_compute_embeddings_but_do_multiple_pipeline_runs stage.
                // "skip_compute_embeddings_but_do_multiple_pipeline_runs", // Note: If some results are not available, this might have been caused by conflicts in writing the results from parallel data_prep steps.
                "skip_compute_embeddings_and_skip_embeddings_data_prep", // Note: Make sure the prepared data exists before you run this stage.
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                // "--data-list-options",
                // "multiwoz21_only",
                // "--data-list-options",
                // "reddit_only",
                "--data-list-options",
                "multiwoz21_test_only",
                "--data-list-options",
                "multiwoz21_train_only",
                "--data-list-options",
                "multiwoz21_validation_only"
                // "--data-list-options",
                // "reddit_test_only",
                // "--data-list-options",
                // "reddit_train_only",
                // "--data-list-options",
                // "reddit_validation_only",
            ]
        },
        {
            "name": "call_submit_jobs_for_experiments_via_tmux_submission; sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors for single data split",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/submission_scripts/call_submit_jobs_scripts/call_submit_jobs_for_experiments_via_tmux_submission.py",
            "console": "integratedTerminal",
            "args": [
                // >>>> BEGIN: Select the run option
                // >>>> Use the following to dry run with all configurations
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_all",
                //
                // >>>> Use the following to dry run a random configuration
                //
                // "--run-option",
                // "dry_run",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>> Use the following to run a random configuration
                //
                // "--run-option",
                // "do_submission",
                // "--run-only-selected-configs-option",
                // "run_single_random",
                //
                // >>>> Use the following to run with all configurations
                //
                "--run-option",
                "do_submission",
                "--run-only-selected-configs-option",
                "run_all",
                //
                // >>>> END: Select the run option
                "--submission-mode",
                "local",
                // >>>> Concrete experiment configurations
                "--experiment-selector-options",
                "sensitivity_analysis_different_local_estimates_pointwise_absolute_n_neighbors",
                "--experiment-stage",
                "skip_compute_embeddings_but_do_multiple_pipeline_runs",
                //    >> Data list options.
                //    >> Note that for multiple options, you need to specify the `--data-list-options` multiple times.
                "--data-list-options",
                // "multiwoz21_validation_and_reddit_validation",
                "reddit_validation_only",
                //    >> START Model group options.
                //    >> Note that for multiple options, you need to specify the `--model-group-options` multiple times.
                "--model-group-options",
                "roberta_base_without_modifications"
                //    >> END Model group options.
            ]
        },
        {
            "name": "gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/scripts/google_cloud/gsutil_rsync_directory_to_and_from_gc_bucket.py",
            "console": "integratedTerminal",
            "args": [
                "--source",
                "bucket",
                "--target",
                "local",
                "--subdirectory",
                "data/analysis/twonn",
                "hydra_output_dir",
                "--dry_run"
            ]
        },
        {
            "name": "run_iterate_over_twonn_results_and_compare_hausdorff_distances.py; on example directory",
            "type": "debugpy",
            "request": "launch",
            "program": "topollm/analysis/investigate_distances_and_influence_on_local_estimates/run_iterate_over_twonn_results_and_compare_hausdorff_distances.py",
            "console": "integratedTerminal",
            "args": [
                "--multirun",
                "data=multiwoz21_test",
                "data.data_subsampling.sampling_mode=random",
                "data.data_subsampling.number_of_samples=10000",
                "data.data_subsampling.sampling_seed=778,779",
                "+data.dataset_type=huggingface_dataset_named_entity",
                "tokenizer.add_prefix_space=True",
                "language_model=roberta-base",
                "embeddings.embedding_data_handler.mode=masked_token",
                "embeddings.embedding_extraction.layer_indices=[-1]",
                "embeddings_data_prep.sampling.num_samples=150000",
                "embeddings_data_prep.sampling.sampling_mode=random",
                "embeddings_data_prep.sampling.seed=42,43",
                "local_estimates.pointwise.n_neighbors_mode=absolute_size",
                "local_estimates.filtering.deduplication_mode=array_deduplicator",
                "local_estimates.filtering.num_samples=60000",
                "local_estimates.pointwise.absolute_n_neighbors=128",
                "analysis.investigate_distances.array_truncation_size=5000",
                "feature_flags.analysis.compare_sampling_methods.do_iterate_all_partial_search_base_directories=True" // This is an example for setting the feature flags
            ]
        }
    ]
}