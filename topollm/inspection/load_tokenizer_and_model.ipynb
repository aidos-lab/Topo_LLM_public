{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import pprint\n",
    "from logging import Formatter\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "from hydra import compose, initialize_config_dir\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "from rich.table import Table\n",
    "from rich.text import Text\n",
    "\n",
    "from topollm.config_classes.constants import HYDRA_CONFIGS_BASE_PATH\n",
    "from topollm.config_classes.main_config import MainConfig\n",
    "from topollm.logging.initialize_configuration_and_log import initialize_configuration\n",
    "from topollm.model_finetuning.load_tokenizer_from_finetuning_config import load_tokenizer_from_finetuning_config\n",
    "from topollm.typing.enums import Verbosity\n",
    "\n",
    "verbosity = Verbosity.NORMAL\n",
    "\n",
    "LOGFORMAT_FILE = \"[%(asctime)s][%(levelname)8s][%(name)s] %(message)s (%(filename)s:%(lineno)s)\"\n",
    "LOGFORMAT_RICH = \"%(message)s\"\n",
    "\n",
    "error_console = Console(stderr=True)\n",
    "\n",
    "rich_handler = RichHandler(console=error_console)\n",
    "\n",
    "rich_handler.setFormatter(Formatter(LOGFORMAT_RICH))\n",
    "\n",
    "rotating_file_path = pathlib.Path(\n",
    "    \"logs\",\n",
    "    \"load_tokenizer_and_model.log\",\n",
    ")\n",
    "rotating_file_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "rotating_file_handler = RotatingFileHandler(\n",
    "    rotating_file_path,\n",
    "    maxBytes=1024 * 1024 * 10,  # 10Mb\n",
    "    backupCount=10,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=LOGFORMAT_FILE,\n",
    "    handlers=[\n",
    "        rich_handler,\n",
    "        rotating_file_handler,\n",
    "    ],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"load_tokenizer_and_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hydra in Jupyter notebook:\n",
    "# https://github.com/facebookresearch/hydra/blob/main/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb\n",
    "from topollm.model_handling.tokenizer.tokenizer_modifier.factory import get_tokenizer_modifier\n",
    "\n",
    "abs_config_dir = pathlib.Path(HYDRA_CONFIGS_BASE_PATH)\n",
    "with initialize_config_dir(version_base=None, config_dir=str(abs_config_dir)):\n",
    "    config = compose(\n",
    "        config_name=\"main_config\",\n",
    "        overrides=[\"feature_flags.finetuning.use_wandb=false\", \"finetuning=finetuning_for_token_classification\"],\n",
    "        return_hydra_config=True,\n",
    "    )\n",
    "    logger.info(pprint.pformat(config, indent=4))\n",
    "\n",
    "    main_config: MainConfig = initialize_configuration(\n",
    "        config=config,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "# # # #\n",
    "finetuning_config = main_config.finetuning\n",
    "\n",
    "base_tokenizer = load_tokenizer_from_finetuning_config(\n",
    "    finetuning_config=finetuning_config,\n",
    "    verbosity=verbosity,\n",
    "    logger=logger,\n",
    ")\n",
    "tokenizer_modifier = get_tokenizer_modifier(\n",
    "    tokenizer_modifier_config=finetuning_config.base_model.tokenizer_modifier,\n",
    "    verbosity=verbosity,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer_modifier.modify_tokenizer(\n",
    "    tokenizer=base_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# START Example data\n",
    "\n",
    "example_token_ids: list[int] = [\n",
    "    0,\n",
    "    8814,\n",
    "    7,\n",
    "    2487,\n",
    "    456,\n",
    "    16415,\n",
    "    45835,\n",
    "    3175,\n",
    "    27785,\n",
    "    3309,\n",
    "    359,\n",
    "    2393,\n",
    "    1173,\n",
    "    32,\n",
    "    32659,\n",
    "    103,\n",
    "    8419,\n",
    "    897,\n",
    "    8419,\n",
    "    46909,\n",
    "    23184,\n",
    "    897,\n",
    "    28696,\n",
    "    155,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "]\n",
    "\n",
    "example_attention_mask = [\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "]\n",
    "\n",
    "example_labels_aligned = [\n",
    "    -100,\n",
    "    0,\n",
    "    0,\n",
    "    7,\n",
    "    0,\n",
    "    0,\n",
    "    -100,\n",
    "    -100,\n",
    "    0,\n",
    "    9,\n",
    "    0,\n",
    "    9,\n",
    "    -100,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    -100,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "    -100,\n",
    "]\n",
    "\n",
    "# END Example data\n",
    "# # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the token_ids\n",
    "\n",
    "example_token_ids_decoded = [tokenizer.convert_ids_to_tokens(token_id) for token_id in example_token_ids]\n",
    "\n",
    "logger.info(\"example_token_ids_decoded:\\n%s\", example_token_ids_decoded)\n",
    "\n",
    "label_list: list[str] = [\n",
    "    \"O\",\n",
    "    \"B-corporation\",\n",
    "    \"I-corporation\",\n",
    "    \"B-creative-work\",\n",
    "    \"I-creative-work\",\n",
    "    \"B-group\",\n",
    "    \"I-group\",\n",
    "    \"B-location\",\n",
    "    \"I-location\",\n",
    "    \"B-person\",\n",
    "    \"I-person\",\n",
    "    \"B-product\",\n",
    "    \"I-product\",\n",
    "]\n",
    "\n",
    "\n",
    "def decode_label(\n",
    "    label: int,\n",
    "    label_list: list[str],\n",
    ") -> str:\n",
    "    \"\"\"Decode label from label index to label string.\"\"\"\n",
    "    padding_label = -100\n",
    "    if label == padding_label:\n",
    "        return \"X\"\n",
    "    return label_list[label]\n",
    "\n",
    "\n",
    "example_labels_aligned_mapped = [\n",
    "    decode_label(\n",
    "        label=label,\n",
    "        label_list=label_list,\n",
    "    )\n",
    "    for label in example_labels_aligned\n",
    "]\n",
    "\n",
    "logger.info(\"example_labels_aligned_mapped:\\n%s\", example_labels_aligned_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = list(\n",
    "    zip(\n",
    "        example_token_ids,\n",
    "        example_attention_mask,\n",
    "        example_token_ids_decoded,\n",
    "        example_labels_aligned,\n",
    "        example_labels_aligned_mapped,\n",
    "        strict=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(\"merged_list:\\n%s\", merged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "# Create a table with headers\n",
    "table = Table(title=\"Merged List\")\n",
    "\n",
    "# Add columns (make sure these match the number of elements in your tuples)\n",
    "table.add_column(\"Token ID\", justify=\"right\")\n",
    "table.add_column(\"Attention Mask\", justify=\"right\")\n",
    "table.add_column(\"Token Decoded\")\n",
    "table.add_column(\"Label Aligned\", justify=\"right\")\n",
    "table.add_column(\"Label Aligned Mapped\")\n",
    "\n",
    "# Add rows to the table\n",
    "for item in merged_list:\n",
    "    table.add_row(*map(str, item))  # Convert all items to strings for display\n",
    "\n",
    "# Print the table to the console\n",
    "console.print(table)\n",
    "\n",
    "\n",
    "def log_table_to_string(rich_table: Table) -> Text:\n",
    "    \"\"\"Generate an ascii formatted presentation of a Rich table.\n",
    "\n",
    "    Eliminates any column styling.\n",
    "    \"\"\"\n",
    "    console = Console(width=150)\n",
    "    with console.capture() as capture:\n",
    "        console.print(rich_table)\n",
    "    return Text.from_ansi(capture.get())\n",
    "\n",
    "\n",
    "logger.info(\"table:\\n%s\", log_table_to_string(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
