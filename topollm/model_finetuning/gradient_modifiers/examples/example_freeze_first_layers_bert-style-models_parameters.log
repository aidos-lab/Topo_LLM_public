[2024-05-12 19:54:05,263][    INFO][__main__] name = 'roberta.embeddings.word_embeddings.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,263][    INFO][__main__] name = 'roberta.embeddings.position_embeddings.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,263][    INFO][__main__] name = 'roberta.embeddings.token_type_embeddings.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,263][    INFO][__main__] name = 'roberta.embeddings.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,263][    INFO][__main__] name = 'roberta.embeddings.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,264][    INFO][__main__] name = 'roberta.encoder.layer.0.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.0.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.0.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.0.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.0.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,265][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.1.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,266][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,267][    INFO][__main__] name = 'roberta.encoder.layer.2.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,268][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,269][    INFO][__main__] name = 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,269][    INFO][__main__] name = 'roberta.encoder.layer.3.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,269][    INFO][__main__] name = 'roberta.encoder.layer.3.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.3.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.3.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.3.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.3.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,270][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.4.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.query.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.query.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.key.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,271][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.key.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.value.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.self.value.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.intermediate.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.intermediate.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.output.dense.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.output.dense.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.output.LayerNorm.weight', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,272][    INFO][__main__] name = 'roberta.encoder.layer.5.output.LayerNorm.bias', param.requires_grad = False (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,273][    INFO][__main__] name = 'roberta.encoder.layer.6.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.6.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.6.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.6.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.6.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,274][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.7.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,275][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.8.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,276][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,277][    INFO][__main__] name = 'roberta.encoder.layer.9.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,278][    INFO][__main__] name = 'roberta.encoder.layer.9.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,278][    INFO][__main__] name = 'roberta.encoder.layer.9.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,278][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,278][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,278][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,279][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,279][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,279][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,279][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,279][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.10.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.query.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.query.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.key.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,280][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.key.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.value.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.self.value.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.intermediate.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.intermediate.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.output.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.output.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.output.LayerNorm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,281][    INFO][__main__] name = 'roberta.encoder.layer.11.output.LayerNorm.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,282][    INFO][__main__] name = 'lm_head.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,282][    INFO][__main__] name = 'lm_head.dense.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,282][    INFO][__main__] name = 'lm_head.dense.bias', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,282][    INFO][__main__] name = 'lm_head.layer_norm.weight', param.requires_grad = True (log_model_info.py:66)
[2024-05-12 19:54:05,282][    INFO][__main__] name = 'lm_head.layer_norm.bias', param.requires_grad = True (log_model_info.py:66)