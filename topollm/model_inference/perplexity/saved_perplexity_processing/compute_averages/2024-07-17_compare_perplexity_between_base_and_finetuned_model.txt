Setting HYDRA_FULL_ERROR environment variable to '1'.
os.environ['HYDRA_FULL_ERROR'] = '1'
[2024-07-17 16:19:48,925][    INFO][__main__] Running script ... (run_load_saved_perplexity_and_compute_averages.py:79)
[2024-07-17 16:19:48,966][    INFO][__main__] get_git_info() = 'feature/compatibility_with_local_hpc_tools/9779505711b4f345d038b616ab43339e5285817c' (initialize_configuration_and_log.py:98)
[2024-07-17 16:19:48,967][    INFO][__main__] Working directory:
pathlib.Path.cwd() = PosixPath('/Users/ruppik/git-source/Topo_LLM/topollm/model_inference/perplexity/saved_perplexity_processing/compute_averages') (initialize_configuration_and_log.py:69)
[2024-07-17 16:19:48,967][    INFO][__main__] Hydra output directory:
/Users/ruppik/git-source/Topo_LLM/hydra_output_dir/run/run_load_saved_perplexity_and_compute_averages/2024-07-17/16-19-48/seed=1234 (initialize_configuration_and_log.py:73)
[2024-07-17 16:19:48,967][    INFO][__main__] omegaconf.DictConfig config:
{'data': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 5000, 'split': 'train', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}, 'embeddings_data_prep': {'num_samples': 30000}, 'embeddings': {'dataset_map': {'batch_size': 1000, 'num_proc': 1}, 'embedding_extraction': {'layer_indices': [-1], 'aggregation': 'mean'}, 'batch_size': 32, 'level': 'token', 'num_workers': 0}, 'feature_flags': {'embeddings_data_prep': {'write_sentences_to_meta': True}, 'finetuning': {'skip_finetuning': False, 'do_create_finetuned_language_model_config': True}, 'wandb': {'use_wandb': True}}, 'finetuning': {'base_model': {'lm_mode': 'MLM', 'pretrained_model_name_or_path': 'roberta-base', 'short_model_name': '${finetuning.base_model.pretrained_model_name_or_path}', 'task_type': 'MASKED_LM', 'tokenizer_modifier': {'mode': 'DO_NOTHING', 'padding_token': '<pad>'}}, 'batch_sizes': {'train': 16, 'eval': 16}, 'gradient_modifier': {'mode': 'DO_NOTHING'}, 'finetuning_datasets': {'eval_dataset': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 10000, 'split': 'validation', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}, 'train_dataset': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 10000, 'split': 'train', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}}, 'compute_metrics_mode': 'FROM_TASK_TYPE', 'eval_steps': 400, 'fp16': False, 'gradient_accumulation_steps': 2, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'linear', 'log_level': 'info', 'logging_steps': 100, 'max_steps': -1, 'mlm_probability': 0.15, 'num_train_epochs': 5, 'save_steps': 400, 'use_cpu': False, 'warmup_steps': 500, 'weight_decay': 0.01, 'tokenizer': {'add_prefix_space': False, 'max_length': 512}, 'peft': {'finetuning_mode': 'standard'}}, 'inference': {'max_length': 100, 'num_return_sequences': 3}, 'language_model': {'tokenizer_modifier': {'mode': 'DO_NOTHING', 'padding_token': '<pad>'}, 'lm_mode': 'MLM', 'pretrained_model_name_or_path': 'roberta-base', 'short_model_name': '${language_model.pretrained_model_name_or_path}'}, 'local_estimates': {'description': 'twonn', 'filtering': {'num_samples': 2500, 'zero_vector_handling_mode': 'keep'}}, 'paths': {'data_dir': '${paths.repository_base_path}/data', 'repository_base_path': '${oc.env:TOPO_LLM_REPOSITORY_BASE_PATH}'}, 'storage': {'array_storage_type': 'zarr', 'chunk_size': 512, 'metadata_storage_type': 'pickle'}, 'submit_jobs': {'machine_configuration': {'accelerator_model': 'rtx6000', 'queue': 'CUDA', 'walltime': '42:00:00', 'dry_run': False}, 'submit_finetuning_jobs': {'training_schedule': {'short_linear_lr_scheduler': {'num_train_epochs': 5, 'lr_scheduler_type': 'linear'}, 'long_constant_lr_scheduler': {'num_train_epochs': 30, 'lr_scheduler_type': 'constant'}}, 'base_model_list': ['roberta-base'], 'finetuning_dataset_list': ['train_and_eval_on_multiwoz21_10000_samples', 'train_and_eval_on_one-year-of-tsla-on-reddit'], 'peft_list': ['standard', 'lora'], 'gradient_modifier_list': ['do_nothing', 'freeze_first_layers_bert-style-models', 'freeze_last_layers_bert-style-models'], 'lora_parameters': {'first': {'lora_r': 16, 'lora_alpha': 32, 'use_rslora': False}}, 'wandb_project': 'Topo_LLM_run_combinations_for_single_r_choice'}, 'submit_pipeline_jobs': {'data_list': ['multiwoz21_validation', 'iclr_2024_submissions_validation'], 'language_model_list': ['model-roberta-base_multiwoz21-train-10000_ftm-lora_r-4_alpha-8_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml', 'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-8_alpha-16_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml', 'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-16_alpha-32_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml', 'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-32_alpha-64_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml'], 'layer_indices_list': ['[-1]', '[-2]', '[-3]', '[-4]', '[-5]', '[-6]', '[-7]', '[-8]', '[-9]', '[-10]', '[-11]', '[-12]'], 'checkpoint_no_list': [400, 800, 1200, 1600], 'data_number_of_samples_list': [3000], 'embeddings_data_prep_num_samples_list': [30000], 'wandb_project': 'Topo_LLM_pipeline_jobs'}}, 'tokenizer': {'add_prefix_space': False, 'max_length': 512}, 'transformations': {'normalization': 'None', 'rotation': 'None'}, 'wandb': {'project': 'Topo_LLM_debug', 'entity': None, 'tags': ['model_finteuning'], 'dir': '${paths.repository_base_path}/wandb_output_dir/${wandb.project}/'}, 'seed': 1234, 'verbosity': 1, 'preferred_torch_backend': 'auto'} (initialize_configuration_and_log.py:84)
[2024-07-17 16:19:48,970][    INFO][__main__] main_config:
{'data': {'column_name': 'body',
          'context': 'dataset_entry',
          'data_dir': None,
          'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                         'proportions': {'test': 0.1,
                                         'train': 0.8,
                                         'validation': 0.1}},
          'dataset_description_string': 'one-year-of-tsla-on-reddit',
          'dataset_name': 'comments',
          'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
          'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
          'feature_column_name': 'ner_tags',
          'number_of_samples': 5000,
          'split': <Split.TRAIN: 'train'>},
 'embeddings': {'batch_size': 32,
                'dataset_map': {'batch_size': 1000, 'num_proc': 1},
                'embedding_extraction': {'aggregation': <AggregationType.MEAN: 'mean'>,
                                         'layer_indices': [-1]},
                'level': <Level.TOKEN: 'token'>,
                'num_workers': 0},
 'embeddings_data_prep': {'num_samples': 30000},
 'feature_flags': {'embeddings_data_prep': {'write_sentences_to_meta': True},
                   'finetuning': {'do_create_finetuned_language_model_config': True,
                                  'skip_finetuning': False},
                   'wandb': {'use_wandb': True}},
 'finetuning': {'base_model': {'checkpoint_no': -1,
                               'lm_mode': <LMmode.MLM: 'MLM'>,
                               'pretrained_model_name_or_path': 'roberta-base',
                               'short_model_name': 'roberta-base',
                               'task_type': <TaskType.MASKED_LM: 'MASKED_LM'>,
                               'tokenizer_modifier': {'mode': <TokenizerModifierMode.DO_NOTHING: 'DO_NOTHING'>,
                                                      'padding_token': '<pad>'}},
                'batch_sizes': {'eval': 16, 'train': 16},
                'compute_metrics_mode': <ComputeMetricsMode.FROM_TASK_TYPE: 'FROM_TASK_TYPE'>,
                'eval_steps': 400,
                'finetuning_datasets': {'eval_dataset': {'column_name': 'body',
                                                         'context': 'dataset_entry',
                                                         'data_dir': None,
                                                         'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                                                                        'proportions': {'test': 0.1,
                                                                                        'train': 0.8,
                                                                                        'validation': 0.1}},
                                                         'dataset_description_string': 'one-year-of-tsla-on-reddit',
                                                         'dataset_name': 'comments',
                                                         'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
                                                         'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
                                                         'feature_column_name': 'ner_tags',
                                                         'number_of_samples': 10000,
                                                         'split': <Split.VALIDATION: 'validation'>},
                                        'train_dataset': {'column_name': 'body',
                                                          'context': 'dataset_entry',
                                                          'data_dir': None,
                                                          'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                                                                         'proportions': {'test': 0.1,
                                                                                         'train': 0.8,
                                                                                         'validation': 0.1}},
                                                          'dataset_description_string': 'one-year-of-tsla-on-reddit',
                                                          'dataset_name': 'comments',
                                                          'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
                                                          'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
                                                          'feature_column_name': 'ner_tags',
                                                          'number_of_samples': 10000,
                                                          'split': <Split.TRAIN: 'train'>}},
                'fp16': False,
                'gradient_accumulation_steps': 2,
                'gradient_checkpointing': True,
                'gradient_modifier': {'mode': <GradientModifierMode.DO_NOTHING: 'DO_NOTHING'>,
                                      'target_modules_to_freeze': []},
                'learning_rate': 5e-05,
                'log_level': 'info',
                'logging_steps': 100,
                'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,
                'max_steps': -1,
                'mlm_probability': 0.15,
                'num_train_epochs': 5,
                'peft': {'finetuning_mode': <FinetuningMode.STANDARD: 'standard'>,
                         'lora_alpha': 32,
                         'lora_dropout': 0.01,
                         'r': 8,
                         'target_modules': ['query', 'key', 'value'],
                         'use_rslora': False},
                'report_to': ['wandb', 'tensorboard'],
                'save_steps': 400,
                'tokenizer': {'add_prefix_space': False,
                              'max_length': 512,
                              'return_special_tokens_mask': True},
                'trainer_modifier': {'frequency': 400,
                                     'mode': <TrainerModifierMode.DO_NOTHING: 'DO_NOTHING'>,
                                     'num_samples': 10},
                'use_cpu': False,
                'warmup_steps': 500,
                'weight_decay': 0.01},
 'inference': {'max_length': 100, 'num_return_sequences': 3},
 'language_model': {'checkpoint_no': -1,
                    'lm_mode': <LMmode.MLM: 'MLM'>,
                    'pretrained_model_name_or_path': 'roberta-base',
                    'short_model_name': 'roberta-base',
                    'task_type': <TaskType.MASKED_LM: 'MASKED_LM'>,
                    'tokenizer_modifier': {'mode': <TokenizerModifierMode.DO_NOTHING: 'DO_NOTHING'>,
                                           'padding_token': '<pad>'}},
 'local_estimates': {'description': 'twonn',
                     'filtering': {'num_samples': 2500,
                                   'zero_vector_handling_mode': <ZeroVectorHandlingMode.KEEP: 'keep'>}},
 'paths': {'data_dir': PosixPath('/Users/ruppik/git-source/Topo_LLM/data'),
           'repository_base_path': PosixPath('/Users/ruppik/git-source/Topo_LLM')},
 'preferred_torch_backend': <PreferredTorchBackend.AUTO: 'auto'>,
 'seed': 1234,
 'storage': {'array_storage_type': <ArrayStorageType.ZARR: 'zarr'>,
             'chunk_size': 512,
             'metadata_storage_type': <MetadataStorageType.PICKLE: 'pickle'>},
 'submit_jobs': {'machine_configuration': {'accelerator_model': 'rtx6000',
                                           'dry_run': False,
                                           'job_run_mode': <JobRunMode.HHU_HILBERT: 'hhu_hilbert'>,
                                           'memory_gb': 32,
                                           'ncpus': 2,
                                           'ngpus': 1,
                                           'queue': 'CUDA',
                                           'run_job_locally_command': ['python3'],
                                           'submit_job_hilbert_command': ['python3',
                                                                          '/gpfs/project/ruppik/.usr_tls/tools/submit_job.py'],
                                           'walltime': '42:00:00'},
                 'submit_finetuning_jobs': {'base_model_list': ['roberta-base'],
                                            'common_batch_size': 16,
                                            'eval_steps': 100,
                                            'finetuning_dataset_list': ['train_and_eval_on_multiwoz21_10000_samples',
                                                                        'train_and_eval_on_one-year-of-tsla-on-reddit'],
                                            'finetuning_python_script_relative_path': PosixPath('topollm/model_finetuning/run_finetune_language_model_on_huggingface_dataset.py'),
                                            'fp16': 'true',
                                            'gradient_modifier_list': ['do_nothing',
                                                                       'freeze_first_layers_bert-style-models',
                                                                       'freeze_last_layers_bert-style-models'],
                                            'lora_parameters': {'first': {'lora_alpha': 32,
                                                                          'lora_r': 16,
                                                                          'use_rslora': False}},
                                            'peft_list': ['standard', 'lora'],
                                            'save_steps': 400,
                                            'training_schedule': {'long_constant_lr_scheduler': {'lr_scheduler_type': 'constant',
                                                                                                 'num_train_epochs': 30},
                                                                  'short_linear_lr_scheduler': {'lr_scheduler_type': 'linear',
                                                                                                'num_train_epochs': 5}},
                                            'wandb_project': 'Topo_LLM_run_combinations_for_single_r_choice'},
                 'submit_pipeline_jobs': {'checkpoint_no_list': [400,
                                                                 800,
                                                                 1200,
                                                                 1600],
                                          'data_list': ['multiwoz21_validation',
                                                        'iclr_2024_submissions_validation'],
                                          'data_number_of_samples_list': [3000],
                                          'embeddings_data_prep_num_samples_list': [30000],
                                          'language_model_list': ['model-roberta-base_multiwoz21-train-10000_ftm-lora_r-4_alpha-8_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml',
                                                                  'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-8_alpha-16_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml',
                                                                  'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-16_alpha-32_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml',
                                                                  'model-roberta-base_multiwoz21-train-10000_ftm-lora_r-32_alpha-64_lora-target-query_key_value_lora-dropout-0.01_rslora-True_5e-05-constant-0.01-50.yaml'],
                                          'layer_indices_list': ['[-1]',
                                                                 '[-2]',
                                                                 '[-3]',
                                                                 '[-4]',
                                                                 '[-5]',
                                                                 '[-6]',
                                                                 '[-7]',
                                                                 '[-8]',
                                                                 '[-9]',
                                                                 '[-10]',
                                                                 '[-11]',
                                                                 '[-12]'],
                                          'pipeline_python_script_relative_path': PosixPath('topollm/pipeline_scripts/run_pipeline_embeddings_data_prep_local_estimate.py'),
                                          'wandb_project': 'Topo_LLM_pipeline_jobs'},
                 'topo_llm_repository_base_path': '/Users/ruppik/git-source/Topo_LLM'},
 'tokenizer': {'add_prefix_space': False,
               'max_length': 512,
               'return_special_tokens_mask': True},
 'transformations': {'normalization': 'None'},
 'verbosity': <Verbosity.NORMAL: 1>,
 'wandb': {'dir': PosixPath('/Users/ruppik/git-source/Topo_LLM/wandb_output_dir/Topo_LLM_debug'),
           'entity': None,
           'project': 'Topo_LLM_debug',
           'tags': ['model_finteuning']}} (initialize_configuration_and_log.py:88)
[2024-07-17 16:19:48,970][    INFO][__main__] data_dir:
/Users/ruppik/git-source/Topo_LLM/data (run_load_saved_perplexity_and_compute_averages.py:89)
[2024-07-17 16:19:48,970][    INFO][__main__] ============= START dataset_subdir:
data-multiwoz21_split-validation_ctxt-dataset_entry_samples-10000_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:113)
[2024-07-17 16:19:48,970][    INFO][__main__] path_list:
[PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-multiwoz21_split-validation_ctxt-dataset_entry_samples-10000_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-roberta-base_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl'), PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-multiwoz21_split-validation_ctxt-dataset_entry_samples-10000_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5_ckpt-2800_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl')] (run_load_saved_perplexity_and_compute_averages.py:164)
[2024-07-17 16:19:49,240][    INFO][__main__] len(averages):
10000 (log_list_info.py:73)
[2024-07-17 16:19:49,240][    INFO][__main__] averages[:20]:
[1.7880761244369852,
 0.4702073988155462,
 0.9633403985434086,
 1.9467399100299347,
 3.146007498027757,
 1.5014593019408897,
 3.478485011961311,
 3.7287551694150483,
 1.6621270431363275,
 1.1348588538583664,
 1.9900663905045803,
 1.3928873494686045,
 1.5368466307279758,
 2.4788998289916075,
 2.705894395837496,
 3.7278218045830727,
 1.4394377374555916,
 0.489412046873688,
 3.4649636511291777,
 1.3516915596250327] (log_list_info.py:74)
[2024-07-17 16:19:49,241][    INFO][__main__] averages[-20:]:
[2.04982127864579,
 1.49627710909859,
 1.731930857790368,
 1.0108542580688664,
 0.9971953046430523,
 2.096484321711614,
 2.388390680918327,
 0.7993025915464387,
 1.3240995901728638,
 1.1510000994312577,
 1.9375439115683548,
 1.6352699971356397,
 3.4420110649532742,
 2.116341844757857,
 3.7041857540607452,
 3.2638550559990107,
 1.4359438857308735,
 1.0559834369007148,
 2.802161704759452,
 2.7952067446471616] (log_list_info.py:78)
[2024-07-17 16:19:49,247][    INFO][__main__] len(averages):
10000 (log_list_info.py:73)
[2024-07-17 16:19:49,247][    INFO][__main__] averages[:20]:
[0.4703042441348468,
 0.12306606154888869,
 0.16241353220510746,
 0.3592684098651356,
 2.1594136508647352,
 0.5831887365267094,
 1.9174103939076304,
 1.2222130057385325,
 0.3859817413416143,
 0.4082917820399995,
 0.45056035845664155,
 0.7634343212414879,
 0.24175459480223557,
 1.4560377977682755,
 1.7603809578717557,
 2.7271481087405443,
 0.12099601772691433,
 0.4013517371402651,
 0.9718651651951404,
 0.12178323719454057] (log_list_info.py:74)
[2024-07-17 16:19:49,247][    INFO][__main__] averages[-20:]:
[0.8426458363981164,
 0.7515306375760247,
 0.47830853898943004,
 0.20300564803947055,
 0.2768126881823668,
 0.6967187231166463,
 0.4077383787371218,
 0.5291590775518368,
 0.7298105050867072,
 0.3040296360462283,
 1.4532450046335725,
 1.0461586531123588,
 0.6865161175973703,
 0.9584489690615041,
 0.838830750350925,
 1.1623334743796476,
 0.15169273174098252,
 0.5187779661966487,
 1.8476799068726408,
 1.2385192446579987] (log_list_info.py:78)
[2024-07-17 16:19:49,248][    INFO][__main__] len(average_of_averages):
2 (log_list_info.py:73)
[2024-07-17 16:19:49,248][    INFO][__main__] average_of_averages[:20]:
[1.92974610629388, 0.783354237163783] (log_list_info.py:74)
[2024-07-17 16:19:49,248][    INFO][__main__] average_of_averages[-20:]:
[1.92974610629388, 0.783354237163783] (log_list_info.py:78)
[2024-07-17 16:19:49,248][    INFO][__main__] len(differences_of_averages):
10000 (log_list_info.py:73)
[2024-07-17 16:19:49,248][    INFO][__main__] differences_of_averages[:20]:
[-1.3177718803021383,
 -0.3471413372666575,
 -0.8009268663383011,
 -1.5874715001647992,
 -0.9865938471630216,
 -0.9182705654141803,
 -1.5610746180536808,
 -2.5065421636765155,
 -1.2761453017947133,
 -0.7265670718183669,
 -1.5395060320479388,
 -0.6294530282271166,
 -1.2950920359257403,
 -1.022862031223332,
 -0.9455134379657402,
 -1.0006736958425284,
 -1.3184417197286773,
 -0.08806030973342288,
 -2.4930984859340373,
 -1.229908322430492] (log_list_info.py:74)
[2024-07-17 16:19:49,248][    INFO][__main__] differences_of_averages[-20:]:
[-1.2071754422476737,
 -0.7447464715225653,
 -1.253622318800938,
 -0.8078486100293959,
 -0.7203826164606855,
 -1.3997655985949677,
 -1.980652302181205,
 -0.2701435139946019,
 -0.5942890850861566,
 -0.8469704633850295,
 -0.4842989069347823,
 -0.5891113440232809,
 -2.755494947355904,
 -1.1578928756963527,
 -2.86535500370982,
 -2.1015215816193633,
 -1.2842511539898909,
 -0.5372054707040661,
 -0.9544817978868112,
 -1.556687499989163] (log_list_info.py:78)
[2024-07-17 16:19:49,248][    INFO][__main__] average_of_differences_of_averages:
-1.1463918691300854 (compute_averages_over_loaded_data_list.py:109)
[2024-07-17 16:19:49,256][    INFO][__main__] average_difference_of_exps:
-13.233662262962191 (compute_averages_over_loaded_data_list.py:125)
[2024-07-17 16:19:49,261][    INFO][__main__] stat_test_linear_result_on_averages:
0.12353740211987048 (compute_averages_over_loaded_data_list.py:134)
[2024-07-17 16:19:49,261][    INFO][__main__] ============= END dataset_subdir:
data-multiwoz21_split-validation_ctxt-dataset_entry_samples-10000_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:185)
[2024-07-17 16:19:49,261][    INFO][__main__] ============= START dataset_subdir:
data-wikitext_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:113)
[2024-07-17 16:19:49,261][    INFO][__main__] path_list:
[PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-wikitext_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-roberta-base_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl'), PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-wikitext_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5_ckpt-2800_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl')] (run_load_saved_perplexity_and_compute_averages.py:164)
[2024-07-17 16:19:49,558][    INFO][__main__] len(averages):
3760 (log_list_info.py:73)
[2024-07-17 16:19:49,558][    INFO][__main__] averages[:20]:
[0.0,
 6.36088904467496,
 0.0,
 1.3099931618837914,
 0.0,
 5.195443638496929,
 0.0,
 1.2850565805003902,
 1.6739319253160776,
 1.4250660772714046,
 1.5460117415694867,
 2.125569158084214,
 2.3238621053918815,
 2.22555801370098,
 0.0,
 4.553221860527993,
 0.0,
 1.5740921604465545,
 1.5280368030366556,
 0.0] (log_list_info.py:74)
[2024-07-17 16:19:49,558][    INFO][__main__] averages[-20:]:
[0.0,
 5.085511010394177,
 0.0,
 1.6430084998780814,
 0.0,
 5.039418800920248,
 0.0,
 1.2523815245948526,
 0.9731919507795972,
 0.0,
 4.375361572951078,
 0.0,
 3.1859121827092842,
 0.0,
 5.226451205400129,
 0.0,
 0.0,
 5.177986256390189,
 0.0,
 0.0] (log_list_info.py:78)
[2024-07-17 16:19:49,562][    INFO][__main__] len(averages):
3760 (log_list_info.py:73)
[2024-07-17 16:19:49,562][    INFO][__main__] averages[:20]:
[0.0,
 6.508092891086232,
 0.0,
 1.951389010771948,
 0.0,
 3.719973166783651,
 0.0,
 1.7514050177649154,
 2.1419469029937104,
 2.239303425358579,
 2.4290811403543557,
 2.130542335918907,
 2.6297645361940716,
 2.377672820288821,
 0.0,
 4.940265727043152,
 0.0,
 2.4576475205997537,
 2.3066671073826877,
 0.0] (log_list_info.py:74)
[2024-07-17 16:19:49,562][    INFO][__main__] averages[-20:]:
[0.0,
 2.9311758198521356,
 0.0,
 2.251694630256207,
 0.0,
 5.363857614994049,
 0.0,
 1.9138254588782728,
 1.2129515491380896,
 0.0,
 4.9379359722137455,
 0.0,
 2.916122947095169,
 0.0,
 3.9053937594095864,
 0.0,
 0.0,
 3.8835408662756286,
 0.0,
 0.0] (log_list_info.py:78)
[2024-07-17 16:19:49,562][    INFO][__main__] len(average_of_averages):
2 (log_list_info.py:73)
[2024-07-17 16:19:49,562][    INFO][__main__] average_of_averages[:20]:
[1.9125691590594285, 2.148395356580405] (log_list_info.py:74)
[2024-07-17 16:19:49,562][    INFO][__main__] average_of_averages[-20:]:
[1.9125691590594285, 2.148395356580405] (log_list_info.py:78)
[2024-07-17 16:19:49,562][    INFO][__main__] len(differences_of_averages):
3760 (log_list_info.py:73)
[2024-07-17 16:19:49,562][    INFO][__main__] differences_of_averages[:20]:
[0.0,
 0.14720384641127193,
 0.0,
 0.6413958488881566,
 0.0,
 -1.4754704717132783,
 0.0,
 0.46634843726452524,
 0.4680149776776328,
 0.8142373480871745,
 0.883069398784869,
 0.004973177834692866,
 0.30590243080219004,
 0.15211480658784104,
 0.0,
 0.38704386651515943,
 0.0,
 0.8835553601531991,
 0.7786303043460321,
 0.0] (log_list_info.py:74)
[2024-07-17 16:19:49,562][    INFO][__main__] differences_of_averages[-20:]:
[0.0,
 -2.154335190542042,
 0.0,
 0.6086861303781255,
 0.0,
 0.32443881407380104,
 0.0,
 0.6614439342834202,
 0.23975959835849237,
 0.0,
 0.5625743992626671,
 0.0,
 -0.26978923561411516,
 0.0,
 -1.3210574459905424,
 0.0,
 0.0,
 -1.2944453901145603,
 0.0,
 0.0] (log_list_info.py:78)
[2024-07-17 16:19:49,563][    INFO][__main__] average_of_differences_of_averages:
0.23582619752098183 (compute_averages_over_loaded_data_list.py:109)
[2024-07-17 16:19:49,565][    INFO][__main__] average_difference_of_exps:
-91.3339498680266 (compute_averages_over_loaded_data_list.py:125)
[2024-07-17 16:19:49,566][    INFO][__main__] stat_test_linear_result_on_averages:
0.7124385876187149 (compute_averages_over_loaded_data_list.py:134)
[2024-07-17 16:19:49,566][    INFO][__main__] ============= END dataset_subdir:
data-wikitext_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:185)
[2024-07-17 16:19:49,566][    INFO][__main__] ============= START dataset_subdir:
data-iclr_2024_submissions_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:113)
[2024-07-17 16:19:49,566][    INFO][__main__] path_list:
[PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-iclr_2024_submissions_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-roberta-base_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl'), PosixPath('/Users/ruppik/git-source/Topo_LLM/data/embeddings/perplexity/data-iclr_2024_submissions_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags/lvl-token/add-prefix-space-False_max-len-512/model-model-roberta-base_task-MASKED_LM_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-linear-0.01-5_ckpt-2800_task-MASKED_LM/layer--1_agg-mean/norm-None/perplexity_dir/perplexity_results_list.jsonl')] (run_load_saved_perplexity_and_compute_averages.py:164)
[2024-07-17 16:19:49,725][    INFO][__main__] len(averages):
721 (log_list_info.py:73)
[2024-07-17 16:19:49,725][    INFO][__main__] averages[:20]:
[1.4180901805431907,
 1.9744839291752214,
 1.4560689412361063,
 1.9095572149168727,
 1.6773039869779691,
 1.4625810662422907,
 1.673759347864467,
 1.9753717391312222,
 1.7959949813051692,
 1.9412383093948187,
 1.8248554378520103,
 1.5509707570994355,
 1.4393102086909078,
 1.6015853699274027,
 1.4772024177179612,
 1.4911303500657596,
 1.237028512472656,
 1.6340557240363418,
 1.1081487852150411,
 1.5286499904282282] (log_list_info.py:74)
[2024-07-17 16:19:49,725][    INFO][__main__] averages[-20:]:
[1.5081523526716212,
 1.229602957520867,
 1.6489865211438288,
 1.203711604473947,
 1.8466188940835988,
 1.5294673856222247,
 1.1396110853116703,
 1.3826396584223128,
 1.6564319412616904,
 1.9390761555792804,
 1.534526867094159,
 1.604921471257841,
 1.3697015273854771,
 1.24403538704566,
 1.1712348218079383,
 1.1896349013771839,
 1.6181837354336892,
 1.519118475165583,
 1.4848002236520712,
 1.716748316428136] (log_list_info.py:78)
[2024-07-17 16:19:49,726][    INFO][__main__] len(averages):
721 (log_list_info.py:73)
[2024-07-17 16:19:49,727][    INFO][__main__] averages[:20]:
[2.2739536804559046,
 2.7599466411303224,
 2.0493938602508694,
 2.5723344126528276,
 2.190820200501808,
 1.9637081781362185,
 2.5917127915110614,
 2.727515112686366,
 2.5745061317268756,
 2.9663975176865423,
 2.5946155523692225,
 2.280197449353616,
 2.3068988258489496,
 2.354031367559963,
 2.4930070178108994,
 2.417757068933564,
 1.9678033979648966,
 2.4284358900583114,
 1.5195309958934335,
 2.4194433811906415] (log_list_info.py:74)
[2024-07-17 16:19:49,727][    INFO][__main__] averages[-20:]:
[2.314592121629941,
 1.9727645604666877,
 2.4519241458342633,
 1.811761097627205,
 2.556902877920762,
 2.11780030363738,
 1.832326051146273,
 2.0121545265601526,
 2.5538951635721316,
 2.006183530438454,
 2.234138177049601,
 2.395288045634857,
 1.9451645779258993,
 1.8262594607505827,
 1.944684278763252,
 2.0225335417565096,
 2.21319207141183,
 2.2948629510578313,
 2.2356820360908873,
 2.512321596710103] (log_list_info.py:78)
[2024-07-17 16:19:49,727][    INFO][__main__] len(average_of_averages):
2 (log_list_info.py:73)
[2024-07-17 16:19:49,727][    INFO][__main__] average_of_averages[:20]:
[1.562841038885887, 2.238769582123767] (log_list_info.py:74)
[2024-07-17 16:19:49,727][    INFO][__main__] average_of_averages[-20:]:
[1.562841038885887, 2.238769582123767] (log_list_info.py:78)
[2024-07-17 16:19:49,727][    INFO][__main__] len(differences_of_averages):
721 (log_list_info.py:73)
[2024-07-17 16:19:49,727][    INFO][__main__] differences_of_averages[:20]:
[0.8558634999127139,
 0.785462711955101,
 0.5933249190147631,
 0.6627771977359549,
 0.5135162135238389,
 0.5011271118939278,
 0.9179534436465944,
 0.7521433735551437,
 0.7785111504217064,
 1.0251592082917236,
 0.7697601145172122,
 0.7292266922541804,
 0.8675886171580418,
 0.7524459976325601,
 1.0158046000929382,
 0.9266267188678046,
 0.7307748854922405,
 0.7943801660219696,
 0.4113822106783924,
 0.8907933907624133] (log_list_info.py:74)
[2024-07-17 16:19:49,727][    INFO][__main__] differences_of_averages[-20:]:
[0.8064397689583198,
 0.7431616029458208,
 0.8029376246904345,
 0.6080494931532578,
 0.7102839838371631,
 0.5883329180151553,
 0.6927149658346026,
 0.6295148681378397,
 0.8974632223104413,
 0.06710737485917373,
 0.699611309955442,
 0.7903665743770161,
 0.5754630505404221,
 0.5822240737049227,
 0.7734494569553136,
 0.8328986403793257,
 0.5950083359781408,
 0.7757444758922483,
 0.7508818124388161,
 0.795573280281967] (log_list_info.py:78)
[2024-07-17 16:19:49,727][    INFO][__main__] average_of_differences_of_averages:
0.6759285432378751 (compute_averages_over_loaded_data_list.py:109)
[2024-07-17 16:19:49,728][    INFO][__main__] average_difference_of_exps:
4.917307164038316 (compute_averages_over_loaded_data_list.py:125)
[2024-07-17 16:19:49,728][    INFO][__main__] stat_test_linear_result_on_averages:
0.0004914175849526446 (compute_averages_over_loaded_data_list.py:134)
[2024-07-17 16:19:49,728][    INFO][__main__] ============= END dataset_subdir:
data-iclr_2024_submissions_split-validation_ctxt-dataset_entry_samples--1_feat-col-ner_tags (run_load_saved_perplexity_and_compute_averages.py:185)
[2024-07-17 16:19:49,728][    INFO][__main__] Running script DONE (run_load_saved_perplexity_and_compute_averages.py:190)
