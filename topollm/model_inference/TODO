# TODO: The following suggestion is raised in a warning for the huggingface pipeline.

--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/logging/__init__.py", line 1100, in emit
    msg = self.format(record)
  File "/opt/conda/lib/python3.10/logging/__init__.py", line 943, in format
    return fmt.format(record)
  File "/opt/conda/lib/python3.10/logging/__init__.py", line 678, in format
    record.message = record.getMessage()
  File "/opt/conda/lib/python3.10/logging/__init__.py", line 368, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/benjamin_ruppik/git-source/Topo_LLM/topollm/model_inference/run_inference_pipeline.py", line 78, in <module>
    main()
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/benjamin_ruppik/git-source/Topo_LLM/topollm/model_inference/run_inference_pipeline.py", line 68, in main
    do_inference(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/topollm/model_inference/do_inference.py", line 131, in do_inference
    results = do_text_generation(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/topollm/model_inference/causal_language_modeling/do_text_generation.py", line 81, in do_text_generation
    results: list[dict] = text_generation_pipeline(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py", line 240, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py", line 1167, in __call__
    logger.warning_once(
  File "/home/benjamin_ruppik/git-source/Topo_LLM/.venv/lib/python3.10/site-packages/transformers/utils/logging.py", line 329, in warning_once
    self.warning(*args, **kwargs)
Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'
Arguments: (<class 'UserWarning'>,)