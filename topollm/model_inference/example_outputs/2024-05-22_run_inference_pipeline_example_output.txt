Calling python script.
Setting HYDRA_FULL_ERROR environment variable to '1'.
os.environ['HYDRA_FULL_ERROR'] = '1'
[2024-05-21 23:36:23,661][HYDRA] Launching 1 jobs locally
[2024-05-21 23:36:23,661][HYDRA] 	#0 : language_model=roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset preferred_torch_backend=auto
[2024-05-21 23:36:23,936][    INFO][__main__] Running script ... (run_inference_pipeline.py:64)
[2024-05-21 23:36:23,954][    INFO][__main__] get_git_info() = 'main/700244a0d2893d6de8f9a79de619e7987b42c242' (initialize_configuration_and_log.py:83)
[2024-05-21 23:36:23,954][    INFO][__main__] Working directory:
os.getcwd() = '$HOME/git-source/Topo_LLM/topollm/model_inference' (initialize_configuration_and_log.py:67)
[2024-05-21 23:36:23,954][    INFO][__main__] Hydra output directory:
$HOME/git-source/Topo_LLM/hydra_output_dir/multirun/run_inference_pipeline/2024-05-21/23-36-23/language_model=roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset,preferred_torch_backend=auto/seed=1234 (initialize_configuration_and_log.py:68)
[2024-05-21 23:36:23,955][    INFO][__main__] hydra config:
{'data': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 5000, 'split': 'train', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}, 'embeddings': {'dataset_map': {'batch_size': 1000, 'num_proc': 1}, 'embedding_extraction': {'layer_indices': [-1], 'aggregation': 'mean'}, 'batch_size': 32, 'level': 'token', 'num_workers': 0}, 'finetuning': {'lm_mode': 'MLM', 'pretrained_model_name_or_path': 'roberta-base', 'short_model_name': '${finetuning.pretrained_model_name_or_path}', 'tokenizer_modifier': {'mode': 'do_nothing', 'padding_token': '<pad>'}, 'gradient_modifier': {'mode': 'do_nothing'}, 'finetuning_datasets': {'eval_dataset': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 10000, 'split': 'validation', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}, 'train_dataset': {'data_split': {'data_split_mode': 'proportions', 'proportions': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}, 'number_of_samples': 10000, 'split': 'train', 'column_name': 'body', 'context': 'dataset_entry', 'dataset_description_string': 'one-year-of-tsla-on-reddit', 'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit', 'dataset_name': 'comments', 'dataset_type': 'huggingface_dataset'}}, 'batch_sizes': {'train': 16, 'eval': 32}, 'eval_steps': 400, 'fp16': False, 'gradient_accumulation_steps': 2, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'linear', 'log_level': 'info', 'logging_steps': 100, 'max_length': 512, 'max_steps': -1, 'mlm_probability': 0.15, 'num_train_epochs': 5, 'save_steps': 400, 'use_cpu': False, 'warmup_steps': 500, 'weight_decay': 0.01, 'tokenizer': {'add_prefix_space': False, 'max_length': 512}, 'peft': {'finetuning_mode': 'standard'}}, 'inference': {'max_length': 100, 'num_return_sequences': 3}, 'language_model': {'tokenizer_modifier': {'mode': 'do_nothing', 'padding_token': '<pad>'}, 'checkpoint_no': 8500, 'lm_mode': 'MLM', 'masking_mode': 'no_masking', 'pretrained_model_name_or_path': '${paths.data_dir}/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-${language_model.checkpoint_no}', 'short_model_name': 'roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset_checkpoint-${language_model.checkpoint_no}'}, 'paths': {'data_dir': '${paths.repository_base_path}/data', 'repository_base_path': '${oc.env:TOPO_LLM_REPOSITORY_BASE_PATH}'}, 'storage': {'array_storage_type': 'zarr', 'chunk_size': 512, 'metadata_storage_type': 'pickle'}, 'tokenizer': {'add_prefix_space': False, 'max_length': 512}, 'transformations': {'normalization': 'None', 'rotation': 'None'}, 'embeddings_data_prep': {'num_samples': 30000}, 'seed': 1234, 'verbosity': 1, 'preferred_torch_backend': 'auto'} (initialize_configuration_and_log.py:69)
[2024-05-21 23:36:23,957][    INFO][__main__] main_config:
{'data': {'column_name': 'body',
          'context': 'dataset_entry',
          'data_dir': None,
          'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                         'proportions': {'test': 0.1,
                                         'train': 0.8,
                                         'validation': 0.1}},
          'dataset_description_string': 'one-year-of-tsla-on-reddit',
          'dataset_name': 'comments',
          'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
          'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
          'number_of_samples': 5000,
          'split': <Split.TRAIN: 'train'>},
 'embeddings': {'batch_size': 32,
                'dataset_map': {'batch_size': 1000, 'num_proc': 1},
                'embedding_extraction': {'aggregation': <AggregationType.MEAN: 'mean'>,
                                         'layer_indices': [-1]},
                'level': <Level.TOKEN: 'token'>,
                'num_workers': 0},
 'embeddings_data_prep': {'num_samples': 30000},
 'finetuning': {'batch_sizes': {'eval': 32, 'train': 16},
                'eval_steps': 400,
                'finetuning_datasets': {'eval_dataset': {'column_name': 'body',
                                                         'context': 'dataset_entry',
                                                         'data_dir': None,
                                                         'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                                                                        'proportions': {'test': 0.1,
                                                                                        'train': 0.8,
                                                                                        'validation': 0.1}},
                                                         'dataset_description_string': 'one-year-of-tsla-on-reddit',
                                                         'dataset_name': 'comments',
                                                         'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
                                                         'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
                                                         'number_of_samples': 10000,
                                                         'split': <Split.VALIDATION: 'validation'>},
                                        'train_dataset': {'column_name': 'body',
                                                          'context': 'dataset_entry',
                                                          'data_dir': None,
                                                          'data_split': {'data_split_mode': <DataSplitMode.PROPORTIONS: 'proportions'>,
                                                                         'proportions': {'test': 0.1,
                                                                                         'train': 0.8,
                                                                                         'validation': 0.1}},
                                                          'dataset_description_string': 'one-year-of-tsla-on-reddit',
                                                          'dataset_name': 'comments',
                                                          'dataset_path': 'SocialGrep/one-year-of-tsla-on-reddit',
                                                          'dataset_type': <DatasetType.HUGGINGFACE_DATASET: 'huggingface_dataset'>,
                                                          'number_of_samples': 10000,
                                                          'split': <Split.TRAIN: 'train'>}},
                'fp16': False,
                'gradient_accumulation_steps': 2,
                'gradient_checkpointing': True,
                'gradient_modifier': {'mode': <GradientModifierMode.DO_NOTHING: 'do_nothing'>,
                                      'target_modules_to_freeze': []},
                'learning_rate': 5e-05,
                'lm_mode': <LMmode.MLM: 'MLM'>,
                'log_level': 'info',
                'logging_steps': 100,
                'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,
                'max_length': 512,
                'max_steps': -1,
                'mlm_probability': 0.15,
                'num_train_epochs': 5,
                'peft': {'finetuning_mode': <FinetuningMode.STANDARD: 'standard'>,
                         'lora_alpha': 32,
                         'lora_dropout': 0.01,
                         'r': 8,
                         'target_modules': ['query', 'key', 'value']},
                'pretrained_model_name_or_path': 'roberta-base',
                'save_steps': 400,
                'short_model_name': 'roberta-base',
                'tokenizer': {'add_prefix_space': False, 'max_length': 512},
                'tokenizer_modifier': {'mode': <TokenizerModifierMode.DO_NOTHING: 'do_nothing'>,
                                       'padding_token': '<pad>'},
                'use_cpu': False,
                'warmup_steps': 500,
                'weight_decay': 0.01},
 'inference': {'max_length': 100, 'num_return_sequences': 3},
 'language_model': {'lm_mode': <LMmode.MLM: 'MLM'>,
                    'masking_mode': 'no_masking',
                    'pretrained_model_name_or_path': '$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500',
                    'short_model_name': 'roberta-base_finetuned-on-multiwoz21_ftm-standard_full-dataset_checkpoint-8500',
                    'tokenizer_modifier': {'mode': <TokenizerModifierMode.DO_NOTHING: 'do_nothing'>,
                                           'padding_token': '<pad>'}},
 'paths': {'data_dir': PosixPath('$HOME/git-source/Topo_LLM/data'),
           'repository_base_path': PosixPath('$HOME/git-source/Topo_LLM')},
 'preferred_torch_backend': <PreferredTorchBackend.AUTO: 'auto'>,
 'seed': 1234,
 'storage': {'array_storage_type': <ArrayStorageType.ZARR: 'zarr'>,
             'chunk_size': 512,
             'metadata_storage_type': <MetadataStorageType.PICKLE: 'pickle'>},
 'tokenizer': {'add_prefix_space': False, 'max_length': 512},
 'transformations': {'normalization': 'None'},
 'verbosity': <Verbosity.NORMAL: 1>} (initialize_configuration_and_log.py:73)
[2024-05-21 23:36:23,989][    INFO][__main__] device = device(type='cuda') (get_torch_device.py:63)
[2024-05-21 23:36:23,990][    INFO][__main__] Loading tokenizer pretrained_model_name_or_path = '$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500' ... (load_tokenizer.py:52)
[2024-05-21 23:36:24,070][    INFO][__main__] Loading tokenizer pretrained_model_name_or_path = '$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500' DONE (load_tokenizer.py:60)
[2024-05-21 23:36:24,070][    INFO][__main__] tokenizer:
RobertaTokenizerFast(name_or_path='$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),
} (load_tokenizer.py:61)
[2024-05-21 23:36:24,071][    INFO][__main__] mode = <TokenizerModifierMode.DO_NOTHING: 'do_nothing'> (factory.py:54)
[2024-05-21 23:36:24,071][    INFO][__main__] Returning unmodified tokenizer. (tokenizer_modifier_do_nothing.py:53)
[2024-05-21 23:36:24,071][    INFO][__main__] Loading model pretrained_model_name_or_path = '$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500' ... (load_model.py:66)
[2024-05-21 23:36:24,584][    INFO][__main__] Loading model pretrained_model_name_or_path = '$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500' DONE (load_model.py:81)
[2024-05-21 23:36:24,585][    INFO][__main__] Moving model to device = device(type='cuda') ... (load_model.py:93)
[2024-05-21 23:36:24,853][    INFO][__main__] Moving model to device = device(type='cuda') DONE (load_model.py:103)
[2024-05-21 23:36:24,854][    INFO][__main__] device:
cuda (load_model.py:106)
[2024-05-21 23:36:24,854][    INFO][__main__] type(model) = <class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'> (log_model_info.py:44)
[2024-05-21 23:36:24,855][    INFO][__main__] model:
RobertaForMaskedLM(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (lm_head): RobertaLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (decoder): Linear(in_features=768, out_features=50265, bias=True)
  )
) (log_model_info.py:47)
[2024-05-21 23:36:24,856][    INFO][__main__] model.config:
RobertaConfig {
  "_name_or_path": "$HOME/git-source/Topo_LLM/data/models/finetuned_models/data-multiwoz21_split-train_ctxt-dataset_entry_samples--1/model-roberta-base/ftm-standard/lora-None/gradmod-do_nothing_target-freeze-[]/lr-5e-05_lr_scheduler_type-linear_wd-0.01/bs-train-32/ep-5/model_files/checkpoint-8500",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.39.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}
 (log_model_info.py:55)
[2024-05-21 23:36:24,857][    INFO][__main__] prompts:
['I am looking for a <mask>.',
 'I am looking for a <mask>, can you help me?',
 'Can you find me a <mask>?',
 'I would like a <mask> hotel in the center of town, please.',
 '<mask> is a cheap restaurant in the south of town.',
 'The train should go to <mask>.',
 'No, it should be <mask>, look again!',
 '<mask> is a city in the south of England.',
 '<mask> is the best city in the world.',
 'I would like to invest in <mask>.',
 'What is the best <mask> in town?',
 'Can you recommend a good <mask>?'] (do_inference.py:79)
[2024-05-21 23:36:25,340][    INFO][__main__] prompts:
['I am looking for a <mask>.',
 'I am looking for a <mask>, can you help me?',
 'Can you find me a <mask>?',
 'I would like a <mask> hotel in the center of town, please.',
 '<mask> is a cheap restaurant in the south of town.',
 'The train should go to <mask>.',
 'No, it should be <mask>, look again!',
 '<mask> is a city in the south of England.',
 '<mask> is the best city in the world.',
 'I would like to invest in <mask>.',
 'What is the best <mask> in town?',
 'Can you recommend a good <mask>?'] (do_fill_mask.py:55)
[2024-05-21 23:36:25,597][    INFO][__main__] result:
[[{'score': 0.27983424067497253,
   'sequence': 'I am looking for a restaurant.',
   'token': 2391,
   'token_str': ' restaurant'},
  {'score': 0.26953569054603577,
   'sequence': 'I am looking for a train.',
   'token': 2341,
   'token_str': ' train'},
  {'score': 0.18918341398239136,
   'sequence': 'I am looking for a hotel.',
   'token': 2303,
   'token_str': ' hotel'},
  {'score': 0.05978425592184067,
   'sequence': 'I am looking for a museum.',
   'token': 5707,
   'token_str': ' museum'},
  {'score': 0.05505136400461197,
   'sequence': 'I am looking for a hospital.',
   'token': 1098,
   'token_str': ' hospital'}],
 [{'score': 0.3956519067287445,
   'sequence': 'I am looking for a train, can you help me?',
   'token': 2341,
   'token_str': ' train'},
  {'score': 0.2854473888874054,
   'sequence': 'I am looking for a restaurant, can you help me?',
   'token': 2391,
   'token_str': ' restaurant'},
  {'score': 0.15053357183933258,
   'sequence': 'I am looking for a hotel, can you help me?',
   'token': 2303,
   'token_str': ' hotel'},
  {'score': 0.049869321286678314,
   'sequence': 'I am looking for a hospital, can you help me?',
   'token': 1098,
   'token_str': ' hospital'},
  {'score': 0.03224537521600723,
   'sequence': 'I am looking for a taxi, can you help me?',
   'token': 9955,
   'token_str': ' taxi'}],
 [{'score': 0.4397919774055481,
   'sequence': 'Can you find me a train?',
   'token': 2341,
   'token_str': ' train'},
  {'score': 0.1846712827682495,
   'sequence': 'Can you find me a hotel?',
   'token': 2303,
   'token_str': ' hotel'},
  {'score': 0.11879655718803406,
   'sequence': 'Can you find me a restaurant?',
   'token': 2391,
   'token_str': ' restaurant'},
  {'score': 0.06085463985800743,
   'sequence': 'Can you find me a hospital?',
   'token': 1098,
   'token_str': ' hospital'},
  {'score': 0.05021219328045845,
   'sequence': 'Can you find me a museum?',
   'token': 5707,
   'token_str': ' museum'}],
 [{'score': 0.8512140512466431,
   'sequence': 'I would like a cheap hotel in the center of town, please.',
   'token': 6162,
   'token_str': ' cheap'},
  {'score': 0.04264480248093605,
   'sequence': 'I would like a nice hotel in the center of town, please.',
   'token': 2579,
   'token_str': ' nice'},
  {'score': 0.02305682748556137,
   'sequence': 'I would like a expensive hotel in the center of town, please.',
   'token': 3214,
   'token_str': ' expensive'},
  {'score': 0.019055917859077454,
   'sequence': 'I would like a moderate hotel in the center of town, please.',
   'token': 7212,
   'token_str': ' moderate'},
  {'score': 0.013648958876729012,
   'sequence': 'I would like a good hotel in the center of town, please.',
   'token': 205,
   'token_str': ' good'}],
 [{'score': 0.42055508494377136,
   'sequence': 'It is a cheap restaurant in the south of town.',
   'token': 243,
   'token_str': 'It'},
  {'score': 0.35598495602607727,
   'sequence': 'That is a cheap restaurant in the south of town.',
   'token': 1711,
   'token_str': 'That'},
  {'score': 0.07969814538955688,
   'sequence': 'This is a cheap restaurant in the south of town.',
   'token': 713,
   'token_str': 'This'},
  {'score': 0.038045745342969894,
   'sequence': 'that is a cheap restaurant in the south of town.',
   'token': 6025,
   'token_str': 'that'},
  {'score': 0.03047792613506317,
   'sequence': 'There is a cheap restaurant in the south of town.',
   'token': 970,
   'token_str': 'There'}],
 [{'score': 0.7711597681045532,
   'sequence': 'The train should go to Cambridge.',
   'token': 6912,
   'token_str': ' Cambridge'},
  {'score': 0.086140938103199,
   'sequence': 'The train should go to Norwich.',
   'token': 18749,
   'token_str': ' Norwich'},
  {'score': 0.07415219396352768,
   'sequence': 'The train should go to Leicester.',
   'token': 9035,
   'token_str': ' Leicester'},
  {'score': 0.05810367316007614,
   'sequence': 'The train should go to Ely.',
   'token': 21294,
   'token_str': ' Ely'},
  {'score': 0.0019479923648759723,
   'sequence': 'The train should go to Birmingham.',
   'token': 8353,
   'token_str': ' Birmingham'}],
 [{'score': 0.43012484908103943,
   'sequence': 'No, it should be expensive, look again!',
   'token': 3214,
   'token_str': ' expensive'},
  {'score': 0.19814378023147583,
   'sequence': 'No, it should be cheap, look again!',
   'token': 6162,
   'token_str': ' cheap'},
  {'score': 0.1125105768442154,
   'sequence': 'No, it should be free, look again!',
   'token': 481,
   'token_str': ' free'},
  {'score': 0.025016577914357185,
   'sequence': 'No, it should be cheaper, look again!',
   'token': 7246,
   'token_str': ' cheaper'},
  {'score': 0.01889764703810215,
   'sequence': 'No, it should be Italian, look again!',
   'token': 3108,
   'token_str': ' Italian'}],
 [{'score': 0.6572988033294678,
   'sequence': 'It is a city in the south of England.',
   'token': 243,
   'token_str': 'It'},
  {'score': 0.18671338260173798,
   'sequence': 'That is a city in the south of England.',
   'token': 1711,
   'token_str': 'That'},
  {'score': 0.05698157846927643,
   'sequence': 'This is a city in the south of England.',
   'token': 713,
   'token_str': 'This'},
  {'score': 0.02327141910791397,
   'sequence': 'that is a city in the south of England.',
   'token': 6025,
   'token_str': 'that'},
  {'score': 0.008282736875116825,
   'sequence': 'Hamilton is a city in the south of England.',
   'token': 31382,
   'token_str': 'Hamilton'}],
 [{'score': 0.5808174014091492,
   'sequence': 'It is the best city in the world.',
   'token': 243,
   'token_str': 'It'},
  {'score': 0.17020152509212494,
   'sequence': 'That is the best city in the world.',
   'token': 1711,
   'token_str': 'That'},
  {'score': 0.09078695625066757,
   'sequence': 'This is the best city in the world.',
   'token': 713,
   'token_str': 'This'},
  {'score': 0.048611950129270554,
   'sequence': 'that is the best city in the world.',
   'token': 6025,
   'token_str': 'that'},
  {'score': 0.026167048141360283,
   'sequence': 'it is the best city in the world.',
   'token': 405,
   'token_str': 'it'}],
 [{'score': 0.1782095730304718,
   'sequence': 'I would like to invest in entertainment.',
   'token': 4000,
   'token_str': ' entertainment'},
  {'score': 0.149213969707489,
   'sequence': 'I would like to invest in architecture.',
   'token': 9437,
   'token_str': ' architecture'},
  {'score': 0.08147785067558289,
   'sequence': 'I would like to invest in college.',
   'token': 1564,
   'token_str': ' college'},
  {'score': 0.0757036805152893,
   'sequence': 'I would like to invest in colleges.',
   'token': 8975,
   'token_str': ' colleges'},
  {'score': 0.0597241036593914,
   'sequence': 'I would like to invest in boats.',
   'token': 8934,
   'token_str': ' boats'}],
 [{'score': 0.35204970836639404,
   'sequence': 'What is the best restaurant in town?',
   'token': 2391,
   'token_str': ' restaurant'},
  {'score': 0.12862585484981537,
   'sequence': 'What is the best museum in town?',
   'token': 5707,
   'token_str': ' museum'},
  {'score': 0.0938398614525795,
   'sequence': 'What is the best place in town?',
   'token': 317,
   'token_str': ' place'},
  {'score': 0.07956046611070633,
   'sequence': 'What is the best one in town?',
   'token': 65,
   'token_str': ' one'},
  {'score': 0.0541800819337368,
   'sequence': 'What is the best attraction in town?',
   'token': 13003,
   'token_str': ' attraction'}],
 [{'score': 0.4936714768409729,
   'sequence': 'Can you recommend a good one?',
   'token': 65,
   'token_str': ' one'},
  {'score': 0.13961222767829895,
   'sequence': 'Can you recommend a good museum?',
   'token': 5707,
   'token_str': ' museum'},
  {'score': 0.1127130463719368,
   'sequence': 'Can you recommend a good restaurant?',
   'token': 2391,
   'token_str': ' restaurant'},
  {'score': 0.0497182197868824,
   'sequence': 'Can you recommend a good place?',
   'token': 317,
   'token_str': ' place'},
  {'score': 0.044365059584379196,
   'sequence': 'Can you recommend a good attraction?',
   'token': 13003,
   'token_str': ' attraction'}]] (do_fill_mask.py:60)
[2024-05-21 23:36:25,607][    INFO][__main__] Running script DONE (run_inference_pipeline.py:77)
Finished running the script.
