{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from topollm.analysis.compare_sampling_methods.make_plots import (\n",
    "    create_boxplot_of_mean_over_different_sampling_seeds,\n",
    "    generate_fixed_params_text,\n",
    ")\n",
    "from topollm.analysis.compare_sampling_methods.run_general_comparisons import (\n",
    "    Y_AXIS_LIMITS_ONLY_FULL,\n",
    "    filter_dataframe_based_on_filters_dict,\n",
    ")\n",
    "from topollm.config_classes.constants import TOPO_LLM_REPOSITORY_BASE_PATH\n",
    "\n",
    "default_logger: logging.Logger = logging.getLogger(name=__name__)\n",
    "# Add stdout handler to default logger\n",
    "default_logger.addHandler(\n",
    "    hdlr=logging.StreamHandler(\n",
    "        stream=sys.stdout,\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger: logging.Logger = default_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_folder_base_path = pathlib.Path(\n",
    "    TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "    \"data/analysis/sample_sizes/run_general_comparisons/array_truncation_size=5000/analysis/twonn/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_saved_dataframes(\n",
    "    root_dir: pathlib.Path = comparisons_folder_base_path,\n",
    "    pattern: str = \"full_local_estimates_df.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    # Initialize an empty list to store dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Traverse the directory structure using pathlib's rglob\n",
    "    for file_path in root_dir.rglob(pattern=pattern):\n",
    "        # Load the CSV file into a dataframe\n",
    "        df = None\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath_or_buffer=file_path, keep_default_na=False)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    if dfs:\n",
    "        concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"No files found with pattern {pattern = } in {root_dir = }\")\n",
    "        print(\"Returning empty dataframe.\")\n",
    "        concatenated_df = pd.DataFrame()  # Empty dataframe if no files found\n",
    "\n",
    "    # Save the concatenated dataframe\n",
    "    output_path = pathlib.Path(root_dir, \"concatenated_full_local_estimates_df.csv\")\n",
    "    concatenated_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Saved concatenated dataframe to {output_path = }\")\n",
    "    print(f\"{concatenated_df[\"model_partial_name\"].unique() = }\")\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "\n",
    "def create_histograms_over_model_checkpoints(\n",
    "    concatenated_df: pd.DataFrame,\n",
    "    concatenated_filters_dict: dict,\n",
    "    plot_save_path: pathlib.Path | None = None,\n",
    "    raw_data_save_path: pathlib.Path | None = None,\n",
    ") -> None:\n",
    "    filtered_concatenated_df = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Filter for the dataframe with just the base model data\n",
    "\n",
    "    same_filters_but_for_base_model = concatenated_filters_dict.copy()\n",
    "    same_filters_but_for_base_model[\"model_partial_name\"] = \"model-roberta-base\"\n",
    "\n",
    "    filtered_for_base_model_concatenated_df = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=same_filters_but_for_base_model,\n",
    "    )\n",
    "\n",
    "    # Set all the values in the \"model_checkpoint\" column to \"-1\"\n",
    "    filtered_for_base_model_concatenated_df[\"model_checkpoint\"] = -1\n",
    "\n",
    "    # # # #\n",
    "    # Create a dataframe by concatenating the two dataframes\n",
    "    data_for_checkpoint_analysis_df: DataFrame = pd.concat(\n",
    "        objs=[filtered_concatenated_df, filtered_for_base_model_concatenated_df],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Group \"data_for_checkpoint_analysis_df\" by value in 'model_checkpoint' column\n",
    "    # and make a boxplot of \"array_data_truncated_mean\" for each group\n",
    "\n",
    "    fixed_params_text: str = generate_fixed_params_text(\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "    )\n",
    "\n",
    "    for y_min, y_max in Y_AXIS_LIMITS_ONLY_FULL.values():\n",
    "        # for y_min, y_max in [(6.0, 10.0)]:\n",
    "        create_boxplot_of_mean_over_different_sampling_seeds(\n",
    "            subset_local_estimates_df=data_for_checkpoint_analysis_df,\n",
    "            plot_save_path=plot_save_path,\n",
    "            raw_data_save_path=raw_data_save_path,\n",
    "            x_column_name=\"model_checkpoint\",\n",
    "            y_column_name=\"array_data_truncated_mean\",\n",
    "            fixed_params_text=fixed_params_text,\n",
    "            y_min=y_min,\n",
    "            y_max=y_max,\n",
    "            logger=logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_list_to_process = [\n",
    "    \"data-multiwoz21_split-train_ctxt-dataset_entry_samples-10000_feat-col-ner_tags\",\n",
    "    \"data-multiwoz21_split-validation_ctxt-dataset_entry_samples-3000_feat-col-ner_tags\",\n",
    "    \"data-multiwoz21_split-test_ctxt-dataset_entry_samples-3000_feat-col-ner_tags\",\n",
    "    \"data-one-year-of-tsla-on-reddit_split-train_ctxt-dataset_entry_samples-10000_feat-col-ner_tags\",\n",
    "    \"data-one-year-of-tsla-on-reddit_split-validation_ctxt-dataset_entry_samples-3000_feat-col-ner_tags\",\n",
    "    \"data-one-year-of-tsla-on-reddit_split-test_ctxt-dataset_entry_samples-3000_feat-col-ner_tags\",\n",
    "]\n",
    "\n",
    "model_partial_name_list_to_process = [\n",
    "    \"model-model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "    # \"model-model-roberta-base_task-masked_lm_one-year-of-tsla-on-reddit-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "]\n",
    "\n",
    "concatenated_df: DataFrame = load_and_concatenate_saved_dataframes()\n",
    "\n",
    "\n",
    "for data_full, model_partial_name in product(data_full_list_to_process, model_partial_name_list_to_process):\n",
    "    concatenated_filters_dict = {\n",
    "        \"data_full\": data_full,\n",
    "        \"data_prep_sampling_method\": \"random\",\n",
    "        \"deduplication\": \"array_deduplicator\",\n",
    "        \"model_partial_name\": model_partial_name,\n",
    "        \"n_neighbors\": 128,\n",
    "        \"data_prep_sampling_samples\": 100_000,\n",
    "        \"local_estimates_samples\": 60_000,\n",
    "    }\n",
    "\n",
    "    create_histograms_over_model_checkpoints(\n",
    "        concatenated_df=concatenated_df,\n",
    "        concatenated_filters_dict=concatenated_filters_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_list: list[str] = [\n",
    "    \"data-multiwoz21_split-train_ctxt-dataset_entry_samples-10000_feat-col-ner_tags/\",\n",
    "    \"data-multiwoz21_split-validation_ctxt-dataset_entry_samples-3000_feat-col-ner_tags/\",\n",
    "    \"data-multiwoz21_split-test_ctxt-dataset_entry_samples-3000_feat-col-ner_tags/\",\n",
    "    \"data-one-year-of-tsla-on-reddit_split-train_ctxt-dataset_entry_samples-10000_feat-col-ner_tags/\",\n",
    "    \"data-one-year-of-tsla-on-reddit_split-validation_ctxt-dataset_entry_samples-3000_feat-col-ner_tags/\",\n",
    "]\n",
    "\n",
    "model_folder_list: list[str] = [\n",
    "    \"model-roberta-base_task-masked_lm/\",\n",
    "]\n",
    "\n",
    "selected_data_folder = data_folder_list[4]\n",
    "selected_model_folder = model_folder_list[0]\n",
    "\n",
    "file_path = pathlib.Path(\n",
    "    comparisons_folder_base_path,\n",
    "    selected_data_folder,\n",
    "    \"lvl-token/add-prefix-space-True_max-len-512/\",\n",
    "    selected_model_folder,\n",
    "    \"layer--1_agg-mean/norm-None/\",\n",
    "    \"full_local_estimates_df.csv\",\n",
    ")\n",
    "\n",
    "results_base_directory_path: pathlib.Path = file_path.parent\n",
    "\n",
    "local_estimates_df: pd.DataFrame = pd.read_csv(\n",
    "    filepath_or_buffer=file_path,\n",
    ")\n",
    "\n",
    "# Select a subset of the data with the same parameters.\n",
    "# This allows comparing over different seeds.\n",
    "#\n",
    "# We do not fix the local_estimates_samples,\n",
    "# since we want to compare the results for different sample sizes.\n",
    "\n",
    "filters_dict = {\n",
    "    \"data_prep_sampling_method\": \"random\",\n",
    "    \"deduplication\": \"array_deduplicator\",\n",
    "    \"n_neighbors\": 128,\n",
    "    \"data_prep_sampling_samples\": 50000,\n",
    "}\n",
    "\n",
    "subset_local_estimates_df = filter_dataframe_based_on_filters_dict(df=local_estimates_df, filters_dict=filters_dict)\n",
    "\n",
    "subset_local_estimates_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
