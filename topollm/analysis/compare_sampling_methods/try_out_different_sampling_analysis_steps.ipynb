{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "from topollm.analysis.compare_sampling_methods.filter_dataframe_based_on_filters_dict import (\n",
    "    filter_dataframe_based_on_filters_dict,\n",
    ")\n",
    "from topollm.analysis.compare_sampling_methods.load_and_concatenate_saved_dataframes import (\n",
    "    load_and_concatenate_saved_dataframes,\n",
    ")\n",
    "from topollm.config_classes.constants import NAME_PREFIXES_TO_FULL_DESCRIPTIONS, TOPO_LLM_REPOSITORY_BASE_PATH\n",
    "from topollm.typing.enums import Verbosity\n",
    "\n",
    "# Create a logger\n",
    "default_logger: logging.Logger = logging.getLogger(name=__name__)\n",
    "default_logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# Create a stream handler\n",
    "stream_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "stream_handler.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# Create a formatter and attach it to the handler\n",
    "formatter = logging.Formatter(fmt=\"[%(asctime)s][%(levelname)8s][%(name)s] %(message)s (%(filename)s:%(lineno)s)\")\n",
    "stream_handler.setFormatter(fmt=formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "if not default_logger.handlers:  # Avoid adding duplicate handlers in case the cell is re-executed\n",
    "    default_logger.addHandler(hdlr=stream_handler)\n",
    "\n",
    "verbosity: Verbosity = Verbosity.NORMAL\n",
    "logger: logging.Logger = default_logger\n",
    "\n",
    "# Example usage\n",
    "logger.debug(msg=\"This is a debug message.\")\n",
    "logger.info(msg=\"This is an info message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_folder_base_path = pathlib.Path(\n",
    "    TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "    \"data/analysis/sample_sizes/\",\n",
    "    \"run_general_comparisons/\",\n",
    "    \"array_truncation_size=5000/\",\n",
    "    \"analysis/twonn/\",\n",
    ")\n",
    "\n",
    "concatenated_df: DataFrame = load_and_concatenate_saved_dataframes(\n",
    "    root_dir=comparisons_folder_base_path,\n",
    ")\n",
    "\n",
    "columns_to_investigate: list[str] = [\n",
    "    \"data_full\",\n",
    "    \"data_subsampling_full\",\n",
    "    \"model_partial_name\",\n",
    "]\n",
    "\n",
    "for column_name in columns_to_investigate:\n",
    "    print(30 * \"=\")\n",
    "    print(\n",
    "        f\"Unique values in column '{column_name = }':\",\n",
    "    )\n",
    "    print(\n",
    "        concatenated_df[column_name].unique(),\n",
    "    )\n",
    "\n",
    "concatenated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the monitoring files containing the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLossExtractor:\n",
    "    \"\"\"Class for extracting the losses of a model over the finetuning checkpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, train_loss_file_path: pathlib.Path, eval_loss_file_path: pathlib.Path):\n",
    "        \"\"\"Initialize the class.\"\"\"\n",
    "        train_loss_df: DataFrame = pd.read_csv(filepath_or_buffer=train_loss_file_path)\n",
    "        eval_loss_df: DataFrame = pd.read_csv(filepath_or_buffer=eval_loss_file_path)\n",
    "        self.loss_dfs_container: dict = {\n",
    "            \"train\": train_loss_df,\n",
    "            \"eval\": eval_loss_df,\n",
    "        }\n",
    "\n",
    "    def get_model_losses_over_finetuning_checkpoints(\n",
    "        self,\n",
    "        data_full: str,\n",
    "        data_subsampling_split: str,\n",
    "        model_partial_name: str,\n",
    "        language_model_seed: int,\n",
    "    ) -> pd.DataFrame | None:\n",
    "        \"\"\"Extract the losses of a model over the finetuning checkpoints.\"\"\"\n",
    "        match data_subsampling_split:\n",
    "            case \"train\":\n",
    "                selected_table = self.loss_dfs_container[\"train\"]\n",
    "                column_name_split_descriptor = \"train\"\n",
    "            case \"validation\":\n",
    "                selected_table = self.loss_dfs_container[\"eval\"]\n",
    "                column_name_split_descriptor = \"eval\"\n",
    "            case _:\n",
    "                return None\n",
    "\n",
    "        match model_partial_name:\n",
    "            case \"model=model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\":\n",
    "                loss_column_name = f\"fresh-microwave-2 - {column_name_split_descriptor}/loss\"\n",
    "            case \"model=model-roberta-base_task-masked_lm_one-year-of-tsla-on-reddit-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\":\n",
    "                loss_column_name = f\"dutiful-snowball-3 - {column_name_split_descriptor}/loss\"\n",
    "            case _:\n",
    "                return None\n",
    "\n",
    "        # Create the output dataframe from the selected table and the loss_column_name\n",
    "        output_table = selected_table[[\"train/global_step\", loss_column_name]]\n",
    "\n",
    "        output_table = output_table.rename(\n",
    "            columns={\n",
    "                \"train/global_step\": NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"ckpt\"],\n",
    "                loss_column_name: \"loss\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return output_table\n",
    "\n",
    "\n",
    "# Initialize the class\n",
    "model_loss_extractor = ModelLossExtractor(\n",
    "    train_loss_file_path=pathlib.Path(\n",
    "        TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "        \"data/models/finetuning_monitoring/\",\n",
    "        \"wandb_export_2024-11-20T18_47_32.346+01_00_train_loss.csv\",\n",
    "    ),\n",
    "    eval_loss_file_path=pathlib.Path(\n",
    "        TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "        \"data/models/finetuning_monitoring/\",\n",
    "        \"wandb_export_2024-11-20T19_02_59.541+01_00_eval_loss.csv\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # # #\n",
    "# Test the loss extraction function\n",
    "\n",
    "# data_full_test, model_partial_name_test = (\n",
    "#     \"data=multiwoz21_spl-mode=do_nothing_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "#     \"model=model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "# )\n",
    "data_full_test, model_partial_name_test = (\n",
    "    \"data=one-year-of-tsla-on-reddit_spl-mode=proportions_spl-shuf=True_spl-seed=0_tr=0.8_va=0.1_te=0.1_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "    \"model=model-roberta-base_task-masked_lm_one-year-of-tsla-on-reddit-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    ")\n",
    "\n",
    "data_subsampling_split_test = \"train\"\n",
    "# data_subsampling_split_test = \"validation\"\n",
    "\n",
    "\n",
    "output = model_loss_extractor.get_model_losses_over_finetuning_checkpoints(\n",
    "    data_full=data_full_test,\n",
    "    data_subsampling_split=data_subsampling_split_test,\n",
    "    model_partial_name=model_partial_name_test,\n",
    "    language_model_seed=1234,\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate different model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topollm.analysis.compare_sampling_methods.make_plots import (\n",
    "    Y_AXIS_LIMITS,\n",
    "    create_boxplot_of_mean_over_different_sampling_seeds,\n",
    "    generate_fixed_params_text,\n",
    ")\n",
    "\n",
    "\n",
    "def create_histograms_over_model_checkpoints(\n",
    "    concatenated_df: pd.DataFrame,\n",
    "    concatenated_filters_dict: dict,\n",
    "    base_model_partial_name: str = \"model=roberta-base\",\n",
    "    figsize: tuple[int, int] = (24, 8),\n",
    "    common_prefix_path: pathlib.Path | None = None,\n",
    "    model_losses_df: pd.DataFrame | None = None,\n",
    "    verbosity: Verbosity = Verbosity.NORMAL,\n",
    "    logger: logging.Logger = default_logger,\n",
    ") -> None:\n",
    "    \"\"\"Create histograms over the different model checkpoints for the concatenated dataframe.\"\"\"\n",
    "    filtered_concatenated_df: pd.DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "        verbosity=verbosity,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Filter for the dataframe with just the base model data\n",
    "\n",
    "    same_filters_but_for_base_model = concatenated_filters_dict.copy()\n",
    "    same_filters_but_for_base_model[\"model_partial_name\"] = base_model_partial_name\n",
    "    # We need to drop the entry for \"model_seed\", because for the base model this value is empty\n",
    "    same_filters_but_for_base_model.pop(\"model_seed\")\n",
    "\n",
    "    filtered_for_base_model_concatenated_df = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=same_filters_but_for_base_model,\n",
    "    )\n",
    "\n",
    "    # Set all the values in the \"model_checkpoint\" column to \"-1\"\n",
    "    filtered_for_base_model_concatenated_df[NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"ckpt\"]] = -1\n",
    "\n",
    "    # # # #\n",
    "    # Create a dataframe by concatenating the two dataframes\n",
    "    data_for_checkpoint_analysis_df: DataFrame = pd.concat(\n",
    "        objs=[filtered_concatenated_df, filtered_for_base_model_concatenated_df],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Group \"data_for_checkpoint_analysis_df\" by value in 'model_checkpoint' column\n",
    "    # and make a boxplot of \"array_data_truncated_mean\" for each group\n",
    "\n",
    "    fixed_params_text: str = generate_fixed_params_text(\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "    )\n",
    "\n",
    "    for y_min, y_max in Y_AXIS_LIMITS.values():\n",
    "        if common_prefix_path is not None:\n",
    "            plot_save_path = pathlib.Path(\n",
    "                common_prefix_path,\n",
    "                \"plots\",\n",
    "                f\"y_{y_min}_{y_max}.pdf\",\n",
    "            )\n",
    "            raw_data_save_path = pathlib.Path(\n",
    "                common_prefix_path,\n",
    "                \"raw_data\",\n",
    "                \"raw_data.csv\",\n",
    "            )\n",
    "            aggregated_results_save_path = pathlib.Path(\n",
    "                common_prefix_path,\n",
    "                \"raw_data\",\n",
    "                \"aggregated_results.csv\",\n",
    "            )\n",
    "        else:\n",
    "            plot_save_path = None\n",
    "            raw_data_save_path = None\n",
    "            aggregated_results_save_path = None\n",
    "\n",
    "        # TODO: Integrate the plotting of the model losses if available\n",
    "\n",
    "        create_boxplot_of_mean_over_different_sampling_seeds(\n",
    "            subset_local_estimates_df=data_for_checkpoint_analysis_df,\n",
    "            x_column_name=NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"ckpt\"],\n",
    "            y_column_name=\"array_data_truncated_mean\",\n",
    "            fixed_params_text=fixed_params_text,\n",
    "            figsize=figsize,  # This should be a bit larger than the default size, because we have many checkpoints to show\n",
    "            y_min=y_min,\n",
    "            y_max=y_max,\n",
    "            plot_save_path=plot_save_path,\n",
    "            raw_data_save_path=raw_data_save_path,\n",
    "            aggregated_results_save_path=aggregated_results_save_path,\n",
    "            verbosity=verbosity,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "def run_checkpoint_analysis_over_different_data_and_models(\n",
    "    data_full_list_to_process: list[str],\n",
    "    data_subsampling_split_list_to_process: list[str],\n",
    "    data_subsampling_sampling_mode_list_to_process: list[str],\n",
    "    model_partial_name_list_to_process: list[str],\n",
    "    language_model_seed_list_to_process: list[int],\n",
    ") -> None:\n",
    "    \"\"\"Run the checkpoint analysis over different data and models.\"\"\"\n",
    "    for (\n",
    "        data_full,\n",
    "        data_subsampling_split,\n",
    "        data_subsampling_sampling_mode,\n",
    "        model_partial_name,\n",
    "        language_model_seed,\n",
    "    ) in tqdm(\n",
    "        product(\n",
    "            data_full_list_to_process,\n",
    "            data_subsampling_split_list_to_process,\n",
    "            data_subsampling_sampling_mode_list_to_process,\n",
    "            model_partial_name_list_to_process,\n",
    "            language_model_seed_list_to_process,\n",
    "        ),\n",
    "        desc=\"Processing different combinations of data_full, data_subsampling_split, and model_partial_name\",\n",
    "    ):\n",
    "        concatenated_filters_dict = {\n",
    "            \"data_full\": data_full,\n",
    "            \"data_subsampling_split\": data_subsampling_split,\n",
    "            \"data_subsampling_sampling_mode\": data_subsampling_sampling_mode,\n",
    "            \"data_subsampling_number_of_samples\": 10_000,\n",
    "            \"model_partial_name\": model_partial_name,\n",
    "            \"model_seed\": language_model_seed,\n",
    "            \"data_prep_sampling_method\": \"random\",\n",
    "            \"data_prep_sampling_samples\": 150_000,\n",
    "            NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"dedup\"]: \"array_deduplicator\",\n",
    "            \"local_estimates_samples\": 60_000,\n",
    "            \"n_neighbors\": 128,\n",
    "        }\n",
    "\n",
    "        common_prefix_path = pathlib.Path(\n",
    "            TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "            \"data\",\n",
    "            \"saved_plots\",\n",
    "            \"mean_estimates_over_different_checkpoints\",\n",
    "            f\"{data_full=}\",\n",
    "            f\"{data_subsampling_split=}\",\n",
    "            f\"{data_subsampling_sampling_mode=}\",\n",
    "            f\"{model_partial_name=}\",\n",
    "            f\"{language_model_seed=}\",\n",
    "        )\n",
    "\n",
    "        create_histograms_over_model_checkpoints(\n",
    "            concatenated_df=concatenated_df,\n",
    "            concatenated_filters_dict=concatenated_filters_dict,\n",
    "            figsize=(22, 8),\n",
    "            common_prefix_path=common_prefix_path,\n",
    "            verbosity=verbosity,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "# # # #\n",
    "# Select which analysis to run and call the analysis\n",
    "data_full_list_to_process: list[str] = [\n",
    "    \"data=multiwoz21_spl-mode=do_nothing_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "    \"data=one-year-of-tsla-on-reddit_spl-mode=proportions_spl-shuf=True_spl-seed=0_tr=0.8_va=0.1_te=0.1_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "]\n",
    "\n",
    "data_subsampling_split_list_to_process: list[str] = [\n",
    "    \"train\",\n",
    "    \"validation\",\n",
    "    \"test\",\n",
    "]\n",
    "\n",
    "data_subsampling_sampling_mode_list_to_process: list[str] = [\n",
    "    \"random\",\n",
    "    \"take_first\",\n",
    "]\n",
    "\n",
    "model_partial_name_list_to_process: list[str] = [\n",
    "    \"model=model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "    \"model=model-roberta-base_task-masked_lm_one-year-of-tsla-on-reddit-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "]\n",
    "\n",
    "# Note: The \"model_seed\" column contains type integer values\n",
    "language_model_seed_list_to_process: list[int] = [\n",
    "    1234,\n",
    "    1235,\n",
    "    1236,\n",
    "]\n",
    "\n",
    "# # # #\n",
    "# Uncomment to run the checkpoint analysis on the full available data\n",
    "#\n",
    "# run_checkpoint_analysis_over_different_data_and_models(\n",
    "#     data_full_list_to_process=data_full_list_to_process,\n",
    "#     data_subsampling_split_list_to_process=data_subsampling_split_list_to_process,\n",
    "#     data_subsampling_sampling_mode_list_to_process=data_subsampling_sampling_mode_list_to_process,\n",
    "#     model_partial_name_list_to_process=model_partial_name_list_to_process,\n",
    "#     language_model_seed_list_to_process=language_model_seed_list_to_process,\n",
    "# )\n",
    "\n",
    "# # # #\n",
    "# Run checkpoint analysis on example data\n",
    "\n",
    "run_checkpoint_analysis_over_different_data_and_models(\n",
    "    data_full_list_to_process=[\"data=multiwoz21_spl-mode=do_nothing_ctxt=dataset_entry_feat-col=ner_tags\"],\n",
    "    data_subsampling_split_list_to_process=[\"train\"],\n",
    "    data_subsampling_sampling_mode_list_to_process=[\"random\"],\n",
    "    model_partial_name_list_to_process=[\n",
    "        \"model=model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "    ],\n",
    "    language_model_seed_list_to_process=[1234],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the influence of the data subsampling method on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topollm.analysis.compare_sampling_methods.make_plots import Y_AXIS_LIMITS_ONLY_FULL\n",
    "\n",
    "\n",
    "def log_subsampling_number_of_samples_values(\n",
    "    filtered_concatenated_df: pd.DataFrame,\n",
    ") -> None:\n",
    "    \"\"\"For every occurence of value in \"data_subsampling_number_of_samples\", check how many rows are present in the filtered dataframe.\"\"\"\n",
    "    data_subsampling_number_of_samples_values = filtered_concatenated_df[\"data_subsampling_number_of_samples\"].unique()\n",
    "\n",
    "    for data_subsampling_number_of_samples in data_subsampling_number_of_samples_values:\n",
    "        data_subsampling_number_of_samples_filters_dict = {\n",
    "            \"data_subsampling_number_of_samples\": data_subsampling_number_of_samples,\n",
    "        }\n",
    "\n",
    "        filtered_concatenated_df_for_number_of_samples: DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "            df=filtered_concatenated_df,\n",
    "            filters_dict=data_subsampling_number_of_samples_filters_dict,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{data_subsampling_number_of_samples = }: {filtered_concatenated_df_for_number_of_samples.shape = }\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Unique data_subsampling_sampling_seed:\\n\",\n",
    "        filtered_concatenated_df[\"data_subsampling_sampling_seed\"].unique(),\n",
    "    )\n",
    "\n",
    "\n",
    "data_full = concatenated_df[\"data_full\"].unique()[1]\n",
    "data_subsampling_split = concatenated_df[\"data_subsampling_split\"].unique()[2]\n",
    "data_subsampling_sampling_mode_list_to_process: str = \"random\"\n",
    "\n",
    "model_full = concatenated_df[\"model_full\"].unique()[0]\n",
    "\n",
    "concatenated_filters_dict = {\n",
    "    \"data_full\": data_full,\n",
    "    \"model_full\": model_full,\n",
    "    \"data_subsampling_split\": data_subsampling_split,\n",
    "    \"data_subsampling_sampling_mode\": data_subsampling_sampling_mode_list_to_process,\n",
    "    \"data_prep_sampling_method\": \"random\",\n",
    "    \"data_prep_sampling_samples\": 100_000,\n",
    "    NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"dedup\"]: \"array_deduplicator\",\n",
    "    \"local_estimates_samples\": 60_000,\n",
    "    \"n_neighbors\": 128,\n",
    "}\n",
    "\n",
    "filtered_concatenated_df: DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "    df=concatenated_df,\n",
    "    filters_dict=concatenated_filters_dict,\n",
    ")\n",
    "\n",
    "log_subsampling_number_of_samples_values(\n",
    "    filtered_concatenated_df=filtered_concatenated_df,\n",
    ")\n",
    "\n",
    "print(f\"{filtered_concatenated_df.shape = }\")\n",
    "\n",
    "# # # #\n",
    "# START Additional data cleaning:\n",
    "# Remove those samples where \"array_data.size\" is smaller than 30_000\n",
    "\n",
    "filtered_concatenated_df_cleaned = filtered_concatenated_df[filtered_concatenated_df[\"array_data.size\"] >= 50_000]\n",
    "\n",
    "# END Additional data cleaning\n",
    "# # # #\n",
    "\n",
    "\n",
    "data_for_different_data_subsampling_number_of_samples_analysis_df: DataFrame = filtered_concatenated_df_cleaned\n",
    "\n",
    "fixed_params_text: str = generate_fixed_params_text(\n",
    "    filters_dict=concatenated_filters_dict,\n",
    ")\n",
    "\n",
    "x_column_name = \"data_subsampling_number_of_samples\"\n",
    "\n",
    "for y_min, y_max in Y_AXIS_LIMITS_ONLY_FULL.values():\n",
    "    # for y_min, y_max in [(6.0, 10.0)]:\n",
    "    create_boxplot_of_mean_over_different_sampling_seeds(\n",
    "        subset_local_estimates_df=data_for_different_data_subsampling_number_of_samples_analysis_df,\n",
    "        plot_save_path=None,  # TODO: Select path\n",
    "        raw_data_save_path=None,  # TODO: Select path\n",
    "        x_column_name=x_column_name,\n",
    "        y_column_name=\"array_data_truncated_mean\",\n",
    "        seed_column_name=\"data_subsampling_sampling_seed\",\n",
    "        fixed_params_text=fixed_params_text,\n",
    "        y_min=y_min,\n",
    "        y_max=y_max,\n",
    "        logger=logger,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
