{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "from topollm.analysis.compare_sampling_methods.make_plots import (\n",
    "    Y_AXIS_LIMITS_ONLY_FULL,\n",
    "    create_boxplot_of_mean_over_different_sampling_seeds,\n",
    "    generate_fixed_params_text,\n",
    ")\n",
    "from topollm.analysis.compare_sampling_methods.run_general_comparisons import (\n",
    "    filter_dataframe_based_on_filters_dict,\n",
    "    load_and_concatenate_saved_dataframes,\n",
    ")\n",
    "from topollm.config_classes.constants import NAME_PREFIXES_TO_FULL_DESCRIPTIONS, TOPO_LLM_REPOSITORY_BASE_PATH\n",
    "from topollm.typing.enums import Verbosity\n",
    "\n",
    "# Create a logger\n",
    "default_logger: logging.Logger = logging.getLogger(name=__name__)\n",
    "default_logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# Create a stream handler\n",
    "stream_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "stream_handler.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# Create a formatter and attach it to the handler\n",
    "formatter = logging.Formatter(fmt=\"[%(asctime)s][%(levelname)8s][%(name)s] %(message)s (%(filename)s:%(lineno)s)\")\n",
    "stream_handler.setFormatter(fmt=formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "if not default_logger.handlers:  # Avoid adding duplicate handlers in case the cell is re-executed\n",
    "    default_logger.addHandler(hdlr=stream_handler)\n",
    "\n",
    "verbosity: Verbosity = Verbosity.NORMAL\n",
    "logger: logging.Logger = default_logger\n",
    "\n",
    "# Example usage\n",
    "logger.debug(msg=\"This is a debug message.\")\n",
    "logger.info(msg=\"This is an info message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_folder_base_path = pathlib.Path(\n",
    "    TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "    \"data/analysis/sample_sizes/\",\n",
    "    \"run_general_comparisons/\",\n",
    "    \"array_truncation_size=5000/\",\n",
    "    \"analysis/twonn/\",\n",
    ")\n",
    "\n",
    "concatenated_df: DataFrame = load_and_concatenate_saved_dataframes(\n",
    "    root_dir=comparisons_folder_base_path,\n",
    ")\n",
    "\n",
    "columns_to_investigate: list[str] = [\n",
    "    \"data_full\",\n",
    "    \"data_subsampling_full\",\n",
    "    \"model_partial_name\",\n",
    "]\n",
    "\n",
    "for column_name in columns_to_investigate:\n",
    "    print(30 * \"=\")\n",
    "    print(\n",
    "        f\"Unique values in column '{column_name = }':\",\n",
    "    )\n",
    "    print(\n",
    "        concatenated_df[column_name].unique(),\n",
    "    )\n",
    "\n",
    "concatenated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate different model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topollm.analysis.compare_sampling_methods.make_plots import Y_AXIS_LIMITS\n",
    "\n",
    "# TODO: Save the raw data to files\n",
    "\n",
    "\n",
    "def create_histograms_over_model_checkpoints(\n",
    "    concatenated_df: pd.DataFrame,\n",
    "    concatenated_filters_dict: dict,\n",
    "    base_model_partial_name: str = \"model=roberta-base\",\n",
    "    figsize: tuple[int, int] = (24, 8),\n",
    "    common_prefix_path: pathlib.Path | None = None,\n",
    "    raw_data_save_path: pathlib.Path | None = None,\n",
    "    verbosity: Verbosity = Verbosity.NORMAL,\n",
    "    logger: logging.Logger = default_logger,\n",
    ") -> None:\n",
    "    \"\"\"Create histograms over the different model checkpoints for the concatenated dataframe.\"\"\"\n",
    "    filtered_concatenated_df: pd.DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "        verbosity=verbosity,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Filter for the dataframe with just the base model data\n",
    "\n",
    "    same_filters_but_for_base_model = concatenated_filters_dict.copy()\n",
    "    same_filters_but_for_base_model[\"model_partial_name\"] = base_model_partial_name\n",
    "\n",
    "    filtered_for_base_model_concatenated_df = filter_dataframe_based_on_filters_dict(\n",
    "        df=concatenated_df,\n",
    "        filters_dict=same_filters_but_for_base_model,\n",
    "    )\n",
    "\n",
    "    # Set all the values in the \"model_checkpoint\" column to \"-1\"\n",
    "    filtered_for_base_model_concatenated_df[\"model_checkpoint\"] = -1\n",
    "\n",
    "    # # # #\n",
    "    # Create a dataframe by concatenating the two dataframes\n",
    "    data_for_checkpoint_analysis_df: DataFrame = pd.concat(\n",
    "        objs=[filtered_concatenated_df, filtered_for_base_model_concatenated_df],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # # # #\n",
    "    # Group \"data_for_checkpoint_analysis_df\" by value in 'model_checkpoint' column\n",
    "    # and make a boxplot of \"array_data_truncated_mean\" for each group\n",
    "\n",
    "    fixed_params_text: str = generate_fixed_params_text(\n",
    "        filters_dict=concatenated_filters_dict,\n",
    "    )\n",
    "\n",
    "    for y_min, y_max in Y_AXIS_LIMITS.values():\n",
    "        if common_prefix_path is not None:\n",
    "            plot_save_path = pathlib.Path(\n",
    "                common_prefix_path,\n",
    "                \"plots\",\n",
    "                f\"y_{y_min}_{y_max}.pdf\",\n",
    "            )\n",
    "        else:\n",
    "            plot_save_path = None\n",
    "\n",
    "        create_boxplot_of_mean_over_different_sampling_seeds(\n",
    "            subset_local_estimates_df=data_for_checkpoint_analysis_df,\n",
    "            x_column_name=\"model_checkpoint\",\n",
    "            y_column_name=\"array_data_truncated_mean\",\n",
    "            fixed_params_text=fixed_params_text,\n",
    "            figsize=figsize,  # This should be a bit larger than the default, because we have more checkpoints to show\n",
    "            y_min=y_min,\n",
    "            y_max=y_max,\n",
    "            plot_save_path=plot_save_path,\n",
    "            raw_data_save_path=raw_data_save_path,\n",
    "            verbosity=verbosity,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "# # # #\n",
    "# Select which analysis to run and call the analysis\n",
    "\n",
    "data_full_list_to_process = [\n",
    "    \"data=multiwoz21_spl-mode=do_nothing_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "    \"data=one-year-of-tsla-on-reddit_spl-mode=proportions_spl-shuf=True_spl-seed=0_tr=0.8_va=0.1_te=0.1_ctxt=dataset_entry_feat-col=ner_tags\",\n",
    "]\n",
    "\n",
    "data_subsampling_split_to_process = [\n",
    "    \"train\",\n",
    "    \"validation\",\n",
    "    \"test\",\n",
    "]\n",
    "\n",
    "model_partial_name_list_to_process = [\n",
    "    \"model=model-roberta-base_task-masked_lm_multiwoz21-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "    \"model=model-roberta-base_task-masked_lm_one-year-of-tsla-on-reddit-train-10000-ner_tags_ftm-standard_lora-None_5e-05-constant-0.01-50\",\n",
    "]\n",
    "\n",
    "# Note: The \"model_seed\" column contains type integer values\n",
    "language_model_seed_list_to_process = [\n",
    "    1234,\n",
    "]\n",
    "\n",
    "data_subsampling_sampling_mode: str = \"random\"\n",
    "\n",
    "for data_full, data_subsampling_split, model_partial_name, language_model_seed in tqdm(\n",
    "    product(\n",
    "        data_full_list_to_process,\n",
    "        data_subsampling_split_to_process,\n",
    "        model_partial_name_list_to_process,\n",
    "        language_model_seed_list_to_process,\n",
    "    ),\n",
    "    desc=\"Processing different combinations of data_full, data_subsampling_split, and model_partial_name\",\n",
    "):\n",
    "    concatenated_filters_dict = {\n",
    "        \"data_full\": data_full,\n",
    "        \"data_subsampling_sampling_mode\": data_subsampling_sampling_mode,\n",
    "        \"data_subsampling_split\": data_subsampling_split,\n",
    "        \"data_subsampling_number_of_samples\": 10_000,\n",
    "        \"model_partial_name\": model_partial_name,\n",
    "        \"model_seed\": language_model_seed,\n",
    "        \"data_prep_sampling_method\": \"random\",\n",
    "        \"data_prep_sampling_samples\": 150_000,\n",
    "        NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"dedup\"]: \"array_deduplicator\",\n",
    "        \"local_estimates_samples\": 60_000,\n",
    "        \"n_neighbors\": 128,\n",
    "    }\n",
    "\n",
    "    common_prefix_path = pathlib.Path(\n",
    "        TOPO_LLM_REPOSITORY_BASE_PATH,\n",
    "        \"data\",\n",
    "        \"saved_plots\",\n",
    "        \"mean_estimates_over_different_checkpoints\",\n",
    "        data_full,\n",
    "        f\"{data_subsampling_split=}\",\n",
    "        model_partial_name,\n",
    "        f\"{language_model_seed=}\",\n",
    "    )\n",
    "\n",
    "    create_histograms_over_model_checkpoints(\n",
    "        concatenated_df=concatenated_df,\n",
    "        concatenated_filters_dict=concatenated_filters_dict,\n",
    "        figsize=(22, 8),\n",
    "        common_prefix_path=common_prefix_path,\n",
    "        verbosity=verbosity,\n",
    "        logger=logger,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the influence of the data subsampling method on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subsampling_number_of_samples_values(\n",
    "    filtered_concatenated_df: pd.DataFrame,\n",
    ") -> None:\n",
    "    \"\"\"For every occurence of value in \"data_subsampling_number_of_samples\", check how many rows are present in the filtered dataframe.\"\"\"\n",
    "    data_subsampling_number_of_samples_values = filtered_concatenated_df[\"data_subsampling_number_of_samples\"].unique()\n",
    "\n",
    "    for data_subsampling_number_of_samples in data_subsampling_number_of_samples_values:\n",
    "        data_subsampling_number_of_samples_filters_dict = {\n",
    "            \"data_subsampling_number_of_samples\": data_subsampling_number_of_samples,\n",
    "        }\n",
    "\n",
    "        filtered_concatenated_df_for_number_of_samples: DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "            df=filtered_concatenated_df,\n",
    "            filters_dict=data_subsampling_number_of_samples_filters_dict,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{data_subsampling_number_of_samples = }: {filtered_concatenated_df_for_number_of_samples.shape = }\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Unique data_subsampling_sampling_seed:\\n\",\n",
    "        filtered_concatenated_df[\"data_subsampling_sampling_seed\"].unique(),\n",
    "    )\n",
    "\n",
    "\n",
    "data_full = concatenated_df[\"data_full\"].unique()[1]\n",
    "data_subsampling_split = concatenated_df[\"data_subsampling_split\"].unique()[2]\n",
    "data_subsampling_sampling_mode: str = \"random\"\n",
    "\n",
    "model_full = concatenated_df[\"model_full\"].unique()[0]\n",
    "\n",
    "concatenated_filters_dict = {\n",
    "    \"data_full\": data_full,\n",
    "    \"model_full\": model_full,\n",
    "    \"data_subsampling_split\": data_subsampling_split,\n",
    "    \"data_subsampling_sampling_mode\": data_subsampling_sampling_mode,\n",
    "    \"data_prep_sampling_method\": \"random\",\n",
    "    \"data_prep_sampling_samples\": 100_000,\n",
    "    NAME_PREFIXES_TO_FULL_DESCRIPTIONS[\"dedup\"]: \"array_deduplicator\",\n",
    "    \"local_estimates_samples\": 60_000,\n",
    "    \"n_neighbors\": 128,\n",
    "}\n",
    "\n",
    "filtered_concatenated_df: DataFrame = filter_dataframe_based_on_filters_dict(\n",
    "    df=concatenated_df,\n",
    "    filters_dict=concatenated_filters_dict,\n",
    ")\n",
    "\n",
    "log_subsampling_number_of_samples_values(\n",
    "    filtered_concatenated_df=filtered_concatenated_df,\n",
    ")\n",
    "\n",
    "print(f\"{filtered_concatenated_df.shape = }\")\n",
    "\n",
    "# # # #\n",
    "# START Additional data cleaning:\n",
    "# Remove those samples where \"array_data.size\" is smaller than 30_000\n",
    "\n",
    "filtered_concatenated_df_cleaned = filtered_concatenated_df[filtered_concatenated_df[\"array_data.size\"] >= 50_000]\n",
    "\n",
    "# END Additional data cleaning\n",
    "# # # #\n",
    "\n",
    "\n",
    "data_for_different_data_subsampling_number_of_samples_analysis_df: DataFrame = filtered_concatenated_df_cleaned\n",
    "\n",
    "fixed_params_text: str = generate_fixed_params_text(\n",
    "    filters_dict=concatenated_filters_dict,\n",
    ")\n",
    "\n",
    "x_column_name = \"data_subsampling_number_of_samples\"\n",
    "\n",
    "for y_min, y_max in Y_AXIS_LIMITS_ONLY_FULL.values():\n",
    "    # for y_min, y_max in [(6.0, 10.0)]:\n",
    "    create_boxplot_of_mean_over_different_sampling_seeds(\n",
    "        subset_local_estimates_df=data_for_different_data_subsampling_number_of_samples_analysis_df,\n",
    "        plot_save_path=None,  # TODO: Select path\n",
    "        raw_data_save_path=None,  # TODO: Select path\n",
    "        x_column_name=x_column_name,\n",
    "        y_column_name=\"array_data_truncated_mean\",\n",
    "        seed_column_name=\"data_subsampling_sampling_seed\",\n",
    "        fixed_params_text=fixed_params_text,\n",
    "        y_min=y_min,\n",
    "        y_max=y_max,\n",
    "        logger=logger,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
