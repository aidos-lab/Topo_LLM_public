[tool.ruff]
# extend = "$HOME/.config/ruff.toml"
target-version = "py310"
# Set the maximum line length.
line-length = 120

select = ["ALL"]
# Add the `line-too-long` rule to the enforced rule set. By default, Ruff omits rules that
# overlap with the use of a formatter, like Black, but we can override this behavior by
# explicitly adding the rule.
# extend-select = ["E501"]
ignore = ["RET504", "D102"]

[tool.ruff.lint.pylint]
max-args = 7

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.pyright]
include = ["topollm", "tests"]
exclude = [
    "**/node_modules",
    "**/__pycache__",
    "data",
    "hydra_output_dir",
    "wandb_output_dir",
]
ignore = ["src/oldstuff"]
defineConstant = { DEBUG = true }

[tool.coverage.report]
exclude_lines = ["pragma: no cover", "if TYPE_CHECKING"]

[tool.poetry]
name = "topollm"
version = "0.1.0"
description = "Investigating embedding spaces generated by language models from a topological perspective"
authors = [
    "Benjamin Matthias Ruppik <mail@ruppik.net>",
    "Julius von Rohrscheidt <julius.rohrscheidt@helmholtz-muenchen.de>",
]
license = "Apache 2.0"
readme = "README.md"

[tool.poetry.dependencies]
accelerate = "*"
datasets = "*"
hydra-core = "*"
gitpython = "*"
gudhi = "*"
pandas = "*"
pydantic = "*"
python = "^3.12"
scikit-learn = "*"
scikit-dimension = "*"
StrEnum = { version = "^0.4", markers = "python_version < '3.11'" }
torch = "2.3.*"
tqdm = "*"
transformers = ">=4,<4.48"
zarr = ">=2,<3" # Our code has not been migrated to zarr v3 yet, we will follow https://zarr.readthedocs.io/en/latest/user-guide/v3_migration.html
tensorboard = "*"
seaborn = "*"
giotto-tda = "^0.6.2"                                               # need to pin this version because otherwise installation fails on MacBook
umap-learn = "*"
peft = "*"
python-dotenv = "*"
graphriccicurvature = "*"
networkx = "*"
pyyaml = "*"
hydra-colorlog = "*"
wandb = "0.16.*"                                                    # We currently need to pin the wandb version because 0.17 does not work locally on the MacBook
rich = "*"
evaluate = "*"
seqeval = "*"
nltk = "^3.8.1"
kaleido = "0.2.1"                                                   # need to pin this version because otherwise installation fails on MacBook
annotated-types = "*"
pydantic-core = "*"
click = "*"
platformdirs = "*"
hydra-zen = "*"
hydra-joblib-launcher = "*"
pypdf = "*"
reportlab = "*"
markupsafe = "^3.0.2"

[tool.poetry.group.dev.dependencies]
hypothesis = "*"
pytest = "*"
pytest-cov = "*"
ipykernel = "*"

[tool.poetry.group.cpu]
optional = true

[tool.poetry.group.cpu.dependencies]
faiss-cpu = "*"

[tool.poetry.group.gpu]
optional = true

[tool.poetry.group.gpu.dependencies]
faiss-gpu = "*"
gpustat = "*"
# Note: Commenting this out appears to lead to hanging install on HHU Hilbert.
#
# [[tool.poetry.source]]
# name = "PyPI"
# priority = "primary"

# This section is for using the PyPI mirror on Hilbert (HHU internal).
# Note: You need to uncomment this section if you do not have access to the Hilbert PyPI mirror.
# This should be commented out before releasing the package.
#
[[tool.poetry.source]]
name = "hhu"
url = "http://pypi.repo.test.hhu.de/simple/"
priority = "primary"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

# Script entry points for various tasks
[tool.poetry.scripts]
tests = "topollm.scripts.run_tests_with_coverage:main"
finetune_language_model = "topollm.model_finetuning.run_finetune_language_model_on_huggingface_dataset:main"
compute_embeddings = "topollm.compute_embeddings.run_compute_embeddings:main"
compute_perplexity = "topollm.model_inference.perplexity.run_compute_perplexity:main"
pipeline_inference = "topollm.model_inference.run_inference_pipeline:main"
pipeline_local_estimates = "topollm.pipeline_scripts.run_pipeline_compute_embeddings_and_data_prep_and_local_estimate:main"
# Submit multiple jobs using one of the launchers
submit_jobs = "topollm.scripts.submission_scripts.orchestrate_job_submission:main"
# Submit multiple jobs using one of the launchers and tmux
submit_jobs_via_tmux = "topollm.scripts.submission_scripts.call_submit_jobs_scripts.call_submit_jobs_for_experiments_via_tmux_submission:submit_jobs_in_separate_tmux_sessions"
# Data copying and syncing
sync_gc_data = "topollm.scripts.google_cloud.gsutil_rsync_directory_to_and_from_gc_bucket:main"
# Run analysis of data
all_perplexity_analysis_steps = "topollm.model_inference.perplexity.saved_perplexity_processing.run_all_perplexity_analysis_steps.py:main"
compare_sampling_methods = "topollm.analysis.compare_sampling_methods.run_general_comparisons:main"
