[tool.ruff]
# extend = "$HOME/.config/ruff.toml"
target-version = "py310"
# Set the maximum line length.
line-length = 120

[tool.ruff.lint]
select = ["ALL"]
# Add the `line-too-long` rule to the enforced rule set. By default, Ruff omits rules that
# overlap with the use of a formatter, like Black, but we can override this behavior by
# explicitly adding the rule.
# extend-select = ["E501"]
ignore = ["ANN101", "ANN102", "RET504", "D102"]

[tool.ruff.lint.pylint]
max-args = 7

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.poetry]
name = "topollm"
version = "0.1.0"
description = "Investigating embedding spaces generated by language models from a topological perspective"
authors = ["Benjamin Matthias Ruppik <ruppik@hhu.de>"]
license = "Apache 2.0"
readme = "README.md"

[tool.poetry.dependencies]
accelerate = "*"
datasets = "*"
hydra-core = "*"
gitpython = "*"
gudhi = "*"
pandas = "*"
pydantic = "*"
python = "^3.12"
scikit-learn = "*"
scikit-dimension = "*"
StrEnum = { version = "^0.4", markers = "python_version < '3.11'" }
torch = "2.*"
tqdm = "*"
transformers = "*"
zarr = "*"
tensorboard = "*"
seaborn = "*"
giotto-tda = "^0.6.2"
umap-learn = "^0.5.5"
peft = "*"
python-dotenv = "*"
graphriccicurvature = "*"
networkx = "*"
pyyaml = "*"
hydra-colorlog = "*"
wandb = "*"
rich = "*"
evaluate = "*"
seqeval = "*"

[tool.poetry.group.dev.dependencies]
hypothesis = "*"
pytest = "*"
pytest-cov = "*"
ipykernel = "*"

[tool.poetry.group.cpu]
optional = true

[tool.poetry.group.cpu.dependencies]
faiss-cpu = "*"

[tool.poetry.group.gpu]
optional = true

[tool.poetry.group.gpu.dependencies]
faiss-gpu = "*"
gpustat = "*"

# This section is for using the PyPI mirror on hilbert (HHU internal)
#
# [[tool.poetry.source]]
#     name = "hhu"
#     url = "http://pypi.repo.test.hhu.de/simple/"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

# Script entry points for various tasks
[tool.poetry.scripts]
run_finetune_language_model = "topollm.model_finetuning.run_finetune_language_model_on_huggingface_dataset:main"
run_compute_embeddings = "topollm.compute_embeddings.run_compute_embeddings:main"
run_compute_perplexity = "topollm.model_inference.perplexity.run_compute_perplexity:main"
run_inference_pipeline = "topollm.model_inference.run_inference_pipeline:main"
run_pipeline_compute_embeddings_and_data_prep_and_local_estimate = "topollm.pipeline_scripts.run_pipeline_compute_embeddings_and_data_prep_and_local_estimate:main"
